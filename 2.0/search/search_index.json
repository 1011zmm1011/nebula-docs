{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Nebula Graph 2.0 Documentation \u00b6 Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency. Tutorial Video \u00b6 YouTube bilibili Preface \u00b6 Manual Change Log Introduction \u00b6 What is Nebula Graph Quick start (for beginners) \u00b6 Quick start workflow Deploy Nebula Graph with Docker Compose Connect to Nebula Graph Nebula Graph CRUD nGQL guide (for all users) \u00b6 Operators Comparison Pipe Property reference Set String Precedence Functions and expressions Math String Date and time Schema Case expressions General query statements Match Space statements Create space Use space Show spaces Describe space Drop space Vertex statements Insert vertex Update vertex Upsert vertex Delete vertex Subgraph and path Get subgraph Query tuning statements Explain and profile Deployment and installation (for Developers and DBA) \u00b6 Resource preparations Compile and install Nebula Graph Install Nebula Graph by compiling the source code","title":"Welcome to Nebula Graph 2.0 Documentation"},{"location":"#welcome_to_nebula_graph_20_documentation","text":"Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency.","title":"Welcome to Nebula Graph 2.0 Documentation"},{"location":"#tutorial_video","text":"YouTube bilibili","title":"Tutorial Video"},{"location":"#preface","text":"Manual Change Log","title":"Preface"},{"location":"#introduction","text":"What is Nebula Graph","title":"Introduction"},{"location":"#quick_start_for_beginners","text":"Quick start workflow Deploy Nebula Graph with Docker Compose Connect to Nebula Graph Nebula Graph CRUD","title":"Quick start (for beginners)"},{"location":"#ngql_guide_for_all_users","text":"Operators Comparison Pipe Property reference Set String Precedence Functions and expressions Math String Date and time Schema Case expressions General query statements Match Space statements Create space Use space Show spaces Describe space Drop space Vertex statements Insert vertex Update vertex Upsert vertex Delete vertex Subgraph and path Get subgraph Query tuning statements Explain and profile","title":"nGQL guide (for all users)"},{"location":"#deployment_and_installation_for_developers_and_dba","text":"Resource preparations Compile and install Nebula Graph Install Nebula Graph by compiling the source code","title":"Deployment and installation (for Developers and DBA)"},{"location":"CHANGELOG/","text":"Manual Changes \u00b6 0.1.1 - Initial release Nebula Graph alpha","title":"Manual Changes"},{"location":"CHANGELOG/#manual_changes","text":"0.1.1 - Initial release Nebula Graph alpha","title":"Manual Changes"},{"location":"1.introduction/1.what-is-nebula-graph/","text":"What is Nebula Graph \u00b6 Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with billions of vertices and trillions of edges, and serving queries with millisecond-latency. What is a graph database \u00b6 A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes the data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them. Benefits of Nebula Graph \u00b6 Open-source \u00b6 Nebula Graph is open under the Apache 2.0 and the Commons Clause 1.0 licenses. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page . Outstanding performance \u00b6 Written in C++ and born for graph, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking . Developer friendly \u00b6 Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are being developed. For more information, see Nebula Graph clients [TODO]. Diversified ecosystem \u00b6 More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , nebula-console , and Nebula Graph Exchange . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances. For more information, see Ecosystem development [TODO]. OpenCypher-compatible query language \u00b6 The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide . Easy data modeling and high flexibility \u00b6 You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling [TODO]. Reliable access control \u00b6 Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization [TODO]. High scalability \u00b6 Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service. High popularity \u00b6 Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website . Use cases \u00b6 Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph. Fraud detection \u00b6 Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected. Real-time recommendation \u00b6 Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services. Intelligent question-answer system \u00b6 Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question. Social networking \u00b6 Information on people and their relationships are typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_nebula_graph","text":"Nebula Graph is an open-source, distributed, easily scalable, and native graph database. It is capable of hosting graphs with billions of vertices and trillions of edges, and serving queries with millisecond-latency.","title":"What is Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#what_is_a_graph_database","text":"A graph database, such as Nebula Graph, is a database that specializes in storing vast graph networks and retrieving information from them. It efficiently stores data as vertices (nodes) and edges (relationships) in labeled property graphs. Properties can be attached to both vertices and edges. Each vertex can have one or multiple tags (labels). Graph databases are well suited for storing most kinds of data models abstracted from reality. Things are connected in almost all fields in the world. Modeling systems like relational databases extract the relationships between entities and squeeze them into table columns alone, with their types and properties stored in other columns or even other tables. This makes the data management time-consuming and cost-ineffective. Nebula Graph, as a typical native graph database, allows you to store the rich relationships as edges with edge types and properties directly attached to them.","title":"What is a graph database"},{"location":"1.introduction/1.what-is-nebula-graph/#benefits_of_nebula_graph","text":"","title":"Benefits of Nebula Graph"},{"location":"1.introduction/1.what-is-nebula-graph/#open-source","text":"Nebula Graph is open under the Apache 2.0 and the Commons Clause 1.0 licenses. More and more people such as database developers, data scientists, security experts, and algorithm engineers are participating in the designing and development of Nebula Graph. To join the opening of source code and ideas, surf the Nebula Graph GitHub page .","title":"Open-source"},{"location":"1.introduction/1.what-is-nebula-graph/#outstanding_performance","text":"Written in C++ and born for graph, Nebula Graph handles graph queries in milliseconds. Among most databases, Nebula Graph shows superior performance in providing graph data services. The larger the data size, the greater the superiority of Nebula Graph. For more information, see Nebula Graph benchmarking .","title":"Outstanding performance"},{"location":"1.introduction/1.what-is-nebula-graph/#developer_friendly","text":"Nebula Graph supports clients in popular programming languages like Java, Python, C++, and Go, and more are being developed. For more information, see Nebula Graph clients [TODO].","title":"Developer friendly"},{"location":"1.introduction/1.what-is-nebula-graph/#diversified_ecosystem","text":"More and more native tools of Nebula Graph have been released, such as Nebula Graph Studio , nebula-console , and Nebula Graph Exchange . Besides, Nebula Graph has the ability to be integrated with many cutting-edge technologies, such as Spark, Flink, and HBase, for the purpose of mutual strengthening in a world of increasing challenges and chances. For more information, see Ecosystem development [TODO].","title":"Diversified ecosystem"},{"location":"1.introduction/1.what-is-nebula-graph/#opencypher-compatible_query_language","text":"The native Nebula Graph Query Language, also known as nGQL, is a declarative, openCypher-compatible textual query language. It is easy to understand and easy to use. For more information, see nGQL guide .","title":"OpenCypher-compatible query language"},{"location":"1.introduction/1.what-is-nebula-graph/#easy_data_modeling_and_high_flexibility","text":"You can easily model the connected data into Nebula Graph for your business without forcing them into a structure such as a relational table, and properties can be added, updated, and deleted freely. For more information, see Data modeling [TODO].","title":"Easy data modeling and high flexibility"},{"location":"1.introduction/1.what-is-nebula-graph/#reliable_access_control","text":"Nebula Graph supports strict role-based access control and external authentication servers such as LDAP (Lightweight Directory Access Protocol) servers to enhance data security. For more information, see Authentication and authorization [TODO].","title":"Reliable access control"},{"location":"1.introduction/1.what-is-nebula-graph/#high_scalability","text":"Nebula Graph is designed in a shared-nothing architecture and supports scaling in and out without interrupting the database service.","title":"High scalability"},{"location":"1.introduction/1.what-is-nebula-graph/#high_popularity","text":"Nebula Graph is being used by tech leaders such as Tencent, Vivo, Meituan, and JD Digits. For more information, visit the Nebula Graph official website .","title":"High popularity"},{"location":"1.introduction/1.what-is-nebula-graph/#use_cases","text":"Nebula Graph can be used to support various graph-based scenarios. To spare the time spent on pushing the kinds of data mentioned in this section into relational databases and on bothering with join queries, use Nebula Graph.","title":"Use cases"},{"location":"1.introduction/1.what-is-nebula-graph/#fraud_detection","text":"Financial institutions have to traverse countless transactions to piece together potential crimes and understand how combinations of transactions and devices might be related to a single fraud scheme. This kind of scenario can be modeled in graphs, and with the help of Nebula Graph, fraud rings and other sophisticated scams can be easily detected.","title":"Fraud detection"},{"location":"1.introduction/1.what-is-nebula-graph/#real-time_recommendation","text":"Nebula Graph offers the ability to instantly process the real-time information produced by a visitor and make accurate recommendations on articles, videos, products, and services.","title":"Real-time recommendation"},{"location":"1.introduction/1.what-is-nebula-graph/#intelligent_question-answer_system","text":"Natural languages can be transformed into knowledge graphs and stored in Nebula Graph. A question organized in a natural language can be resolved by a semantic parser in an intelligent question-answer system and re-organized. Then, possible answers to the question can be retrieved from the knowledge graph and provided to the one who asked the question.","title":"Intelligent question-answer system"},{"location":"1.introduction/1.what-is-nebula-graph/#social_networking","text":"Information on people and their relationships are typical graph data. Nebula Graph can easily handle the social networking information of billions of people and trillions of relationships, and provide lightning-fast queries for friend recommendations and job promotions in the case of massive concurrency.","title":"Social networking"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/","text":"Architecture overview \u00b6 Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. Each service has its executable binaries and processes launched from the binaries. You can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster. The Meta Service \u00b6 The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service . The Graph Service and the Storage Service \u00b6 Nebula Graph applies a disaggregated storage and compute architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. And they run on different processes, i.e., nebula-graphd and nebula-storaged. The benefits of disaggregated storage and compute are as follows: Great scalability. A disaggregated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability. If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, or even unfelt by the users. Cost-effective. The separation of computing and storage provides a higher resource utilization rate, and it enables you to manage the cost flexibly according to business demands. The cost savings can be more significant if you use the Nebula Graph Cloud service. Open to more possibilities. With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service (TODO) and Storage Service (TODO).","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#architecture_overview","text":"Nebula Graph consists of three services: the Graph Service, the Storage Service, and the Meta Service. Each service has its executable binaries and processes launched from the binaries. You can deploy a Nebula Graph cluster on a single machine or multiple machines using these binaries. The following figure shows the architecture of a typical Nebula Graph cluster.","title":"Architecture overview"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_meta_service","text":"The Meta Service in the Nebula Graph architecture is run by the nebula-metad processes. It is responsible for metadata management, such as schema operations, cluster administration, and user privilege management. For details on the Meta Service, see Meta Service .","title":"The Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/1.architecture-overview/#the_graph_service_and_the_storage_service","text":"Nebula Graph applies a disaggregated storage and compute architecture. The Graph Service is responsible for querying. The Storage Service is responsible for storage. And they run on different processes, i.e., nebula-graphd and nebula-storaged. The benefits of disaggregated storage and compute are as follows: Great scalability. A disaggregated structure makes both the Graph Service and the Storage Service flexible and easy to scale in or out. High availability. If part of the Graph Service fails, the data stored by the Storage Service suffers no loss. And if the rest part of the Graph Service is still able to serve the clients, service recovery can be performed quickly, or even unfelt by the users. Cost-effective. The separation of computing and storage provides a higher resource utilization rate, and it enables you to manage the cost flexibly according to business demands. The cost savings can be more significant if you use the Nebula Graph Cloud service. Open to more possibilities. With the ability to run separately, the Graph Service may work with multiple types of storage engines, and the Storage Service may serve more types of computing engines. For details on the Graph Service and the Storage Service, see Graph Service (TODO) and Storage Service (TODO).","title":"The Graph Service and the Storage Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/","text":"Meta Service \u00b6 This topic describes the architecture and functions of the Meta Service. The architecture of the Meta Service \u00b6 The architecture of the Meta Service is as follows. The Meta Service is run by the nebula-metad processes. You can deploy nebula-metad processes according to the scenario: In a test environment, you can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that you deploy three processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by quorum, and only the leader can provide service to the clients and other components of Nebula Graph. The followers run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader. Functions of the Meta Service \u00b6 Manages user accounts \u00b6 The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Graph Service through an account, the Graph Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication and authorization (TODO). Manages partitions \u00b6 The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions. Manages graph spaces \u00b6 Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space. Manages schema information \u00b6 Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Nebula Graph schema and data modeling (TODO). Manages TTL-based data eviction \u00b6 The Meta Service provides automatic data eviction and space reclamation based on TTL (time to live) options for Nebula Graph. For more information on TTL, see TTL options (TODO). Manages jobs \u00b6 The Job Manager module in the Meta Service is responsible for the creation, queuing, querying and deletion of jobs.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#meta_service","text":"This topic describes the architecture and functions of the Meta Service.","title":"Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#the_architecture_of_the_meta_service","text":"The architecture of the Meta Service is as follows. The Meta Service is run by the nebula-metad processes. You can deploy nebula-metad processes according to the scenario: In a test environment, you can deploy one or three nebula-metad processes on different machines or a single machine. In a production environment, we recommend that you deploy three processes on different machines for high availability. All the nebula-metad processes form a Raft-based cluster, with one process as the leader and the others as the followers. The leader is elected by quorum, and only the leader can provide service to the clients and other components of Nebula Graph. The followers run in a standby way and each has a data replication of the leader. Once the leader fails, one of the followers will be elected as the new leader.","title":"The architecture of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#functions_of_the_meta_service","text":"","title":"Functions of the Meta Service"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_user_accounts","text":"The Meta Service stores the information of user accounts and the privileges granted to the accounts. When the clients send queries to the Graph Service through an account, the Graph Service checks the account information and whether the account has the right privileges to execute the queries or not. For more information on Nebula Graph access control, see Authentication and authorization (TODO).","title":"Manages user accounts"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_partitions","text":"The Meta Service stores and manages the locations of the storage partitions and helps balance the partitions.","title":"Manages partitions"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_graph_spaces","text":"Nebula Graph supports multiple graph spaces. Data stored in different graph spaces are securely isolated. The Meta Service stores the metadata of all graph spaces and tracks the changes of them, such as adding or dropping a graph space.","title":"Manages graph spaces"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_schema_information","text":"Nebula Graph is a strong-typed graph database. Its schema contains tags (i.e., the vertex types), edge types, tag properties, and edge type properties. The Meta Service stores the schema information. Besides, it performs the addition, modification, and deletion of the schema, and logs the versions of them. For more information on Nebula Graph schema, see Nebula Graph schema and data modeling (TODO).","title":"Manages schema information"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_ttl-based_data_eviction","text":"The Meta Service provides automatic data eviction and space reclamation based on TTL (time to live) options for Nebula Graph. For more information on TTL, see TTL options (TODO).","title":"Manages TTL-based data eviction"},{"location":"1.introduction/3.nebula-graph-architecture/2.meta-service/#manages_jobs","text":"The Job Manager module in the Meta Service is responsible for the creation, queuing, querying and deletion of jobs.","title":"Manages jobs"},{"location":"2.quick-start/1.quick-start-workflow/","text":"Quick start workflow \u00b6 The quick start introduces the simplest workflow to using Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD. Deploy Nebula Graph with Docker Compose Connect to Nebula Graph CRUD in Nebula Graph Other frequently read topics are recommended as follows. They are not in the quick start, but you may need them as soon as you pass the quick start phase. Deploy a Nebula Graph cluster (doc TODO) Compaction and job management","title":"Quick start workflow"},{"location":"2.quick-start/1.quick-start-workflow/#quick_start_workflow","text":"The quick start introduces the simplest workflow to using Nebula Graph, including deploying Nebula Graph, connecting to Nebula Graph, and doing basic CRUD. Deploy Nebula Graph with Docker Compose Connect to Nebula Graph CRUD in Nebula Graph Other frequently read topics are recommended as follows. They are not in the quick start, but you may need them as soon as you pass the quick start phase. Deploy a Nebula Graph cluster (doc TODO) Compaction and job management","title":"Quick start workflow"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/","text":"Deploy Nebula Graph with Docker Compose \u00b6 There are multiple ways to deploy Nebula Graph, but using Docker Compose is usually considered to be a fast starter. Reading guide \u00b6 If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to do before deploying Nebula Graph? How to fast deploy Nebula Graph with Docker Compose? How to check the status and ports of the Nebula Graph services? How to check the data and logs of the Nebula Graph services? How to stop the Nebula Graph services? What are the other ways to install Nebula Graph? Prerequisites \u00b6 You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For a detailed instruction, see Docker document: Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues\uff0cback up the service data if you need, and delete the nebula-docker-compose/data directory. How to deploy \u00b6 Clone the master branch of the nebula-docker-compose repository to your host with Git. DON'T : The master branch contains the Docker Compose solution for the latest Nebula Graph development release. DON'T use this release for production. $ git clone https://github.com/vesoft-inc/nebula-docker-compose.git Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. $ docker-compose up -d The following information indicates the services have started: Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done NOTE : For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. # Start a Nebula Graph service in a Docker container $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v2-nightly # Connect to Nebula Graph with Nebula Console docker> nebula-console -u <user> -p <password> --address = graphd --port = 3699 Check the Nebula Graph service status and port \u00b6 Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32776->13000/tcp, 0 .0.0.0:32772->13002/tcp, 0 .0.0.0:32780->3699/tcp nebula-docker-compose_graphd2_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32769->13000/tcp, 0 .0.0.0:32768->13002/tcp, 0 .0.0.0:32773->3699/tcp nebula-docker-compose_graphd_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32791->13000/tcp, 0 .0.0.0:32788->13002/tcp, 0 .0.0.0:32794->3699/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32793->11000/tcp, 0 .0.0.0:32790->11002/tcp, 0 .0.0.0:32787->45500/tcp, 45501 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32792->11000/tcp, 0 .0.0.0:32789->11002/tcp, 0 .0.0.0:32786->45500/tcp, 45501 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32785->11000/tcp, 0 .0.0.0:32784->11002/tcp, 0 .0.0.0:32782->45500/tcp, 45501 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32777->12000/tcp, 0 .0.0.0:32774->12002/tcp, 0 .0.0.0:32770->44500/tcp, 44501 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32778->12000/tcp, 0 .0.0.0:32775->12002/tcp, 0 .0.0.0:32771->44500/tcp, 44501 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32783->12000/tcp, 0 .0.0.0:32781->12002/tcp, 0 .0.0.0:32779->44500/tcp, 44501 /tcp Nebula Graph provides services to the clients through port 3699 by default. You can adjust the port number by modifying the network configurations . If the Nebula Graph clients work outside the docker container, you must use the source port of any nebula-graphd process for connection. For example, you can use port 32780 according to the preceding return message. The port number changes every time the process restarts. To check the machine status and partition distribution, run the SHOW HOSTS statement. Check the service data and logs \u00b6 All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2 Stop the Nebula Graph services \u00b6 You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing network nebula-docker-compose_nebula-net Other ways to install Nebula Graph \u00b6 Use Source Code Use Docker (doc TODO) Use .rpm or .deb Files (doc TODO) FAQ \u00b6 How to update the docker images of Nebula Graph services \u00b6 To update the images of the Graph Service, Storage Service, and Meta Service, run docker-compose pull in the nebula-docker-compose directory.","title":"Deploy Nebula\u00a0Graph with Docker Compose"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#deploy_nebula_graph_with_docker_compose","text":"There are multiple ways to deploy Nebula Graph, but using Docker Compose is usually considered to be a fast starter.","title":"Deploy Nebula Graph with Docker Compose"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#reading_guide","text":"If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to do before deploying Nebula Graph? How to fast deploy Nebula Graph with Docker Compose? How to check the status and ports of the Nebula Graph services? How to check the data and logs of the Nebula Graph services? How to stop the Nebula Graph services? What are the other ways to install Nebula Graph?","title":"Reading guide"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#prerequisites","text":"You have installed the following applications on your host. Application Recommended version Official installation reference Docker Latest Install Docker Engine Docker Compose Latest Install Docker Compose Git Latest Download Git If you are deploying Nebula Graph as a non-root user, grant the user with Docker-related privileges. For a detailed instruction, see Docker document: Manage Docker as a non-root user . You have started the Docker service on your host. If you have already deployed another version of Nebula Graph with Docker Compose on your host, to avoid compatibility issues\uff0cback up the service data if you need, and delete the nebula-docker-compose/data directory.","title":"Prerequisites"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_deploy","text":"Clone the master branch of the nebula-docker-compose repository to your host with Git. DON'T : The master branch contains the Docker Compose solution for the latest Nebula Graph development release. DON'T use this release for production. $ git clone https://github.com/vesoft-inc/nebula-docker-compose.git Go to the nebula-docker-compose directory. $ cd nebula-docker-compose/ Run the following command to start all the Nebula Graph services. $ docker-compose up -d The following information indicates the services have started: Creating network \"nebula-docker-compose_nebula-net\" with the default driver Creating nebula-docker-compose_metad1_1 ... done Creating nebula-docker-compose_metad2_1 ... done Creating nebula-docker-compose_metad0_1 ... done Creating nebula-docker-compose_storaged2_1 ... done Creating nebula-docker-compose_graphd2_1 ... done Creating nebula-docker-compose_storaged1_1 ... done Creating nebula-docker-compose_graphd_1 ... done Creating nebula-docker-compose_graphd1_1 ... done Creating nebula-docker-compose_storaged0_1 ... done NOTE : For more information of the preceding services, see Nebula Graph architecture . Connect to Nebula Graph. # Start a Nebula Graph service in a Docker container $ docker run --rm -ti --network nebula-docker-compose_nebula-net --entrypoint = /bin/sh vesoft/nebula-console:v2-nightly # Connect to Nebula Graph with Nebula Console docker> nebula-console -u <user> -p <password> --address = graphd --port = 3699","title":"How to deploy"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#check_the_nebula_graph_service_status_and_port","text":"Run docker-compose ps to list all the services of Nebula Graph and their status and ports. $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- nebula-docker-compose_graphd1_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32776->13000/tcp, 0 .0.0.0:32772->13002/tcp, 0 .0.0.0:32780->3699/tcp nebula-docker-compose_graphd2_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32769->13000/tcp, 0 .0.0.0:32768->13002/tcp, 0 .0.0.0:32773->3699/tcp nebula-docker-compose_graphd_1 ./bin/nebula-graphd --flag ... Up ( healthy ) 0 .0.0.0:32791->13000/tcp, 0 .0.0.0:32788->13002/tcp, 0 .0.0.0:32794->3699/tcp nebula-docker-compose_metad0_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32793->11000/tcp, 0 .0.0.0:32790->11002/tcp, 0 .0.0.0:32787->45500/tcp, 45501 /tcp nebula-docker-compose_metad1_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32792->11000/tcp, 0 .0.0.0:32789->11002/tcp, 0 .0.0.0:32786->45500/tcp, 45501 /tcp nebula-docker-compose_metad2_1 ./bin/nebula-metad --flagf ... Up ( healthy ) 0 .0.0.0:32785->11000/tcp, 0 .0.0.0:32784->11002/tcp, 0 .0.0.0:32782->45500/tcp, 45501 /tcp nebula-docker-compose_storaged0_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32777->12000/tcp, 0 .0.0.0:32774->12002/tcp, 0 .0.0.0:32770->44500/tcp, 44501 /tcp nebula-docker-compose_storaged1_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32778->12000/tcp, 0 .0.0.0:32775->12002/tcp, 0 .0.0.0:32771->44500/tcp, 44501 /tcp nebula-docker-compose_storaged2_1 ./bin/nebula-storaged --fl ... Up ( healthy ) 0 .0.0.0:32783->12000/tcp, 0 .0.0.0:32781->12002/tcp, 0 .0.0.0:32779->44500/tcp, 44501 /tcp Nebula Graph provides services to the clients through port 3699 by default. You can adjust the port number by modifying the network configurations . If the Nebula Graph clients work outside the docker container, you must use the source port of any nebula-graphd process for connection. For example, you can use port 32780 according to the preceding return message. The port number changes every time the process restarts. To check the machine status and partition distribution, run the SHOW HOSTS statement.","title":"Check the Nebula Graph service status and port"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#check_the_service_data_and_logs","text":"All the data and logs of Nebula Graph are stored persistently in the nebula-docker-compose/data and nebula-docker-compose/logs directories. The structure of the directories is as follows: nebula-docker-compose/ |-- docker-compose.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 meta0 \u2502 \u251c\u2500\u2500 meta1 \u2502 \u251c\u2500\u2500 meta2 \u2502 \u251c\u2500\u2500 storage0 \u2502 \u251c\u2500\u2500 storage1 \u2502 \u2514\u2500\u2500 storage2 \u2514\u2500\u2500 logs \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph1 \u251c\u2500\u2500 graph2 \u251c\u2500\u2500 meta0 \u251c\u2500\u2500 meta1 \u251c\u2500\u2500 meta2 \u251c\u2500\u2500 storage0 \u251c\u2500\u2500 storage1 \u2514\u2500\u2500 storage2","title":"Check the service data and logs"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#stop_the_nebula_graph_services","text":"You can run the following command to stop the Nebula Graph services: $ docker-compose down The following information indicates you have successfully stopped the Nebula Graph services: Stopping nebula-docker-compose_storaged0_1 ... done Stopping nebula-docker-compose_graphd1_1 ... done Stopping nebula-docker-compose_graphd_1 ... done Stopping nebula-docker-compose_storaged1_1 ... done Stopping nebula-docker-compose_graphd2_1 ... done Stopping nebula-docker-compose_storaged2_1 ... done Stopping nebula-docker-compose_metad0_1 ... done Stopping nebula-docker-compose_metad2_1 ... done Stopping nebula-docker-compose_metad1_1 ... done Removing nebula-docker-compose_storaged0_1 ... done Removing nebula-docker-compose_graphd1_1 ... done Removing nebula-docker-compose_graphd_1 ... done Removing nebula-docker-compose_storaged1_1 ... done Removing nebula-docker-compose_graphd2_1 ... done Removing nebula-docker-compose_storaged2_1 ... done Removing nebula-docker-compose_metad0_1 ... done Removing nebula-docker-compose_metad2_1 ... done Removing nebula-docker-compose_metad1_1 ... done Removing network nebula-docker-compose_nebula-net","title":"Stop the Nebula Graph services"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#other_ways_to_install_nebula_graph","text":"Use Source Code Use Docker (doc TODO) Use .rpm or .deb Files (doc TODO)","title":"Other ways to install Nebula Graph"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/2.deploy-nebula-graph-with-docker-compose/#how_to_update_the_docker_images_of_nebula_graph_services","text":"To update the images of the Graph Service, Storage Service, and Meta Service, run docker-compose pull in the nebula-docker-compose directory.","title":"How to update the docker images of Nebula Graph services"},{"location":"2.quick-start/3.connect-to-nebula-graph/","text":"Connect to Nebula Graph \u00b6 Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. This topic provides an overview of Nebula Graph clients and basic instructions on how to use the native CLI client, nebula-console. Nebula Graph clients \u00b6 Client Introduction Connection guide nebula-console The native command-line interface of Nebula Graph Connect to Nebula Graph with nebula-console Nebula Graph Studio The official graphical user interface for Nebula Graph Connect to Nebula Graph with Nebula Graph Studio nebula-go The official Golang client of Nebula Graph nebula-go code example nebula-python The official Python client of Nebula Graph How to use nebula-python in your code nebula-java The official Python client of Nebula Graph Graph Client Example If you don't have a Nebula Graph database yet, we recommend that you try the cloud service. Nebula Graph Cloud Service supports on-demand deployment and fast building, and uses Nebula Graph Studio as its default client. Use Nebula Console to connect to Nebula Graph \u00b6 Prerequisites \u00b6 You have started the Nebula Graph services. For how to start the services, see Start and Stop Nebula Graph services (doc TODO). The machine you plan to run nebula-console on has network access to the Nebula Graph services. Steps \u00b6 On the nebula-console Releases page, select a nebula-console version and click Assets . NOTE: We recommend that you select the latest release. In the Assets area, find the correct binary file for the machine where you want to run nebula-console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. NOTE: For Windows, rename the file to nebula-console.exe . On the machine to run nebula-console, grant the execute permission of the nebula-console binary file to the user. NOTE: For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. * For Linux or macOS: ```bash $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [-t 120] [-e \"nGQL_statement\" | -f filename.nGQL] ``` * For Windows: ```powershell > nebula-console.exe -addr <ip> -port <port> -u <username> -p <password> [-t 120] [-e \"nGQL_statement\" | -f filename.nGQL] ``` The description of the parameters is as follows. Option Description -h Shows the help menu. -addr Sets the IP address of the graphd service. The default address is 127.0.0.1. -port Sets the port number of the graphd service. The default port number is 3699. If you have deployed Nebula Graph in a docker container but nebula-console is working outside the container, check the source port of any nebula-graphd process and use it for connection. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any characters as the username. -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. You'll get the return messages and the connection stops then. Nebula-console export mode \u00b6 When the export mode is enabled, nebula-console exports all the query results into a CSV file. When the export mode is disabled, the export stops. The syntax is as follows. NOTE : The following commands are case insensitive. Enable nebula-console export mode: nebula> :SET CSV <your_file.csv>; Disable nebula-console export mode: nebula> :UNSET CSV; Disconnect nebula-console from Nebula Graph \u00b6 You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, nebula-console supports using these commands in lower case without the colon (\":\"), such as quit . nebula> :QUIT Bye root! FAQ \u00b6 How can I install nebula-console from the source code \u00b6 To download and compile the latest source code of nebula-console, follow the instructions on the nebula console GitHub page .","title":"Connect to Nebula\u00a0Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#connect_to_nebula_graph","text":"Nebula Graph supports multiple types of clients, including a CLI client, a GUI client, and clients developed in popular programming languages. This topic provides an overview of Nebula Graph clients and basic instructions on how to use the native CLI client, nebula-console.","title":"Connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#nebula_graph_clients","text":"Client Introduction Connection guide nebula-console The native command-line interface of Nebula Graph Connect to Nebula Graph with nebula-console Nebula Graph Studio The official graphical user interface for Nebula Graph Connect to Nebula Graph with Nebula Graph Studio nebula-go The official Golang client of Nebula Graph nebula-go code example nebula-python The official Python client of Nebula Graph How to use nebula-python in your code nebula-java The official Python client of Nebula Graph Graph Client Example If you don't have a Nebula Graph database yet, we recommend that you try the cloud service. Nebula Graph Cloud Service supports on-demand deployment and fast building, and uses Nebula Graph Studio as its default client.","title":"Nebula Graph clients"},{"location":"2.quick-start/3.connect-to-nebula-graph/#use_nebula_console_to_connect_to_nebula_graph","text":"","title":"Use Nebula Console to connect to Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#prerequisites","text":"You have started the Nebula Graph services. For how to start the services, see Start and Stop Nebula Graph services (doc TODO). The machine you plan to run nebula-console on has network access to the Nebula Graph services.","title":"Prerequisites"},{"location":"2.quick-start/3.connect-to-nebula-graph/#steps","text":"On the nebula-console Releases page, select a nebula-console version and click Assets . NOTE: We recommend that you select the latest release. In the Assets area, find the correct binary file for the machine where you want to run nebula-console and download the file to the machine. (Optional) Rename the binary file to nebula-console for convenience. NOTE: For Windows, rename the file to nebula-console.exe . On the machine to run nebula-console, grant the execute permission of the nebula-console binary file to the user. NOTE: For Windows, skip this step. $ chmod 111 nebula-console In the command line interface, change the working directory to the one where the nebula-console binary file is stored. Run the following command to connect to Nebula Graph. * For Linux or macOS: ```bash $ ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [-t 120] [-e \"nGQL_statement\" | -f filename.nGQL] ``` * For Windows: ```powershell > nebula-console.exe -addr <ip> -port <port> -u <username> -p <password> [-t 120] [-e \"nGQL_statement\" | -f filename.nGQL] ``` The description of the parameters is as follows. Option Description -h Shows the help menu. -addr Sets the IP address of the graphd service. The default address is 127.0.0.1. -port Sets the port number of the graphd service. The default port number is 3699. If you have deployed Nebula Graph in a docker container but nebula-console is working outside the container, check the source port of any nebula-graphd process and use it for connection. -u/-user Sets the username of your Nebula Graph account. Before enabling authentication, you can use any characters as the username. -p/-password Sets the password of your Nebula Graph account. Before enabling authentication, you can use any characters as the password. -t/-timeout Sets an integer-type timeout threshold of the connection. The unit is second. The default value is 120. -e/-eval Sets a string-type nGQL statement. The nGQL statement is executed once the connection succeeds. The connection stops after the result is returned. -f/-file Sets the path of an nGQL file. The nGQL statements in the file are executed once the connection succeeds. You'll get the return messages and the connection stops then.","title":"Steps"},{"location":"2.quick-start/3.connect-to-nebula-graph/#nebula-console_export_mode","text":"When the export mode is enabled, nebula-console exports all the query results into a CSV file. When the export mode is disabled, the export stops. The syntax is as follows. NOTE : The following commands are case insensitive. Enable nebula-console export mode: nebula> :SET CSV <your_file.csv>; Disable nebula-console export mode: nebula> :UNSET CSV;","title":"Nebula-console export mode"},{"location":"2.quick-start/3.connect-to-nebula-graph/#disconnect_nebula-console_from_nebula_graph","text":"You can use :EXIT or :QUIT to disconnect from Nebula Graph. For convenience, nebula-console supports using these commands in lower case without the colon (\":\"), such as quit . nebula> :QUIT Bye root!","title":"Disconnect nebula-console from Nebula Graph"},{"location":"2.quick-start/3.connect-to-nebula-graph/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/3.connect-to-nebula-graph/#how_can_i_install_nebula-console_from_the_source_code","text":"To download and compile the latest source code of nebula-console, follow the instructions on the nebula console GitHub page .","title":"How can I install nebula-console from the source code"},{"location":"2.quick-start/4.nebula-graph-crud/","text":"Nebula Graph CRUD \u00b6 This topic describes the basic CRUD operations in Nebula Graph. Graph space and Nebula Graph schema \u00b6 A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have one or more tags. Tag The type of a vertex. It defines a group of properties that describes a type of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes a type of edges. For more information, see Nebula Graph schema (doc TODO). In this topic, we use the following dataset to demonstrate basic CRUD operations. Check the machine status in the Nebula Graph cluster \u00b6 First, we recommend that you check the machine status to make sure that all the Storage services are connected to the Meta Services. Run SHOW HOSTS as follows. (root@nebula) [(none)]> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ Got 3 rows (time spent 1775/2334 us) From the Status column of the table in the return message, you can see that all the Storage services are online. Create and use a graph space \u00b6 nGQL syntax \u00b6 Create a graph space: CREATE SPACE <graph_space_name> [(partition_num=<partition number>, replica_factor=<replica number>), vid_type=fixed_string(<string_number>))] Property Description partition_num Specifies the number of partitions in each replica. The suggested number is the number of hard disks in the cluster times 5. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the Nebula Graph cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must always be an odd number for the need of quorum-based voting. vid_type To insert a vertex with a string-typed VID, set the fixed_string length for the maximum VID length. The default fixed_string is 8. Note: fix_string(N) is similar to varchar(N) in SQL. List graph spaces: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name> Examples \u00b6 Use the following statement to create a graph space named nba . nebula> CREATE SPACE nba(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Execution succeeded (time spent 2817/3280 us) Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ Got 3 rows (time spent 744/1123 us) If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE (doc TODO). Use the nba graph space. nebula> USE nba; Execution succeeded (time spent 1322/2206 us) You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) Create tags and edge types \u00b6 nGQL syntax \u00b6 CREATE {TAG | EDGE} {<tag_name> | <edge_type>}(<property_name> <data_type> [, <property_name> <data_type> ...]); Examples \u00b6 Create tags player and team , edge types follow and serve . Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); Execution succeeded (time spent 2694/3116 us) Thu, 15 Oct 2020 06:22:29 UTC nebula> CREATE TAG team(name string); Execution succeeded (time spent 2630/3002 us) Thu, 15 Oct 2020 06:22:37 UTC nebula> CREATE EDGE follow(degree int); Execution succeeded (time spent 3087/3467 us) Thu, 15 Oct 2020 06:22:43 UTC nebula> CREATE EDGE serve(start_year int, end_year int); Execution succeeded (time spent 2645/3123 us) Thu, 15 Oct 2020 06:22:50 UTC Insert vertices and edges \u00b6 You can use the INSERT statement to insert vertices or edges based on existing tags or edge types. nGQL syntax \u00b6 Insert vertices: INSERT VERTEX <tag_name> (<property_name>[, <property_name>...]) [, <tag_name> (<property_name>[, <property_name>...]), ...] {VALUES | VALUE} <vid>: (<property_value>[, <property_value>...]) [, <vid>: (<property_value>[, <property_value>...]; VID is short for vertex ID. A VID must be a unique string value in a graph space. Insert edges: INSERT EDGE <edge_type> (<property_name>[, <property_name>...]) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : (<property_value>[, <property_value>...]) [, <src_vid> -> <dst_vid>[@<rank> : (<property_name>[, <property_name>...]), ...] Examples \u00b6 Insert vertices representing NBA players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); Execution succeeded (time spent 2919/3485 us) Fri, 16 Oct 2020 03:41:00 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 3007/3539 us) Fri, 16 Oct 2020 03:41:58 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Execution succeeded (time spent 2449/2934 us) Fri, 16 Oct 2020 03:42:16 UTC nebula> INSERT VERTEX team(name) VALUES \"team200\":(\"Warriors\"), \"team201\":(\"Nuggets\"); Execution succeeded (time spent 3514/4331 us) Fri, 16 Oct 2020 03:42:45 UTC Insert edges representing the relations between NBA players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player101\":(95); Execution succeeded (time spent 1488/1918 us) Wed, 21 Oct 2020 06:57:32 UTC nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player102\":(90); Execution succeeded (time spent 2483/2890 us) Wed, 21 Oct 2020 07:05:48 UTC nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player101\":(75); Execution succeeded (time spent 1208/1689 us) Wed, 21 Oct 2020 07:07:12 UTC nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016), \"player101\" -> \"team201\":(1999, 2018); Execution succeeded (time spent 2170/2651 us) Wed, 21 Oct 2020 07:08:59 UTC Read data \u00b6 The GO statement traverses the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and return information in a form specified in the YIELD clause. FETCH is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. MATCH is the most commonly used statement for graph data querying. It relies on native indexes and patterns to match data stored in Nebula Graph. > CAUTION: Improper use of indexes can reduce the writing performance by 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. nGQL syntax \u00b6 MATCH MATCH <pattern> [<WHERE clause>] RETURN <output> GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [WHERE <expression> [AND | OR expression ...])] YIELD [DISTINCT] <return_list> FETCH Fetch properties on tags: FETCH PROP ON {<tag_name> | <tag_name_list> | *} <vid_list> [YIELD [DISTINCT] <return_list>] Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD [DISTINCT] <return_list>] LOOKUP LOOKUP ON {<tag_name> | <edge_type>} WHERE <expression> [AND expression ...])] [YIELD <return_list>] Examples of GO \u00b6 Find the vertices that VID \"player100\" follows. nebula> GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | player101 | +-------------+ | player102 | +-------------+ Got 2 rows (time spent 1935/2420 us) Search for the players that the player with VID \"player100\" follows. Filter the players that the player with VID \"player100\" follows whose age is equal to or greater than 35. Rename the columns in the result with Teammate and Age . nebula> GO FROM \"player100\" OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; +-------------+-----+ | Teammate | Age | +-------------+-----+ | Tony Parker | 36 | +-------------+-----+ Got 1 rows (time spent 3871/4349 us) Clause/Sign Description YIELD Specifies what values or results you want to return from the query. $$ Represents the target vertices. \\ A line-breaker. Search for the players that the player with VID \"player100\" follows. Then Retrieve the teams of the players that the player with VID \"player100\" follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 2902/3496 us) Clause/Sign Description $^ Represents the source vertex of the edge. | A pipe symbol that can combine multiple queries. $- Represents the output of the query before the pipe symbol. With a temporary variable: NOTE : Once a compound statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 3103/3711 us) Example of FETCH \u00b6 Use FETCH : Fetch the properties of the player with VID player100. nebula> FETCH PROP ON player \"player100\"; +----------+-------------+------------+ | VertexID | player.name | player.age | +----------+-------------+------------+ | player100 | Tim Duncan | 42 | +----------+-------------+------------+ Got 1 rows (time spent 2006/2406 us) Examples of LOOKUP and MATCH \u00b6 Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Indexes can reduce the writing performance by 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Find the information of the vertex with the tag player and its value of the name property is \"Tony Parker\" . // Create an index on the player name property. CREATE TAG INDEX player_name_0 on player(name(10)) Execution succeeded (time spent 3465/4150 us) // Rebuild the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_name_0 +------------+ | New Job Id | +------------+ | 31 | +------------+ Got 1 rows (time spent 2379/3033 us) // Use LOOKUP to retrieve the vertex information. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; +-------------+---------------+------------+ | VertexID | player.name | player.age | +-------------+---------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+---------------+------------+ // Use MATCH to retrieve the vertex information. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ Got 1 rows (time spent 5132/6246 us) Update vertices and edges \u00b6 You can use the UPDATE statement or the UPSERT statement to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , it inserts a new vertex or edge if it does not exist. nGQL syntax \u00b6 UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>] UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>] UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>] Examples \u00b6 UPDATE the name property of the vertex with VID \"player100\" and check the result with the FETCH statement: nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; Execution succeeded (time spent 3483/3914 us) Wed, 21 Oct 2020 10:53:14 UTC nebula> FETCH PROP ON player \"player100\"; +----------+-------------+------------+ | VertexID | player.name | player.age | +----------+-------------+------------+ | player100 | Tim | 42 | +----------+-------------+------------+ Got 1 rows (time spent 2463/3042 us) UPDATE the degree value of an edge and check the result with the FETCH statement: nebula> UPDATE EDGE \"player100\" -> \"player101\" OF follow SET degree = 96; Execution succeeded (time spent 3932/4432 us) nebula> FETCH PROP ON follow \"player100\" -> \"player101\"; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | player100 | player101 | 0 | 96 | +-------------+-------------+--------------+---------------+ Got 1 rows (time spent 2205/2800 us) Insert a vertex with VID \"player111\" and UPSERT it. nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); Execution succeeded (time spent 2115/2900 us) Wed, 21 Oct 2020 11:11:50 UTC nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | Dwight Howard | 33 | +---------------+-----+ Got 1 rows (time spent 1815/2329 us) Delete vertices and edges \u00b6 nGQL syntax \u00b6 Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...] Examples \u00b6 Delete vertices: nebula> DELETE VERTEX \"team1\", \"team2\"; Execution succeeded (time spent 4337/4782 us) Delete edges: nebula> DELETE EDGE follow \"team1\" -> \"team2\"; Execution succeeded (time spent 3700/4101 us) About indexes \u00b6 You can add indexes to tags or edge types with the CREATE INDEX statement. Must-read for using index \u00b6 Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Rebuild indexes for pre-existing data. Otherwise, the pre-existing data can't be indexed. For more information, see REBUILD INDEX . nGQL syntax \u00b6 Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list); Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name> Examples \u00b6 Create an index for the name property on all vertices with the tag player . nebula> CREATE TAG INDEX player_index_0 on player(name); nebula> REBUILD TAG INDEX player_index_0; FAQ \u00b6 How is the time spent value at the end of each return message calculated? \u00b6 Take the return message of SHOW SPACES as an example: nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen.","title":"Nebula\u00a0Graph CRUD"},{"location":"2.quick-start/4.nebula-graph-crud/#nebula_graph_crud","text":"This topic describes the basic CRUD operations in Nebula Graph.","title":"Nebula Graph CRUD"},{"location":"2.quick-start/4.nebula-graph-crud/#graph_space_and_nebula_graph_schema","text":"A Nebula Graph instance consists of one or more graph spaces. Graph spaces are physically isolated from each other. You can use different graph spaces in the same instance to store different datasets. To insert data into a graph space, define a schema for the graph database. Nebula Graph schema is based on the following components. Schema component Description Vertex Represents an entity in the real world. A vertex can have one or more tags. Tag The type of a vertex. It defines a group of properties that describes a type of vertices. Edge Represents a directed relationship between two vertices. Edge type The type of an edge. It defines a group of properties that describes a type of edges. For more information, see Nebula Graph schema (doc TODO). In this topic, we use the following dataset to demonstrate basic CRUD operations.","title":"Graph space and Nebula Graph schema"},{"location":"2.quick-start/4.nebula-graph-crud/#check_the_machine_status_in_the_nebula_graph_cluster","text":"First, we recommend that you check the machine status to make sure that all the Storage services are connected to the Meta Services. Run SHOW HOSTS as follows. (root@nebula) [(none)]> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 0 | No valid partition | No valid partition | +-----------+-------+--------+--------------+---------------------+------------------------+ Got 3 rows (time spent 1775/2334 us) From the Status column of the table in the return message, you can see that all the Storage services are online.","title":"Check the machine status in the Nebula Graph cluster"},{"location":"2.quick-start/4.nebula-graph-crud/#create_and_use_a_graph_space","text":"","title":"Create and use a graph space"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax","text":"Create a graph space: CREATE SPACE <graph_space_name> [(partition_num=<partition number>, replica_factor=<replica number>), vid_type=fixed_string(<string_number>))] Property Description partition_num Specifies the number of partitions in each replica. The suggested number is the number of hard disks in the cluster times 5. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the Nebula Graph cluster. The suggested number is 3 in a production environment and 1 in a test environment. The replica number must always be an odd number for the need of quorum-based voting. vid_type To insert a vertex with a string-typed VID, set the fixed_string length for the maximum VID length. The default fixed_string is 8. Note: fix_string(N) is similar to varchar(N) in SQL. List graph spaces: nebula> SHOW SPACES; Use a graph space: USE <graph_space_name>","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples","text":"Use the following statement to create a graph space named nba . nebula> CREATE SPACE nba(partition_num=15, replica_factor=1, vid_type=fixed_string(30)); Execution succeeded (time spent 2817/3280 us) Check the partition distribution with SHOW HOSTS to make sure that the partitions are distributed in a balanced way. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ Got 3 rows (time spent 744/1123 us) If the Leader distribution is uneven, use BALANCE LEADER to redistribute the partitions. For more information, see BALANCE (doc TODO). Use the nba graph space. nebula> USE nba; Execution succeeded (time spent 1322/2206 us) You can use SHOW SPACES to check the graph space you created. nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#create_tags_and_edge_types","text":"","title":"Create tags and edge types"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_1","text":"CREATE {TAG | EDGE} {<tag_name> | <edge_type>}(<property_name> <data_type> [, <property_name> <data_type> ...]);","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_1","text":"Create tags player and team , edge types follow and serve . Component name Type Property player Tag name (string), age (int) team Tag name (string) follow Edge type degree (int) serve Edge type start_year (int), end_year (int) nebula> CREATE TAG player(name string, age int); Execution succeeded (time spent 2694/3116 us) Thu, 15 Oct 2020 06:22:29 UTC nebula> CREATE TAG team(name string); Execution succeeded (time spent 2630/3002 us) Thu, 15 Oct 2020 06:22:37 UTC nebula> CREATE EDGE follow(degree int); Execution succeeded (time spent 3087/3467 us) Thu, 15 Oct 2020 06:22:43 UTC nebula> CREATE EDGE serve(start_year int, end_year int); Execution succeeded (time spent 2645/3123 us) Thu, 15 Oct 2020 06:22:50 UTC","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#insert_vertices_and_edges","text":"You can use the INSERT statement to insert vertices or edges based on existing tags or edge types.","title":"Insert vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_2","text":"Insert vertices: INSERT VERTEX <tag_name> (<property_name>[, <property_name>...]) [, <tag_name> (<property_name>[, <property_name>...]), ...] {VALUES | VALUE} <vid>: (<property_value>[, <property_value>...]) [, <vid>: (<property_value>[, <property_value>...]; VID is short for vertex ID. A VID must be a unique string value in a graph space. Insert edges: INSERT EDGE <edge_type> (<property_name>[, <property_name>...]) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : (<property_value>[, <property_value>...]) [, <src_vid> -> <dst_vid>[@<rank> : (<property_name>[, <property_name>...]), ...]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_2","text":"Insert vertices representing NBA players and teams: nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); Execution succeeded (time spent 2919/3485 us) Fri, 16 Oct 2020 03:41:00 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 3007/3539 us) Fri, 16 Oct 2020 03:41:58 UTC nebula> INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); Execution succeeded (time spent 2449/2934 us) Fri, 16 Oct 2020 03:42:16 UTC nebula> INSERT VERTEX team(name) VALUES \"team200\":(\"Warriors\"), \"team201\":(\"Nuggets\"); Execution succeeded (time spent 3514/4331 us) Fri, 16 Oct 2020 03:42:45 UTC Insert edges representing the relations between NBA players and teams: nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player101\":(95); Execution succeeded (time spent 1488/1918 us) Wed, 21 Oct 2020 06:57:32 UTC nebula> INSERT EDGE follow(degree) VALUES \"player100\" -> \"player102\":(90); Execution succeeded (time spent 2483/2890 us) Wed, 21 Oct 2020 07:05:48 UTC nebula> INSERT EDGE follow(degree) VALUES \"player102\" -> \"player101\":(75); Execution succeeded (time spent 1208/1689 us) Wed, 21 Oct 2020 07:07:12 UTC nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016), \"player101\" -> \"team201\":(1999, 2018); Execution succeeded (time spent 2170/2651 us) Wed, 21 Oct 2020 07:08:59 UTC","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#read_data","text":"The GO statement traverses the database based on specific conditions. A GO traversal starts from one or more vertices, along one or more edges, and return information in a form specified in the YIELD clause. FETCH is used to get properties from vertices or edges. The LOOKUP statement is based on indexes . It is used together with the WHERE clause to search for the data that meet the specific conditions. MATCH is the most commonly used statement for graph data querying. It relies on native indexes and patterns to match data stored in Nebula Graph. > CAUTION: Improper use of indexes can reduce the writing performance by 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service.","title":"Read data"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_3","text":"MATCH MATCH <pattern> [<WHERE clause>] RETURN <output> GO GO [[<M> TO] <N> STEPS ] FROM <vertex_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [WHERE <expression> [AND | OR expression ...])] YIELD [DISTINCT] <return_list> FETCH Fetch properties on tags: FETCH PROP ON {<tag_name> | <tag_name_list> | *} <vid_list> [YIELD [DISTINCT] <return_list>] Fetch properties on edges: FETCH PROP ON <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid> ...] [YIELD [DISTINCT] <return_list>] LOOKUP LOOKUP ON {<tag_name> | <edge_type>} WHERE <expression> [AND expression ...])] [YIELD <return_list>]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_go","text":"Find the vertices that VID \"player100\" follows. nebula> GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | player101 | +-------------+ | player102 | +-------------+ Got 2 rows (time spent 1935/2420 us) Search for the players that the player with VID \"player100\" follows. Filter the players that the player with VID \"player100\" follows whose age is equal to or greater than 35. Rename the columns in the result with Teammate and Age . nebula> GO FROM \"player100\" OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; +-------------+-----+ | Teammate | Age | +-------------+-----+ | Tony Parker | 36 | +-------------+-----+ Got 1 rows (time spent 3871/4349 us) Clause/Sign Description YIELD Specifies what values or results you want to return from the query. $$ Represents the target vertices. \\ A line-breaker. Search for the players that the player with VID \"player100\" follows. Then Retrieve the teams of the players that the player with VID \"player100\" follows. To combine the two queries, use a pipe or a temporary variable. With a pipe: nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 2902/3496 us) Clause/Sign Description $^ Represents the source vertex of the edge. | A pipe symbol that can combine multiple queries. $- Represents the output of the query before the pipe symbol. With a temporary variable: NOTE : Once a compound statement is submitted to the server as a whole, the life cycle of the temporary variables in the statement ends. nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ Got 1 rows (time spent 3103/3711 us)","title":"Examples of GO"},{"location":"2.quick-start/4.nebula-graph-crud/#example_of_fetch","text":"Use FETCH : Fetch the properties of the player with VID player100. nebula> FETCH PROP ON player \"player100\"; +----------+-------------+------------+ | VertexID | player.name | player.age | +----------+-------------+------------+ | player100 | Tim Duncan | 42 | +----------+-------------+------------+ Got 1 rows (time spent 2006/2406 us)","title":"Example of FETCH"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_of_lookup_and_match","text":"Make sure there is an index for LOOKUP or MATCH to use. If there is not, create an index first. Indexes can reduce the writing performance by 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Find the information of the vertex with the tag player and its value of the name property is \"Tony Parker\" . // Create an index on the player name property. CREATE TAG INDEX player_name_0 on player(name(10)) Execution succeeded (time spent 3465/4150 us) // Rebuild the index to make sure it takes effect on pre-existing data. nebula> REBUILD TAG INDEX player_name_0 +------------+ | New Job Id | +------------+ | 31 | +------------+ Got 1 rows (time spent 2379/3033 us) // Use LOOKUP to retrieve the vertex information. nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; +-------------+---------------+------------+ | VertexID | player.name | player.age | +-------------+---------------+------------+ | \"player101\" | \"Tony Parker\" | 36 | +-------------+---------------+------------+ // Use MATCH to retrieve the vertex information. nebula> MATCH (v:player{name:\"Tony Parker\"}) RETURN v; +-----------------------------------------------------+ | v | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ Got 1 rows (time spent 5132/6246 us)","title":"Examples of LOOKUP and MATCH"},{"location":"2.quick-start/4.nebula-graph-crud/#update_vertices_and_edges","text":"You can use the UPDATE statement or the UPSERT statement to update existing data. UPSERT is the combination of UPDATE and INSERT . If you update a vertex or an edge with UPSERT , it inserts a new vertex or edge if it does not exist.","title":"Update vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_4","text":"UPDATE vertices: UPDATE VERTEX <vid> SET <properties to be updated> [WHEN <condition>] [YIELD <columns>] UPDATE edges: UPDATE EDGE <source vid> -> <destination vid> [@rank] OF <edge_type> SET <properties to be updated> [WHEN <condition>] [YIELD <columns to be output>] UPSERT vertices or edges: UPSERT {VERTEX <vid> | EDGE <edge_type>} SET <update_columns> [WHEN <condition>] [YIELD <columns>]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_3","text":"UPDATE the name property of the vertex with VID \"player100\" and check the result with the FETCH statement: nebula> UPDATE VERTEX \"player100\" SET player.name = \"Tim\"; Execution succeeded (time spent 3483/3914 us) Wed, 21 Oct 2020 10:53:14 UTC nebula> FETCH PROP ON player \"player100\"; +----------+-------------+------------+ | VertexID | player.name | player.age | +----------+-------------+------------+ | player100 | Tim | 42 | +----------+-------------+------------+ Got 1 rows (time spent 2463/3042 us) UPDATE the degree value of an edge and check the result with the FETCH statement: nebula> UPDATE EDGE \"player100\" -> \"player101\" OF follow SET degree = 96; Execution succeeded (time spent 3932/4432 us) nebula> FETCH PROP ON follow \"player100\" -> \"player101\"; +-------------+-------------+--------------+---------------+ | follow._src | follow._dst | follow._rank | follow.degree | +-------------+-------------+--------------+---------------+ | player100 | player101 | 0 | 96 | +-------------+-------------+--------------+---------------+ Got 1 rows (time spent 2205/2800 us) Insert a vertex with VID \"player111\" and UPSERT it. nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); Execution succeeded (time spent 2115/2900 us) Wed, 21 Oct 2020 11:11:50 UTC nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 \\ WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 \\ YIELD $^.player.name AS Name, $^.player.age AS Age; +---------------+-----+ | Name | Age | +---------------+-----+ | Dwight Howard | 33 | +---------------+-----+ Got 1 rows (time spent 1815/2329 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#delete_vertices_and_edges","text":"","title":"Delete vertices and edges"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_5","text":"Delete vertices: DELETE VERTEX <vid1>[, <vid2>...] Delete edges: DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <src_vid> -> <dst_vid>...]","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_4","text":"Delete vertices: nebula> DELETE VERTEX \"team1\", \"team2\"; Execution succeeded (time spent 4337/4782 us) Delete edges: nebula> DELETE EDGE follow \"team1\" -> \"team2\"; Execution succeeded (time spent 3700/4101 us)","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#about_indexes","text":"You can add indexes to tags or edge types with the CREATE INDEX statement.","title":"About indexes"},{"location":"2.quick-start/4.nebula-graph-crud/#must-read_for_using_index","text":"Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. Rebuild indexes for pre-existing data. Otherwise, the pre-existing data can't be indexed. For more information, see REBUILD INDEX .","title":"Must-read for using index"},{"location":"2.quick-start/4.nebula-graph-crud/#ngql_syntax_6","text":"Create an index: CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list); Rebuild an index: REBUILD {TAG | EDGE} INDEX <index_name>","title":"nGQL syntax"},{"location":"2.quick-start/4.nebula-graph-crud/#examples_5","text":"Create an index for the name property on all vertices with the tag player . nebula> CREATE TAG INDEX player_index_0 on player(name); nebula> REBUILD TAG INDEX player_index_0;","title":"Examples"},{"location":"2.quick-start/4.nebula-graph-crud/#faq","text":"","title":"FAQ"},{"location":"2.quick-start/4.nebula-graph-crud/#how_is_the_time_spent_value_at_the_end_of_each_return_message_calculated","text":"Take the return message of SHOW SPACES as an example: nebula> SHOW SPACES; +------+ | Name | +------+ | nba | +------+ Got 1 rows (time spent 1235/1934 us) The first number 1235 shows the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the client, fetch the data from the storage server and perform a series of calculations. The second number 1934 shows the time spent from the client's perspective, that is, the time it takes for the client from sending a request, receiving a response, and displaying the result on the screen.","title":"How is the time spent value at the end of each return message calculated?"},{"location":"3.ngql-guide/1.nGQL-overview/","text":"Nebula Graph Query Language (nGQL) \u00b6 This document gives an introduction to the query language of Nebula Graph, nGQL. What is nGQL \u00b6 nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns. nGQL designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Nebula Graph 2.0 or later version support openCypher 9 . What can nGQL do \u00b6 Supports basic graph traverse Supports pattern match Supports aggregation Supports graph mutation Supports distributed transaction (future release) Supports pipe Example Data \u00b6 The example data in Nebula Graph document statements can be downloaded here . After downloading the example data, you can import it to Nebula Graph by using Nebula Graph Studio . Placeholder Identifiers and Values \u00b6 Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, see the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"nGQL overview"},{"location":"3.ngql-guide/1.nGQL-overview/#nebula_graph_query_language_ngql","text":"This document gives an introduction to the query language of Nebula Graph, nGQL.","title":"Nebula Graph Query Language (nGQL)"},{"location":"3.ngql-guide/1.nGQL-overview/#what_is_ngql","text":"nGQL is a declarative graph query language for Nebula Graph. It allows expressive and efficient graph patterns. nGQL designed for both developers and operations professionals. nGQL is an SQL-like query language, so it's easy to learn. nGQL is a project in progress. New features and optimizations are done steadily. There can be differences between syntax and implementation. Nebula Graph 2.0 or later version support openCypher 9 .","title":"What is nGQL"},{"location":"3.ngql-guide/1.nGQL-overview/#what_can_ngql_do","text":"Supports basic graph traverse Supports pattern match Supports aggregation Supports graph mutation Supports distributed transaction (future release) Supports pipe","title":"What can nGQL do"},{"location":"3.ngql-guide/1.nGQL-overview/#example_data","text":"The example data in Nebula Graph document statements can be downloaded here . After downloading the example data, you can import it to Nebula Graph by using Nebula Graph Studio .","title":"Example Data"},{"location":"3.ngql-guide/1.nGQL-overview/#placeholder_identifiers_and_values","text":"Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, see the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"Placeholder Identifiers and Values"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/","text":"INSERT VERTEX \u00b6 INSERT VERTEX <tag_name> (<prop_name_list>) [, <tag_name> (<prop_name_list>), ...] {VALUES | VALUE} VID: (<prop_value_list>[, <prop_value_list>]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] The INSERT VERTEX statement inserts one or more vertices into a graph space in a Nebula Graph instance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.X, string and integer VID types are supported. The VID type is set when a graph space is created. For detail information on the maximum VID length, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . If the property values do not match the data type in the tag, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. (TODO: create tag doc) Examples \u00b6 nebula> CREATE TAG t1(); -- Create tag t1 with no property nebula> INSERT VERTEX t1() VALUE \"10\":(); -- Insert vertex \"10\" with no property nebula> CREATE TAG t2 (name string, age int); -- Create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); -- Insert vertex \"11\" with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); -- Failed. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); -- Insert two vertices nebula> CREATE TAG t3(p1 int); nebula> CREATE TAG t4(p2 string); nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); -- Insert vertex \"21\" with two tags. A vertex can be inserted/written multiple times. Only the last written values can be read. -- insert vertex \"11\" with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); nebula> FETCH PROP ON t2 \"11\"; +----------+---------+--------+ | VertexID | t2.name | t2.age | +----------+---------+--------+ | \"11\" | \"n4\" | 15 | -- Only the last version can be read +----------+---------+--------+ nebula> CREATE TAG t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-8)]: Storage Error: The not null field cannot be null. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); nebula> FETCH PROP ON t5 \"003\"; +----------+-------+-------+-------+ | VertexID | t5.p1 | t5.p2 | t5.p3 | +----------+-------+-------+-------+ | \"003\" | \"cd\" | 5 | NULL | -- The value for p3 is the default NULL. +----------+-------+-------+-------+ nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); nebula> FETCH PROP on t5 \"004\"; +----------+---------+-------+-------+ | VertexID | t5.p1 | t5.p2 | t5.p3 | +----------+---------+-------+-------+ | \"004\" | \"shala\" | 4 | NULL | -- The allowed maximum length for property p1 is 5. +----------+---------+-------+-------+","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#insert_vertex","text":"INSERT VERTEX <tag_name> (<prop_name_list>) [, <tag_name> (<prop_name_list>), ...] {VALUES | VALUE} VID: (<prop_value_list>[, <prop_value_list>]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] The INSERT VERTEX statement inserts one or more vertices into a graph space in a Nebula Graph instance. tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list contains the names of the properties on the tag. VID is the vertex ID. In Nebula Graph 2.X, string and integer VID types are supported. The VID type is set when a graph space is created. For detail information on the maximum VID length, see CREATE SPACE . prop_value_list must provide the property values according to the prop_name_list . If the property values do not match the data type in the tag, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. (TODO: create tag doc)","title":"INSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/1.insert-vertex/#examples","text":"nebula> CREATE TAG t1(); -- Create tag t1 with no property nebula> INSERT VERTEX t1() VALUE \"10\":(); -- Insert vertex \"10\" with no property nebula> CREATE TAG t2 (name string, age int); -- Create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n1\", 12); -- Insert vertex \"11\" with two properties nebula> INSERT VERTEX t2 (name, age) VALUES \"12\":(\"n1\", \"a13\"); -- Failed. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES \"13\":(\"n3\", 12), \"14\":(\"n4\", 8); -- Insert two vertices nebula> CREATE TAG t3(p1 int); nebula> CREATE TAG t4(p2 string); nebula> INSERT VERTEX t3 (p1), t4(p2) VALUES \"21\": (321, \"hello\"); -- Insert vertex \"21\" with two tags. A vertex can be inserted/written multiple times. Only the last written values can be read. -- insert vertex \"11\" with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n2\", 13); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n3\", 14); nebula> INSERT VERTEX t2 (name, age) VALUES \"11\":(\"n4\", 15); nebula> FETCH PROP ON t2 \"11\"; +----------+---------+--------+ | VertexID | t2.name | t2.age | +----------+---------+--------+ | \"11\" | \"n4\" | 15 | -- Only the last version can be read +----------+---------+--------+ nebula> CREATE TAG t5(p1 fixed_string(5) NOT NULL, p2 int, p3 int DEFAULT NULL); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"001\":(\"Abe\", 2, 3); nebula> INSERT VERTEX t5(p1, p2, p3) VALUES \"002\":(NULL, 4, 5); [ERROR (-8)]: Storage Error: The not null field cannot be null. nebula> INSERT VERTEX t5(p1, p2) VALUES \"003\":(\"cd\", 5); nebula> FETCH PROP ON t5 \"003\"; +----------+-------+-------+-------+ | VertexID | t5.p1 | t5.p2 | t5.p3 | +----------+-------+-------+-------+ | \"003\" | \"cd\" | 5 | NULL | -- The value for p3 is the default NULL. +----------+-------+-------+-------+ nebula> INSERT VERTEX t5(p1, p2) VALUES \"004\":(\"shalalalala\", 4); nebula> FETCH PROP on t5 \"004\"; +----------+---------+-------+-------+ | VertexID | t5.p1 | t5.p2 | t5.p3 | +----------+---------+-------+-------+ | \"004\" | \"shala\" | 4 | NULL | -- The allowed maximum length for property p1 is 5. +----------+---------+-------+-------+","title":"Examples"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/","text":"UPDATE VERTEX \u00b6 UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] Use UPDATE VERTEX to update properties on a vertex. The UPDATE VERTEX statement only updates one tag of a vertex at a time. Nebula Graph supports compare-and-set (CAS) operation. NOTE: WHEN and YIELD are optional. vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the constraints are met, UPDATE is executed successfully. condition supports expression operations. columns is the columns to be returned. YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX \"player100\" SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tony Parker\" \\ YIELD $^.player.name AS name, $^.player.age AS age; +-------+-----+ | name | age | +-------+-----+ | \"Tim\" | 42 | +-------+-----+ There is one tag in vertex \"player100\", namely player. nebula> UPDATE VERTEX \"team200\" SET player.name = 'Cory Joseph' WHEN $^.team.name == 'Rocket'; [ERROR (-8)]: Storage Error: Invalid Update col or yield col. UPDATE VERTEX does not support multiple tags, so an error occurs here.","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/2.update-vertex/#update_vertex","text":"UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] Use UPDATE VERTEX to update properties on a vertex. The UPDATE VERTEX statement only updates one tag of a vertex at a time. Nebula Graph supports compare-and-set (CAS) operation. NOTE: WHEN and YIELD are optional. vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the constraints are met, UPDATE is executed successfully. condition supports expression operations. columns is the columns to be returned. YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX \"player100\" SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tony Parker\" \\ YIELD $^.player.name AS name, $^.player.age AS age; +-------+-----+ | name | age | +-------+-----+ | \"Tim\" | 42 | +-------+-----+ There is one tag in vertex \"player100\", namely player. nebula> UPDATE VERTEX \"team200\" SET player.name = 'Cory Joseph' WHEN $^.team.name == 'Rocket'; [ERROR (-8)]: Storage Error: Invalid Update col or yield col. UPDATE VERTEX does not support multiple tags, so an error occurs here.","title":"UPDATE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/","text":"UPSERT VERTEX \u00b6 UPSERT VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the conditions are met, UPSERT is executed successfully. condition supports expression operations. columns is the columns to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT VERTEX to update properties on a vertex if it exists or insert a new vertex if it does not exist. The UPDATE VERTEX statement only updates one tag of a vertex at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the vertex does not exist, a new vertex is created no matter whether the condition in the WHEN clause is met or not. The property columns not specified by the SET statement use the default values of the columns. If there are no default values, an error is returned. If the vertex exists and the WHEN condition is met, the vertex is updated. If the vertex exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do an upsert operation on the vertex. +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Dwight Howard\" | 33 | +-----------------+-----+ nebula> FETCH PROP ON * \"player123\"; -- An empty set is returned, indicating vertex \"player123\" does not exist. Empty set (Time spent: 3.069/4.382 ms) nebula> UPSERT VERTEX \"player123\" SET player.age = $^.player.age + 1; If the vertex \"player123\" does not exist and the default value of age is NULL , the player.age of vertex \"player123\" is NULL . If player.age has a default value, the player.age of vertex \"player123\" is the default value plus one. nebula> CREATE TAG person(followers int, age int DEFAULT 0); -- Create example tag person nebula> UPSERT VERTEX \"300\" SET person.followers = $^.person.age + 1, person.age = 8; -- the number of followers is 1, age is 8 nebula> UPSERT VERTEX \"300\" SET person.age = 8, person.followers = $^.person.age + 1; -- the number of followers is 9, age is 8","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/3.upsert-vertex/#upsert_vertex","text":"UPSERT VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. update_columns is the properties of the vertex to be updated. For example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates the vertex to be updated. condition is some constraints. Only when the conditions are met, UPSERT is executed successfully. condition supports expression operations. columns is the columns to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT VERTEX to update properties on a vertex if it exists or insert a new vertex if it does not exist. The UPDATE VERTEX statement only updates one tag of a vertex at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the vertex does not exist, a new vertex is created no matter whether the condition in the WHEN clause is met or not. The property columns not specified by the SET statement use the default values of the columns. If there are no default values, an error is returned. If the vertex exists and the WHEN condition is met, the vertex is updated. If the vertex exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES \"player111\":(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX \"player111\" SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" AND $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do an upsert operation on the vertex. +-----------------+-----+ | Name | Age | +-----------------+-----+ | \"Dwight Howard\" | 33 | +-----------------+-----+ nebula> FETCH PROP ON * \"player123\"; -- An empty set is returned, indicating vertex \"player123\" does not exist. Empty set (Time spent: 3.069/4.382 ms) nebula> UPSERT VERTEX \"player123\" SET player.age = $^.player.age + 1; If the vertex \"player123\" does not exist and the default value of age is NULL , the player.age of vertex \"player123\" is NULL . If player.age has a default value, the player.age of vertex \"player123\" is the default value plus one. nebula> CREATE TAG person(followers int, age int DEFAULT 0); -- Create example tag person nebula> UPSERT VERTEX \"300\" SET person.followers = $^.person.age + 1, person.age = 8; -- the number of followers is 1, age is 8 nebula> UPSERT VERTEX \"300\" SET person.age = 8, person.followers = $^.person.age + 1; -- the number of followers is 9, age is 8","title":"UPSERT VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/","text":"DELETE VERTEX \u00b6 DELETE VERTEX <vid> [, <vid> ...] Use DELETE VERTEX to delete vertices and the related incoming and outgoing edges of the vertices. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipe. For more information about pipe, see Pipe operator . Examples \u00b6 nebula> DELETE VERTEX \"team1\"; This query deletes the vertex whose ID is \"team1\". nebula> GO FROM \"player100\" OVER serve YIELD serve._dst AS id | DELETE VERTEX $-.id; This query shows that you can use DELETE VERTEX together with pipe. Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes information related to the vertices. NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#delete_vertex","text":"DELETE VERTEX <vid> [, <vid> ...] Use DELETE VERTEX to delete vertices and the related incoming and outgoing edges of the vertices. The DELETE VERTEX statement deletes one vertex or multiple vertices at a time. You can use DELETE VERTEX together with pipe. For more information about pipe, see Pipe operator .","title":"DELETE VERTEX"},{"location":"3.ngql-guide/12.vertex-statements/4.delete-vertex/#examples","text":"nebula> DELETE VERTEX \"team1\"; This query deletes the vertex whose ID is \"team1\". nebula> GO FROM \"player100\" OVER serve YIELD serve._dst AS id | DELETE VERTEX $-.id; This query shows that you can use DELETE VERTEX together with pipe. Nebula Graph traverses the incoming and outgoing edges related to the vertices and deletes them all. Then Nebula Graph deletes information related to the vertices. NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"Examples"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/","text":"INSERT EDGE \u00b6 INSERT EDGE <edge_type> ( <prop_name_list> ) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] The INSERT EDGE statement inserts an edge from a source vertex (given by src_vid) to a destination vertex (given by dst_vid). <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. (TODO: create edge doc) rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0. You can insert many edges with the same edge type for two vertices by using different rank values. OpenCypher compatibility openCypher has no such a concept as rank. Examples \u00b6 nebula> CREATE EDGE e1(); -- create edge type t1 with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property, the edge rank is 1 nebula> CREATE EDGE e2 (name string, age int); -- create edge type e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); -- insert edge from \"11\" to \"13\" with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); -- ERROR. \"a13\" is not int An edge can be inserted/written multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); nebula> FETCH PROP ON e2 \"11\"->\"13\"; +---------+---------+----------+---------+--------+ | e2._src | e2._dst | e2._rank | e2.name | e2.age | +---------+---------+----------+---------+--------+ | \"11\" | \"13\" | 0 | \"n1\" | 14 | +---------+---------+----------+---------+--------+ -- Only the last write can be read","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#insert_edge","text":"INSERT EDGE <edge_type> ( <prop_name_list> ) {VALUES | VALUE} <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] The INSERT EDGE statement inserts an edge from a source vertex (given by src_vid) to a destination vertex (given by dst_vid). <edge_type> denotes the edge type, which must be created before INSERT EDGE . Only one edge type can be specified in this statement. <prop_name_list> is the property name list in the given <edge_type> . <prop_value_list> must provide the value list according to <prop_name_list> . If the property values do not match the data type in the edge type, an error is returned. When the NOT NULL constraint is set for a given property, an error is returned if no property is given. When the default value for a property is NULL , you can omit to specify the property value. (TODO: create edge doc) rank is optional. It specifies the edge rank of the same edge type. If not specified, the default value is 0. You can insert many edges with the same edge type for two vertices by using different rank values. OpenCypher compatibility openCypher has no such a concept as rank.","title":"INSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/1.insert-edge/#examples","text":"nebula> CREATE EDGE e1(); -- create edge type t1 with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\":(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property nebula> INSERT EDGE e1 () VALUES \"10\"->\"11\"@1:(); -- insert an edge from vertex \"10\" to vertex \"11\" with empty property, the edge rank is 1 nebula> CREATE EDGE e2 (name string, age int); -- create edge type e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 1); -- insert edge from \"11\" to \"13\" with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ \"12\"->\"13\":(\"n1\", 1), \"13\"->\"14\":(\"n2\", 2); -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", \"a13\"); -- ERROR. \"a13\" is not int An edge can be inserted/written multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 12); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 13); nebula> INSERT EDGE e2 (name, age) VALUES \"11\"->\"13\":(\"n1\", 14); nebula> FETCH PROP ON e2 \"11\"->\"13\"; +---------+---------+----------+---------+--------+ | e2._src | e2._dst | e2._rank | e2.name | e2.age | +---------+---------+----------+---------+--------+ | \"11\" | \"13\" | 0 | \"n1\" | 14 | +---------+---------+----------+---------+--------+ -- Only the last write can be read","title":"Examples"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/","text":"UPDATE EDGE \u00b6 UPDATE EDGE <src_vid> -> <dest_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] Use UPDATE EDGE to update properties on an edge. The UPDATE EDGE statement only updates one edge at a time. Nebula Graph supports compare-and-set (CAS). NOTE: WHEN and YIELD are optional. update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPDATE is executed successfully. condition supports expression operations. properties is the properties to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE \"100\" -> \"200\"@0 OF serve SET start_year = serve.start_year + 1;","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/2.update-edge/#update_edge","text":"UPDATE EDGE <src_vid> -> <dest_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] Use UPDATE EDGE to update properties on an edge. The UPDATE EDGE statement only updates one edge at a time. Nebula Graph supports compare-and-set (CAS). NOTE: WHEN and YIELD are optional. update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPDATE is executed successfully. condition supports expression operations. properties is the properties to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE \"100\" -> \"200\"@0 OF serve SET start_year = serve.start_year + 1;","title":"UPDATE EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/","text":"UPSERT EDGE \u00b6 UPSERT EDGE <src_vid> -> <dst_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPSERT is executed successfully. condition supports expression operations. properties specifies the properties to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT EDGE to update properties on an edge if it exists or insert a new edge if it does not exist. The UPDATE EDGE statement updates only one edge at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the edge does not exist, a new edge is created no matter whether the condition in the WHEN clause is met or not. The properties not specified by the SET statement use the default property values. If there are no default values, an error is returned. If the edge exists and the WHEN condition is met, the edge is updated. If the edge exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016); -- Insert a new edge. nebula> UPSERT EDGE \"player100\" -> \"team200\" OF serve SET start_year = serve.start_year + 2 WHEN serve.end_year == 2016 YIELD serve.start_year AS Start, serve.end_year AS End; +-------+------+ | Start | End | +-------+------+ | 1999 | 2016 | +-------+------+ nebula> FETCH PROP ON serve \"player100\" -> \"team200\"; +-------------+------------+-------------+------------------+----------------+ | serve._src | serve._dst | serve._rank | serve.start_year | serve.end_year | +-------------+------------+-------------+------------------+----------------+ | \"player100\" | \"team200\" | 0 | 1999 | 2016 | +-------------+------------+-------------+------------------+----------------+","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/3.upsert-edge/#upsert_edge","text":"UPSERT EDGE <src_vid> -> <dst_vid> [@rank] OF <edge_type> SET <update_properties> [WHEN <condition>] [YIELD <properties>] update_properties is the properties of the edge to be updated. For example, e1.col1 = $^.e1.col2 + 1 means to update e1.col1 to e1.col2+1 . NOTE: $^ indicates the edge to be updated. condition is some constraints. Only when the condition is met, UPSERT is executed successfully. condition supports expression operations. properties specifies the properties to be returned, YIELD returns the latest updated values. UPSERT is a combination of UPDATE and INSERT . Use UPSERT EDGE to update properties on an edge if it exists or insert a new edge if it does not exist. The UPDATE EDGE statement updates only one edge at a time. The performance of UPSERT is much lower than that of INSERT , because UPSERT is a read-modify-write serialization operation at the partition level. DON'T: DO NOT use UPSERT for scenarios with highly concurrent writes. If the edge does not exist, a new edge is created no matter whether the condition in the WHEN clause is met or not. The properties not specified by the SET statement use the default property values. If there are no default values, an error is returned. If the edge exists and the WHEN condition is met, the edge is updated. If the edge exists and the WHEN condition is not met, Nebula Graph does nothing. Consider the following example: nebula> INSERT EDGE serve(start_year, end_year) VALUES \"player100\" -> \"team200\":(1997, 2016); -- Insert a new edge. nebula> UPSERT EDGE \"player100\" -> \"team200\" OF serve SET start_year = serve.start_year + 2 WHEN serve.end_year == 2016 YIELD serve.start_year AS Start, serve.end_year AS End; +-------+------+ | Start | End | +-------+------+ | 1999 | 2016 | +-------+------+ nebula> FETCH PROP ON serve \"player100\" -> \"team200\"; +-------------+------------+-------------+------------------+----------------+ | serve._src | serve._dst | serve._rank | serve.start_year | serve.end_year | +-------------+------------+-------------+------------------+----------------+ | \"player100\" | \"team200\" | 0 | 1999 | 2016 | +-------------+------------+-------------+------------------+----------------+","title":"UPSERT EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/","text":"DELETE EDGE \u00b6 DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <edge_type> <src_vid> -> <dst_vid>[@<rank>] ...] Use DELETE EDGE to delete edges. The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe. For more information about pipe, see Pipe operator . Examples \u00b6 nebula> DELETE EDGE serve \"player100\" -> \"team200\"@0; This query deletes the serve edge from \"player100\" to \"team200\" , of which the rank value is 0. nebula> GO FROM \"player100\" OVER follow WHERE follow._dst == \"team200\" YIELD follow._src AS src, follow._dst AS dst, follow._rank AS rank | \\ DELETE EDGE follow $-.src->$-.dst @ $-.rank; This query shows that you can use DELETE EDGE together with pipe. This query first traverses all the follow edges with different rank values from \"player100\" to \"team200\" then deletes them. To delete all the outgoing edges for a vertex, delete the vertex. For more information, see DELETE VERTEX . NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#delete_edge","text":"DELETE EDGE <edge_type> <src_vid> -> <dst_vid>[@<rank>] [, <edge_type> <src_vid> -> <dst_vid>[@<rank>] ...] Use DELETE EDGE to delete edges. The DELETE EDGE statement deletes one edge or multiple edges at a time. You can use DELETE EDGE together with pipe. For more information about pipe, see Pipe operator .","title":"DELETE EDGE"},{"location":"3.ngql-guide/13.edge-statements/4.delete-edge/#examples","text":"nebula> DELETE EDGE serve \"player100\" -> \"team200\"@0; This query deletes the serve edge from \"player100\" to \"team200\" , of which the rank value is 0. nebula> GO FROM \"player100\" OVER follow WHERE follow._dst == \"team200\" YIELD follow._src AS src, follow._dst AS dst, follow._rank AS rank | \\ DELETE EDGE follow $-.src->$-.dst @ $-.rank; This query shows that you can use DELETE EDGE together with pipe. This query first traverses all the follow edges with different rank values from \"player100\" to \"team200\" then deletes them. To delete all the outgoing edges for a vertex, delete the vertex. For more information, see DELETE VERTEX . NOTE: Atomic operation is not guaranteed during the entire process for now, so please retry when a failure occurs.","title":"Examples"},{"location":"3.ngql-guide/14.native-index-statements/","text":"Index overview \u00b6 Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index. Native indexes \u00b6 Native indexes allow querying data based on a given property. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types. You can do partial match search by using composite indexes. Use composite indexes only for partial match searches when the declared fields in the composite index are used from left to right. For more information, see LOOKUP FAQ . String operators like CONTAINS and STARTS WITH are not allowed in native index searching. Use full-text indexes to do fuzzy search. Operations on native indexes \u00b6 You can do the following operations against native indexes: Create index Show index Describe index Rebuild index Show index status Drop index Query index Full-text indexes \u00b6 Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR and NOT . To do complete text match, use native indexes. Operations on full-text indexes \u00b6 Before doing any operations on full-text indexes, please mak sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes . Null values \u00b6 Indexes do not support indexing null values at this time. Range queries \u00b6 In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range search for numeric, date, and time type properties.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#index_overview","text":"Indexes are built to fast process graph queries. Nebula Graph supports two kinds of indexes: native indexes and full-text indexes. This topic introduces the index types and helps choose the right index.","title":"Index overview"},{"location":"3.ngql-guide/14.native-index-statements/#native_indexes","text":"Native indexes allow querying data based on a given property. There are two kinds of native indexes: tag index and edge type index. Native indexes must be updated manually. You can use the REBUILD INDEX statement to update native indexes. Native indexes support indexing multiple properties on a tag or an edge type (composite indexes), but do not support indexing across multiple tags or edge types. You can do partial match search by using composite indexes. Use composite indexes only for partial match searches when the declared fields in the composite index are used from left to right. For more information, see LOOKUP FAQ . String operators like CONTAINS and STARTS WITH are not allowed in native index searching. Use full-text indexes to do fuzzy search.","title":"Native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_native_indexes","text":"You can do the following operations against native indexes: Create index Show index Describe index Rebuild index Show index status Drop index Query index","title":"Operations on native indexes"},{"location":"3.ngql-guide/14.native-index-statements/#full-text_indexes","text":"Full-text indexes are used to do prefix, wildcard, regexp, and fuzzy search on a string property. Full-text indexes allow indexing just one property. Only strings within a specified length (no longer than 256 bytes) are indexed. Full-text indexes do not support logical operations such as AND , OR and NOT . To do complete text match, use native indexes.","title":"Full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#operations_on_full-text_indexes","text":"Before doing any operations on full-text indexes, please mak sure that you deploy full-text indexes. Details on full-text indexes deployment, see Deploy Elasticsearch and Deploy Listener . At this time, full-text indexes are created automatically on the Elasticsearch cluster. And rebuilding or altering full-text indexes are not supported. To drop full-text indexes, you need to drop them on the Elasticsearch cluster manually. To query full-text indexes, see Search with full-text indexes .","title":"Operations on full-text indexes"},{"location":"3.ngql-guide/14.native-index-statements/#null_values","text":"Indexes do not support indexing null values at this time.","title":"Null values"},{"location":"3.ngql-guide/14.native-index-statements/#range_queries","text":"In addition to querying single results from native indexes, you can also do range queries. Not all the native indexes support range queries. You can only do range search for numeric, date, and time type properties.","title":"Range queries"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/","text":"CREATE INDEX \u00b6 Use CREATE INDEX to add native indexes for existing tags, edge types or properties. NOTE: For how to create text-based indexes, see CREATE FULLTEXT INDEX (doc TODO). Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Indexes make these global retrieval operations efficient on large graphs. Prerequisites \u00b6 Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE EDGE (doc TODO) and CREATE EDGE (doc TODO). Must-read for using index \u00b6 Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If you must use indexes, we suggest that you: Import data into Nebula Graph. Create indexes. Rebuild the indexes. The preceding workflow minimizes the negative influences of using indexes. Syntax \u00b6 CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([prop_name_list]) IF NOT EXISTS : - Creating an existent index results in an error. You can use the IF NOT EXISTS option to conditionally create the index and avoid the error. prop_name_list : To index a variable string property, you must use the prop_name(length) syntax to specify an index prefix length. To index a fixed-length string property, you must use the prop_name syntax, and the string length is read from the property. To index a tag or an edge type, omit the prop_name_list in the parentheses. Create tag/edge type indexes \u00b6 The following statement creates an index on the player tag. nebula> CREATE TAG INDEX player_index on player(); The following statement creates indexes on the edge type like . nebula> CREATE EDGE INDEX like_index on like(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag, or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type. For more information, see List vertices or edges with a tag or an edge type . Create single-property indexes \u00b6 nebula> CREATE TAG INDEX player_index_0 on player(name(10)); The preceding statement creates an index for the name property on all vertices carrying the player tag. This statement creates an index using the first 10 characters of the name property. nebula> CREATE TAG var_string(p1 string); nebula> CREATE TAG INDEX var ON var_string(p1(10)); nebula> CREATE TAG fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX fix ON fix_string(p1); nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The preceding statement creates an index for the degree property on all edges carrying the follow edge type. Create composite property indexes \u00b6 An index on multiple properties is called a composite index. NOTE : Creating index across multiple tags is not supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name(10), age); This statement creates a composite index for the name and age property on all vertices carrying the player tag. Using index \u00b6 After the index is created and data is inserted, you can use the LOOKUP statement to query the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself.","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_index","text":"Use CREATE INDEX to add native indexes for existing tags, edge types or properties. NOTE: For how to create text-based indexes, see CREATE FULLTEXT INDEX (doc TODO). Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Indexes make these global retrieval operations efficient on large graphs.","title":"CREATE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#prerequisites","text":"Before you create an index, make sure that the relative tag or edge type is created. For how to create tags or edge types, see CREATE EDGE (doc TODO) and CREATE EDGE (doc TODO).","title":"Prerequisites"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#must-read_for_using_index","text":"Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. If you must use indexes, we suggest that you: Import data into Nebula Graph. Create indexes. Rebuild the indexes. The preceding workflow minimizes the negative influences of using indexes.","title":"Must-read for using index"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#syntax","text":"CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} ([prop_name_list]) IF NOT EXISTS : - Creating an existent index results in an error. You can use the IF NOT EXISTS option to conditionally create the index and avoid the error. prop_name_list : To index a variable string property, you must use the prop_name(length) syntax to specify an index prefix length. To index a fixed-length string property, you must use the prop_name syntax, and the string length is read from the property. To index a tag or an edge type, omit the prop_name_list in the parentheses.","title":"Syntax"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_tagedge_type_indexes","text":"The following statement creates an index on the player tag. nebula> CREATE TAG INDEX player_index on player(); The following statement creates indexes on the edge type like . nebula> CREATE EDGE INDEX like_index on like(); After indexing a tag or an edge type, you can use the LOOKUP statement to retrieve the VID of all vertices with the tag, or the source vertex ID, destination vertex ID, and ranks of all edges with the edge type. For more information, see List vertices or edges with a tag or an edge type .","title":"Create tag/edge type indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_single-property_indexes","text":"nebula> CREATE TAG INDEX player_index_0 on player(name(10)); The preceding statement creates an index for the name property on all vertices carrying the player tag. This statement creates an index using the first 10 characters of the name property. nebula> CREATE TAG var_string(p1 string); nebula> CREATE TAG INDEX var ON var_string(p1(10)); nebula> CREATE TAG fix_string(p1 FIXED_STRING(10)); nebula> CREATE TAG INDEX fix ON fix_string(p1); nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The preceding statement creates an index for the degree property on all edges carrying the follow edge type.","title":"Create single-property indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#create_composite_property_indexes","text":"An index on multiple properties is called a composite index. NOTE : Creating index across multiple tags is not supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name(10), age); This statement creates a composite index for the name and age property on all vertices carrying the player tag.","title":"Create composite property indexes"},{"location":"3.ngql-guide/14.native-index-statements/1.create-native-index/#using_index","text":"After the index is created and data is inserted, you can use the LOOKUP statement to query the data. You do not need to specify which indexes to use in a query, Nebula Graph figures that out by itself.","title":"Using index"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/","text":"Show INDEXES \u00b6 SHOW {TAG | EDGE} INDEXES Use SHOW INDEXES to list the defined tag or edge type indexes names. Example \u00b6 nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"fix\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"var\" | +------------------+ nebula> SHOW EDGE INDEXES; +------------------+ | Names | +------------------+ | \"follow_index_0\" | +------------------+","title":"SHOW INDEX"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#show_indexes","text":"SHOW {TAG | EDGE} INDEXES Use SHOW INDEXES to list the defined tag or edge type indexes names.","title":"Show INDEXES"},{"location":"3.ngql-guide/14.native-index-statements/2.show-native-indexes/#example","text":"nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"fix\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"var\" | +------------------+ nebula> SHOW EDGE INDEXES; +------------------+ | Names | +------------------+ | \"follow_index_0\" | +------------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/","text":"DESCRIBE INDEX \u00b6 DESCRIBE {TAG | EDGE} INDEX <index_name> Use DESCRIBE INDEX to get information about the index. DESCRIBE INDEX returns the following columns: Field The property name. - Type The property type. Example \u00b6 nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | +--------+--------------------+ | \"age\" | \"int64\" | +--------+--------------------+","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#describe_index","text":"DESCRIBE {TAG | EDGE} INDEX <index_name> Use DESCRIBE INDEX to get information about the index. DESCRIBE INDEX returns the following columns: Field The property name. - Type The property type.","title":"DESCRIBE INDEX"},{"location":"3.ngql-guide/14.native-index-statements/3.describe-native-index/#example","text":"nebula> DESCRIBE TAG INDEX player_index_0; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(30)\" | +--------+--------------------+ nebula> DESCRIBE TAG INDEX player_index_1; +--------+--------------------+ | Field | Type | +--------+--------------------+ | \"name\" | \"fixed_string(10)\" | +--------+--------------------+ | \"age\" | \"int64\" | +--------+--------------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/","text":"REBUILD INDEX \u00b6 REBUILD {TAG | EDGE} INDEX <index_name> Use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create index, see CREATE INDEX . If the index is created before data insertion, there is no need to rebuild the index. If data is updated or newly inserted before the index creation, you need to rebuild the indexes in order to make sure that the indexes contain the previously added data. NOTE: During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because the not all the data is indexed during rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS . Example \u00b6 nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name(10)); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> SHOW TAG INDEX STATUS; Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB . TODO: update SHOW TAG INDEX STATUS . Code not ready.","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#rebuild_index","text":"REBUILD {TAG | EDGE} INDEX <index_name> Use REBUILD INDEX to rebuild the created tag or edge type index. For details on how to create index, see CREATE INDEX . If the index is created before data insertion, there is no need to rebuild the index. If data is updated or newly inserted before the index creation, you need to rebuild the indexes in order to make sure that the indexes contain the previously added data. NOTE: During the rebuilding, all queries skip the index and perform sequential scans. This means that the return results can be different because the not all the data is indexed during rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For details on index status, see SHOW INDEX STATUS .","title":"REBUILD INDEX"},{"location":"3.ngql-guide/14.native-index-statements/4.rebuild-native-index/#example","text":"nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name(10)); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index; +------------+ | New Job Id | +------------+ | 66 | +------------+ nebula> SHOW TAG INDEX STATUS; Nebula Graph creates a job to rebuild the index. The job ID is displayed in the preceding return message. To check if the rebuilding process is complete, use the SHOW JOB <job_id> statement. For more information, see SHOW JOB . TODO: update SHOW TAG INDEX STATUS . Code not ready.","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/","text":"SHOW INDEX STATUS \u00b6 SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the created tag or edge type index status. For details on how to create index, see CREATE INDEX . SHOW INDEX STATUS returns the following fields: Name The index name. Index Status Index Status includes QUEUE , RUNNING , FINISHED , FAILED , STOPPED , INVALID . Example \u00b6 nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | +----------------------+--------------+ | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#show_index_status","text":"SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the created tag or edge type index status. For details on how to create index, see CREATE INDEX . SHOW INDEX STATUS returns the following fields: Name The index name. Index Status Index Status includes QUEUE , RUNNING , FINISHED , FAILED , STOPPED , INVALID .","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/14.native-index-statements/5.show-native-index-status/#example","text":"nebula> SHOW TAG INDEX STATUS; +----------------------+--------------+ | Name | Index Status | +----------------------+--------------+ | \"player_index_0\" | \"FINISHED\" | +----------------------+--------------+ | \"player_index_1\" | \"FINISHED\" | +----------------------+--------------+","title":"Example"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/","text":"DROP INDEX \u00b6 DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> The DROP INDEX statement removes an existing index from the current graph space. Removing a nonexistent index results in an error. You can use the IF EXISTS option to conditionally drop the index and avoid the error. To run this statement you need some privilege. For information about the built-in roles in Nebula Graph, see Built-in roles (TODO). Example \u00b6 nebula> DROP TAG INDEX player_index_0; This query drops a tag index names player_index_0 .","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#drop_index","text":"DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> The DROP INDEX statement removes an existing index from the current graph space. Removing a nonexistent index results in an error. You can use the IF EXISTS option to conditionally drop the index and avoid the error. To run this statement you need some privilege. For information about the built-in roles in Nebula Graph, see Built-in roles (TODO).","title":"DROP INDEX"},{"location":"3.ngql-guide/14.native-index-statements/6.drop-native-index/#example","text":"nebula> DROP TAG INDEX player_index_0; This query drops a tag index names player_index_0 .","title":"Example"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/","text":"Full-text search \u00b6 LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100. timeout (optional): Specifies the timeout time. The default value is 200ms. Use the LOOKUP ON statement to do full-text search. The search string is specified in the WHERE clause. Before doing a full-text search, make sure that you deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener . Before you start \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Natural language full-text search \u00b6 A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive. Examples \u00b6 nebula> CREATE SPACE nba (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); nebula> USE nba; nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG INDEX name ON player(name(20)); nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\"); +-----------------+ | _vid | +-----------------+ | \"Boris Diaw\" | +-----------------+ | \"Ben Simmons\" | +-----------------+ | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----------------+-----+ | _vid | name | age | +-----------------+-----------------+-----+ | \"Chris Paul\" | \"Chris Paul\" | 33 | +-----------------+-----------------+-----+ | \"Boris Diaw\" | \"Boris Diaw\" | 36 | +-----------------+-----------------+-----+ | \"Blake Griffin\" | \"Blake Griffin\" | 30 | +-----------------+-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | COUNT(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+---------------------+-----+ | _vid | name | age | +---------------------+---------------------+-----+ | \"Russell Westbrook\" | \"Russell Westbrook\" | 30 | +---------------------+---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\"); +---------------------+ | _vid | +---------------------+ | \"Danny Green\" | +---------------------+ | \"David West\" | +---------------------+ | \"Russell Westbrook\" | +---------------------+ ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+--------------+ | _vid | name | +--------------+--------------+ | \"Tim Duncan\" | \"Tim Duncan\" | +--------------+--------------+","title":"Search with full-text index"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#full-text_search","text":"LOOKUP ON {<tag> | <edge_type>} WHERE <expression> [YIELD <return_list>] <expression> ::= PREFIX | WILDCARD | REGEXP | FUZZY <return_list> <prop_name> [AS <prop_alias>] [, <prop_name> [AS <prop_alias>] ...] PREFIX(schema_name.prop_name, prefix_string, row_limit, timeout) WILDCARD(schema_name.prop_name, wildcard_string, row_limit, timeout) REGEXP(schema_name.prop_name, regexp_string, row_limit, timeout) FUZZY(schema_name.prop_name, fuzzy_string, fuzziness, operator, row_limit, timeout) fuzziness (optional): Maximum edit distance allowed for matching. The default value is AUTO . For other valid values and more information, see Elasticsearch document . operator (optional): Boolean logic used to interpret text. Valid values are OR (default) and AND . row_limit (optional): Specifies the number of rows to return. The default value is 100. timeout (optional): Specifies the timeout time. The default value is 200ms. Use the LOOKUP ON statement to do full-text search. The search string is specified in the WHERE clause. Before doing a full-text search, make sure that you deployed a Elasticsearch cluster and a Listener cluster. For more information, see Deploy Elasticsearch and Deploy Listener .","title":"Full-text search"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#before_you_start","text":"Before you start using the full-text index, please make sure that you know the restrictions .","title":"Before you start"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#natural_language_full-text_search","text":"A natural language search interprets the search string as a phrase in natural human language. The search is case-insensitive.","title":"Natural language full-text search"},{"location":"3.ngql-guide/15.full-text-index-statements/1.search-with-text-based-index/#examples","text":"nebula> CREATE SPACE nba (partition_num=3,replica_factor=1, vid_type=fixed_string(30)); nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); nebula> USE nba; nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; nebula> CREATE TAG player(name string, age int); nebula> CREATE TAG INDEX name ON player(name(20)); nebula> INSERT VERTEX player(name, age) VALUES \\ \"Russell Westbrook\": (\"Russell Westbrook\", 30), \\ \"Chris Paul\": (\"Chris Paul\", 33),\\ \"Boris Diaw\": (\"Boris Diaw\", 36),\\ \"David West\": (\"David West\", 38),\\ \"Danny Green\": (\"Danny Green\", 31),\\ \"Tim Duncan\": (\"Tim Duncan\", 42),\\ \"James Harden\": (\"James Harden\", 29),\\ \"Tony Parker\": (\"Tony Parker\", 36),\\ \"Aron Baynes\": (\"Aron Baynes\", 32),\\ \"Ben Simmons\": (\"Ben Simmons\", 22),\\ \"Blake Griffin\": (\"Blake Griffin\", 30); nebula> LOOKUP ON player WHERE PREFIX(player.name, \"B\"); +-----------------+ | _vid | +-----------------+ | \"Boris Diaw\" | +-----------------+ | \"Ben Simmons\" | +-----------------+ | \"Blake Griffin\" | +-----------------+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") YIELD player.name, player.age; +-----------------+-----------------+-----+ | _vid | name | age | +-----------------+-----------------+-----+ | \"Chris Paul\" | \"Chris Paul\" | 33 | +-----------------+-----------------+-----+ | \"Boris Diaw\" | \"Boris Diaw\" | 36 | +-----------------+-----------------+-----+ | \"Blake Griffin\" | \"Blake Griffin\" | 30 | +-----------------+-----------------+-----+ nebula> LOOKUP ON player WHERE WILDCARD(player.name, \"*ri*\") | YIELD count(*); +----------+ | COUNT(*) | +----------+ | 3 | +----------+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \"R.*\") YIELD player.name, player.age; +---------------------+---------------------+-----+ | _vid | name | age | +---------------------+---------------------+-----+ | \"Russell Westbrook\" | \"Russell Westbrook\" | 30 | +---------------------+---------------------+-----+ nebula> LOOKUP ON player WHERE REGEXP(player.name, \".*\"); +---------------------+ | _vid | +---------------------+ | \"Danny Green\" | +---------------------+ | \"David West\" | +---------------------+ | \"Russell Westbrook\" | +---------------------+ ... nebula> LOOKUP ON player WHERE FUZZY(player.name, \"Tim Dunncan\", AUTO, OR) YIELD player.name; +--------------+--------------+ | _vid | name | +--------------+--------------+ | \"Tim Duncan\" | \"Tim Duncan\" | +--------------+--------------+","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/","text":"GET SUBGRAPH \u00b6 The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the start vertices over the specified types of edges. Syntax \u00b6 GET SUBGRAPH [<step_count> STEPS] FROM {<vid>, <vid>...} [IN <edge_type>, <edge_type>...] [OUT <edge_type>, <edge_type>...] [BOTH <edge_type>, <edge_type>...] Clause Description STEPS Specifies the steps to go from the start vertices. A step_count must be a non-negative integer. Its default value is 1. When <step_count> is specified to N , the Nebula Graph returns zero to N steps subgraph. FROM Specifies the start vertices. IN Gets the subgraphs from the start vertices over the specified incoming edges (edges pointing to the start vertices). OUT Gets the subgraphs from the start vertices over the specified outgoing edges (edges pointing out from the start vertices). BOTH Gets the subgraphs from the start vertices over the specified types of edges, both incoming and outgoing. When the traversal direction is not specified, both the incoming and outgoing edges are returned. Examples \u00b6 The following graph is used as the sample. Go one step from the vertex with VID \"player100\" over all types of edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\"; +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | _vertices | _edges | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player100) player.name:Tim,player.age:42] | [player100-[follow]->player101@0 degree:96,player100-[follow]->player102@0 degree:90,player100-[serve]->team200@0 end_year:2016,start_year:1997] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player101) player.age:36,player.name:Tony Parker,(player102) player.age:33,player.name:LaMarcus Aldridge,(team200) team.name:Warriors] | [player102-[follow]->player101@0 degree:75] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ Got 2 rows (time spent 6289/7423 us) The returned subgraph is as follows. Go one step from the vertex with VID \"player100\" over incoming follow edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" IN follow; +-----------+--------+ | _vertices | _edges | +-----------+--------+ | [] | [] | +-----------+--------+ | [] | [] | +-----------+--------+ Got 2 rows (time spent 2292/3091 us) There is no incoming follow edge to \"player100\", so no vertex or edge is returned. Go one step from the vertex \"player100\" over outgoing serve edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" OUT serve; +---------------------------------------------+--------------------------------------------------------------+ | _vertices | _edges | +---------------------------------------------+--------------------------------------------------------------+ | [(player100) player.age:42,player.name:Tim] | [player100-[serve]->team200@0 start_year:1997,end_year:2016] | +---------------------------------------------+--------------------------------------------------------------+ | [(team200) team.name:Warriors] | [] | +---------------------------------------------+--------------------------------------------------------------+ Got 2 rows (time spent 2107/2547 us) The returned subgraph is as follows.","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#get_subgraph","text":"The GET SUBGRAPH statement retrieves information of vertices and edges reachable from the start vertices over the specified types of edges.","title":"GET SUBGRAPH"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#syntax","text":"GET SUBGRAPH [<step_count> STEPS] FROM {<vid>, <vid>...} [IN <edge_type>, <edge_type>...] [OUT <edge_type>, <edge_type>...] [BOTH <edge_type>, <edge_type>...] Clause Description STEPS Specifies the steps to go from the start vertices. A step_count must be a non-negative integer. Its default value is 1. When <step_count> is specified to N , the Nebula Graph returns zero to N steps subgraph. FROM Specifies the start vertices. IN Gets the subgraphs from the start vertices over the specified incoming edges (edges pointing to the start vertices). OUT Gets the subgraphs from the start vertices over the specified outgoing edges (edges pointing out from the start vertices). BOTH Gets the subgraphs from the start vertices over the specified types of edges, both incoming and outgoing. When the traversal direction is not specified, both the incoming and outgoing edges are returned.","title":"Syntax"},{"location":"3.ngql-guide/16.subgraph-and-path/1.get-subgraph/#examples","text":"The following graph is used as the sample. Go one step from the vertex with VID \"player100\" over all types of edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\"; +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | _vertices | _edges | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player100) player.name:Tim,player.age:42] | [player100-[follow]->player101@0 degree:96,player100-[follow]->player102@0 degree:90,player100-[serve]->team200@0 end_year:2016,start_year:1997] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ | [(player101) player.age:36,player.name:Tony Parker,(player102) player.age:33,player.name:LaMarcus Aldridge,(team200) team.name:Warriors] | [player102-[follow]->player101@0 degree:75] | +--------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+ Got 2 rows (time spent 6289/7423 us) The returned subgraph is as follows. Go one step from the vertex with VID \"player100\" over incoming follow edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" IN follow; +-----------+--------+ | _vertices | _edges | +-----------+--------+ | [] | [] | +-----------+--------+ | [] | [] | +-----------+--------+ Got 2 rows (time spent 2292/3091 us) There is no incoming follow edge to \"player100\", so no vertex or edge is returned. Go one step from the vertex \"player100\" over outgoing serve edges and get the subgraph. nebula> GET SUBGRAPH 1 STEPS FROM \"player100\" OUT serve; +---------------------------------------------+--------------------------------------------------------------+ | _vertices | _edges | +---------------------------------------------+--------------------------------------------------------------+ | [(player100) player.age:42,player.name:Tim] | [player100-[serve]->team200@0 start_year:1997,end_year:2016] | +---------------------------------------------+--------------------------------------------------------------+ | [(team200) team.name:Warriors] | [] | +---------------------------------------------+--------------------------------------------------------------+ Got 2 rows (time spent 2107/2547 us) The returned subgraph is as follows.","title":"Examples"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/","text":"FIND PATH \u00b6 FIND { SHORTEST | ALL | NOLOOP } PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [UPTO <N> STEPS] [| ORDER BY $-.path] [| LIMIT <M>] <vertex_id_list> ::= [vertex_id [, vertex_id] ...] The FIND PATH statement finds the paths between the selected source vertices and destination vertices. SHORTEST finds the shortest path. ALL finds all the paths. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. <N> is the hop number. The default value is 5. <M> specifies the maximum number of rows to return. Limitations \u00b6 When a list of source and/or destination vertex IDs are specified, the shortest path between any source vertices and the destination vertices is returned. There can be cycles when searching all paths. FIND PATH does not support property filtering. FIND PATH does not support specifying a direction. FIND PATH is a single-thread procedure, so it uses much memory. When finding path with the NOLOOP keyword, the returned paths do not include cycles. If NOLOOP is not used, FIND PATH can retrieve paths containing cycles. If NOLOOP is used, FIND PATH can retrieve paths without cycles. Examples \u00b6 In Nebula Console, a path is shown as vertex_id <edge_name, rank> vertex_id . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team201\" OVER *; +------------------------------------------------------------------+ | path | +------------------------------------------------------------------+ | (\"player102\")-[:follow@0]->(\"player101\")-[:serve@0]->(\"team201\") | +------------------------------------------------------------------+ nebula> FIND SHORTEST PATH FROM \"team200\" TO \"player100\" OVER * REVERSELY; +---------------------------------------+ | path | +---------------------------------------+ | (\"team200\")<-[:serve@0]-(\"player100\") | +---------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#find_path","text":"FIND { SHORTEST | ALL | NOLOOP } PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [REVERSELY | BIDIRECT] [UPTO <N> STEPS] [| ORDER BY $-.path] [| LIMIT <M>] <vertex_id_list> ::= [vertex_id [, vertex_id] ...] The FIND PATH statement finds the paths between the selected source vertices and destination vertices. SHORTEST finds the shortest path. ALL finds all the paths. <vertex_id_list> is a list of vertex IDs separated with commas (,). It supports $- and $var . <edge_type_list> is a list of edge types separated with commas (,). * is all edge types. <N> is the hop number. The default value is 5. <M> specifies the maximum number of rows to return.","title":"FIND PATH"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#limitations","text":"When a list of source and/or destination vertex IDs are specified, the shortest path between any source vertices and the destination vertices is returned. There can be cycles when searching all paths. FIND PATH does not support property filtering. FIND PATH does not support specifying a direction. FIND PATH is a single-thread procedure, so it uses much memory. When finding path with the NOLOOP keyword, the returned paths do not include cycles. If NOLOOP is not used, FIND PATH can retrieve paths containing cycles. If NOLOOP is used, FIND PATH can retrieve paths without cycles.","title":"Limitations"},{"location":"3.ngql-guide/16.subgraph-and-path/2.find-path/#examples","text":"In Nebula Console, a path is shown as vertex_id <edge_name, rank> vertex_id . nebula> FIND SHORTEST PATH FROM \"player102\" TO \"team201\" OVER *; +------------------------------------------------------------------+ | path | +------------------------------------------------------------------+ | (\"player102\")-[:follow@0]->(\"player101\")-[:serve@0]->(\"team201\") | +------------------------------------------------------------------+ nebula> FIND SHORTEST PATH FROM \"team200\" TO \"player100\" OVER * REVERSELY; +---------------------------------------+ | path | +---------------------------------------+ | (\"team200\")<-[:serve@0]-(\"player100\") | +---------------------------------------+ nebula> FIND ALL PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+ nebula> FIND NOLOOP PATH FROM \"player100\" TO \"team200\" OVER *; +---------------------------------------+ | path | +---------------------------------------+ | (\"player100\")-[:serve@0]->(\"team200\") | +---------------------------------------+","title":"Examples"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/","text":"EXPLAIN and PROFILE \u00b6 EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance with the execution plan and profile. Execution Plan \u00b6 The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions. An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, or filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator, while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators. Syntax \u00b6 EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement> Output formats \u00b6 The output of an EXPLAIN or a PROFILE statement has two formats, the default \"row\" format and the \"dot\" format. You can use the format option to modify the output format. Omitting the format option indicates using the default \"row\" format. Format \"row\" \u00b6 The \"row\" format outputs the return message in a table as follows. EXPLAIN : nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 104/705 us) Execution Plan +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 0 | ShowTags | 2 | | outputVar: [ {\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 2 | Start | | | outputVar: [ {\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ PROFILE : nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 0 | ShowTags | 2 | ver: 0, rows: 1, execTime: 79us, totalTime: 1692us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 2 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ The descriptions of the columns are as follows: Column Description id Indicates the ID of the operator. name Indicates the name of the operator. dependencies Shows the ID of the operator that the current operator depends on. profiling data Shows the execution profile. ver is the version of the operator, which you can use to identify loops; rows shows the number of rows to be output by the operator; execTime shows the execution time only; totalTime contains the execution time and the system scheduling and queueing time. operator info Shows the detailed information of the operator. Format \"dot\" \u00b6 You can use the format=\"dot\" option to output the return message in the DOT language, and then use Graphviz to generate a graph of the plan. NOTE : Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- Transformed into a Graphviz graph, it is as follows:","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#explain_and_profile","text":"EXPLAIN helps output the execution plan of an nGQL statement without executing the statement. PROFILE executes the statement, then outputs the execution plan as well as the execution profile. You can optimize the queries for better performance with the execution plan and profile.","title":"EXPLAIN and PROFILE"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#execution_plan","text":"The execution plan is determined by the execution planner in the Nebula Graph query engine. The execution planner processes the parsed nGQL statements into actions. An action is the smallest unit that can be executed. A typical action fetches all neighbors of a given vertex, gets the properties of an edge, or filters vertices or edges based on the given conditions. Each action is assigned to an operator that performs the action. For example, a SHOW TAGS statement is processed into two actions and assigned to a Start operator and a ShowTags operator, while a more complex GO statement may be processed into more than 10 actions and assigned to 10 operators.","title":"Execution Plan"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#syntax","text":"EXPLAIN EXPLAIN [format=\"row\" | \"dot\"] <your_nGQL_statement> PROFILE PROFILE [format=\"row\" | \"dot\"] <your_nGQL_statement>","title":"Syntax"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#output_formats","text":"The output of an EXPLAIN or a PROFILE statement has two formats, the default \"row\" format and the \"dot\" format. You can use the format option to modify the output format. Omitting the format option indicates using the default \"row\" format.","title":"Output formats"},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#format_row","text":"The \"row\" format outputs the return message in a table as follows. EXPLAIN : nebula> EXPLAIN format=\"row\" SHOW TAGS; Execution succeeded (time spent 104/705 us) Execution Plan +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 0 | ShowTags | 2 | | outputVar: [ {\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ | 2 | Start | | | outputVar: [ {\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------+-----------------------------------------------------------------------+ PROFILE : nebula> PROFILE format=\"row\" SHOW TAGS; +--------+ | Name | +--------+ | player | +--------+ | team | +--------+ Got 2 rows (time spent 2038/2728 us) Execution Plan +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | id | name | dependencies | profiling data | operator info | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 0 | ShowTags | 2 | ver: 0, rows: 1, execTime: 79us, totalTime: 1692us | outputVar: [{\"colNames\":[],\"name\":\"__ShowTags_0\",\"type\":\"DATASET\"}] | | | | | | inputVar: | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ | 2 | Start | | ver: 0, rows: 0, execTime: 1us, totalTime: 57us | outputVar: [{\"colNames\":[],\"name\":\"__Start_2\",\"type\":\"DATASET\"}] | +----+----------+--------------+----------------------------------------------------+---------------------------------------------------------------------+ The descriptions of the columns are as follows: Column Description id Indicates the ID of the operator. name Indicates the name of the operator. dependencies Shows the ID of the operator that the current operator depends on. profiling data Shows the execution profile. ver is the version of the operator, which you can use to identify loops; rows shows the number of rows to be output by the operator; execTime shows the execution time only; totalTime contains the execution time and the system scheduling and queueing time. operator info Shows the detailed information of the operator.","title":"Format \"row\""},{"location":"3.ngql-guide/17.query-tuning-statements/1.explain-and-profile/#format_dot","text":"You can use the format=\"dot\" option to output the return message in the DOT language, and then use Graphviz to generate a graph of the plan. NOTE : Graphviz is open source graph visualization software. Graphviz provides an online tool for previewing DOT language files and exporting them to other formats such as SVG or JSON. For more information, see Graphviz Online . nebula> EXPLAIN format=\"dot\" SHOW TAGS; Execution succeeded (time spent 161/665 us) Execution Plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- plan --------------------------------------------------------------------------------------------------------------------------------------------- ------------- digraph exec_plan { rankdir=LR; \"ShowTags_0\"[label=\"ShowTags_0|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__ShowTags_0\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar:\\l\", shape=Mrecord]; \"Start_2\"->\"ShowTags_0\"; \"Start_2\"[label=\"Start_2|outputVar: \\[\\{\\\"colNames\\\":\\[\\],\\\"name\\\":\\\"__Start_2\\\",\\\"type\\\":\\\"DATASET\\\"\\}\\]\\l|inputVar: \\l\", shape=Mrecord]; } --------------------------------------------------------------------------------------------------------------------------------------------- ------------- Transformed into a Graphviz graph, it is as follows:","title":"Format \"dot\""},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/","text":"Job manager and the JOB statements \u00b6 The long-term tasks running by the Storage Service are called jobs. For example, there are jobs for COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data size in the graph space is large. The job manager helps you run, show, stop, and recover the jobs. SUBMIT JOB COMPACT \u00b6 The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation. nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+ For more information about compact configuration, see Storage Service configuration (doc TODO). SUBMIT JOB FLUSH \u00b6 The SUBMIT JOB FLUSH statement writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+ SUBMIT JOB STATS \u00b6 The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . NOTE: If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 97 | +------------+ SHOW JOB \u00b6 The Meta Service parses a SUBMIT JOB request into tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks. The job ID is created when you run the SUBMIT JOB statement. nebula> SHOW JOB 96; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ The description of the return message is as follows. Column Description Job Id(TaskId) The first row shows the job ID, and the other rows show the task IDs. Command(Dest) The first row shows the command executed, and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information about job status, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED . Job status \u00b6 The description of the job status is as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task failed. STOPPED The job or task is stopped without running. REMOVED The job or task is removed. Status switching is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/ SHOW JOBS \u00b6 The SHOW JOBS statement lists all the unexpired jobs. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration (doc TODO). nebula> SHOW JOBS; +--------+----------------------+------------+------------+------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+----------------------+------------+------------+------------+ | 97 | \"STATS\" | \"FINISHED\" | 1606546132 | 1606546132 | +--------+----------------------+------------+------------+------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 1606544069 | 1606544069 | +--------+----------------------+------------+------------+------------+ | 95 | \"STATS\" | \"FINISHED\" | 1606539731 | 1606539731 | +--------+----------------------+------------+------------+------------+ | 86 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 1606369104 | 1606369104 | +--------+----------------------+------------+------------+------------+ STOP JOB \u00b6 The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+ RECOVER JOB \u00b6 The RECOVER JOB statement re-executes the failed jobs and returns the number of recovered jobs. nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+ FAQ \u00b6 How to troubleshoot job problems \u00b6 The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:12000/admin?space={test}&op=compact\"","title":"Job manager and the JOB statements"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#job_manager_and_the_job_statements","text":"The long-term tasks running by the Storage Service are called jobs. For example, there are jobs for COMPACT , FLUSH , and STATS . These jobs can be time-consuming if the data size in the graph space is large. The job manager helps you run, show, stop, and recover the jobs.","title":"Job manager and the JOB statements"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_compact","text":"The SUBMIT JOB COMPACT statement triggers the long-term RocksDB compact operation. nebula> SUBMIT JOB COMPACT; +------------+ | New Job Id | +------------+ | 40 | +------------+ For more information about compact configuration, see Storage Service configuration (doc TODO).","title":"SUBMIT JOB COMPACT"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_flush","text":"The SUBMIT JOB FLUSH statement writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; +------------+ | New Job Id | +------------+ | 96 | +------------+","title":"SUBMIT JOB FLUSH"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#submit_job_stats","text":"The SUBMIT JOB STATS statement starts a job that makes the statistics of the current graph space. Once this job succeeds, you can use the SHOW STATS statement to list the statistics. For more information, see SHOW STATS . NOTE: If the data stored in the graph space changes, in order to get the latest statistics, you have to run SUBMIT JOB STATS again. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 97 | +------------+","title":"SUBMIT JOB STATS"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#show_job","text":"The Meta Service parses a SUBMIT JOB request into tasks and assigns them to the nebula-storaged processes. The SHOW JOB <job_id> statement shows the information about a specific job and all its tasks. The job ID is created when you run the SUBMIT JOB statement. nebula> SHOW JOB 96; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606544069 | 1606544069 | +----------------+---------------+------------+------------+------------+ The description of the return message is as follows. Column Description Job Id(TaskId) The first row shows the job ID, and the other rows show the task IDs. Command(Dest) The first row shows the command executed, and the other rows show on which storaged processes the task is running. Status Shows the status of the job or task. For more information about job status, see Job status . Start Time Shows a timestamp indicating the time when the job or task enters the RUNNING phase. Stop Time Shows a timestamp indicating the time when the job or task gets FINISHED , FAILED , or STOPPED .","title":"SHOW JOB "},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#job_status","text":"The description of the job status is as follows. Status Description QUEUE The job or task is waiting in a queue. The Start Time is empty in this phase. RUNNING The job or task is running. The Start Time shows the beginning of this phase. FINISHED The job or task is successfully finished. The Stop Time shows the time when the job or task enters this phase. FAILED The job or task failed. STOPPED The job or task is stopped without running. REMOVED The job or task is removed. Status switching is described as follows. Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/","title":"Job status"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#show_jobs","text":"The SHOW JOBS statement lists all the unexpired jobs. The default job expiration interval is one week. You can change it by modifying the job_expired_secs parameter of the Meta Service. For how to modify job_expired_secs , see Meta Service configuration (doc TODO). nebula> SHOW JOBS; +--------+----------------------+------------+------------+------------+ | Job Id | Command | Status | Start Time | Stop Time | +--------+----------------------+------------+------------+------------+ | 97 | \"STATS\" | \"FINISHED\" | 1606546132 | 1606546132 | +--------+----------------------+------------+------------+------------+ | 96 | \"FLUSH\" | \"FINISHED\" | 1606544069 | 1606544069 | +--------+----------------------+------------+------------+------------+ | 95 | \"STATS\" | \"FINISHED\" | 1606539731 | 1606539731 | +--------+----------------------+------------+------------+------------+ | 86 | \"REBUILD_EDGE_INDEX\" | \"FINISHED\" | 1606369104 | 1606369104 | +--------+----------------------+------------+------------+------------+","title":"SHOW JOBS"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#stop_job","text":"The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22; +---------------+ | Result | +---------------+ | \"Job stopped\" | +---------------+","title":"STOP JOB"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#recover_job","text":"The RECOVER JOB statement re-executes the failed jobs and returns the number of recovered jobs. nebula> RECOVER JOB; +-------------------+ | Recovered job num | +-------------------+ | 5 job recovered | +-------------------+","title":"RECOVER JOB"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/18.operation-and-maintenance-statements/4.job-statements/#how_to_troubleshoot_job_problems","text":"The SUBMIT JOB operations use the HTTP port. Please check if the HTTP ports on the machines where the Storage Service is running are working well. You can use the following command to debug. curl \"http://{storaged-ip}:12000/admin?space={test}&op=compact\"","title":"How to troubleshoot job problems"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/","text":"Statement list \u00b6 This document gives a list to the commonly used nGQL statements. To do a fast statement search, locate the target statement in this document and click the link. General query statements \u00b6 Statements Description SHOW STATS Shows the statistics of a graph space collected by the latest STATS job. MATCH Provides pattern search. LOOKUP Retrieves data based on indexes. Space statements \u00b6 Statements Description CREATE SPACE Creates a new graph space with the given name. USE SPACE Specifies a graph space as the current working space. SHOW SPACES Lists all the graph spaces in a Nebula Graph instance. DESCRIBE SPACE Returns information about a graph space. DROP SPACE Drops the given graph space. Vertex statements \u00b6 Statements Description INSERT VERTEX Inserts one or more vertices into a graph space. UPDATE VERTEX Updates properties on a vertex. UPSERT VERTEX Updates or inserts a vertex. DELETE VERTEX Deletes one or more vertices and their related edges. Edge statements \u00b6 Statements Description INSERT EDGE Inserts one or more edges into a graph space. UPDATE EDGE Updates properties on an edge. UPSERT EDGE Updates or inserts an edge. DELETE EDGE Deletes one or more edges. Native index statements \u00b6 Statements Description CREATE INDEX Creates native index for one or more properties of a tag or an edge type. SHOW INDEXES Lists the defined tag or edg type indexes names. DESCRIBE INDEX Gets information about the index. REBUILD INDEX Rebuilds the created tag or edg type index. DROP INDEX Removes an existing index from the current graph space. Full-text index statements \u00b6 Statements Description SIGN IN TEXT SERVICE Signs in to the Elasticsearch clients. SHOW TEXT SEARCH CLIENTS Lists the text search clients. SIGN OUT TEXT SERVICE Signs out all the text search clients. ADD LISTENER ELASTICSEARCH Adds the Listeners for a graph space. SHOW LISTENER Lists the Listeners. REMOVE LISTENER ELASTICSEARCH Removes all the Elasticsearch Listeners for a graph space. Subgraph and path statements \u00b6 Statements Description GET SUBGRAPH Retrieves subgraphs based on the specified conditions. Query tuning statements \u00b6 Statements Description EXPLAIN Returns the execution plan of an nGQL statement without executing the statement. PROFILE Executes the statement, then outputs the execution plan as well as the execution profile. Operation and maintenance statements \u00b6 Statements Description SUBMIT JOB COMPACT Triggers the long-term RocksDB compact operation. SUBMIT JOB FLUSH Writes the RocksDB memfile in memory to the hard disk. SUBMIT JOB STATS Starts a job that makes the statistics of the current graph space. SHOW JOB Shows the information of a specific job and all its tasks. STOP JOB Stops the jobs that are not finished. RECOVER JOB Re-executes the failed jobs and returns the number of recovered jobs.","title":"Search guide"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#statement_list","text":"This document gives a list to the commonly used nGQL statements. To do a fast statement search, locate the target statement in this document and click the link.","title":"Statement list"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#general_query_statements","text":"Statements Description SHOW STATS Shows the statistics of a graph space collected by the latest STATS job. MATCH Provides pattern search. LOOKUP Retrieves data based on indexes.","title":"General query statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#space_statements","text":"Statements Description CREATE SPACE Creates a new graph space with the given name. USE SPACE Specifies a graph space as the current working space. SHOW SPACES Lists all the graph spaces in a Nebula Graph instance. DESCRIBE SPACE Returns information about a graph space. DROP SPACE Drops the given graph space.","title":"Space statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#vertex_statements","text":"Statements Description INSERT VERTEX Inserts one or more vertices into a graph space. UPDATE VERTEX Updates properties on a vertex. UPSERT VERTEX Updates or inserts a vertex. DELETE VERTEX Deletes one or more vertices and their related edges.","title":"Vertex statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#edge_statements","text":"Statements Description INSERT EDGE Inserts one or more edges into a graph space. UPDATE EDGE Updates properties on an edge. UPSERT EDGE Updates or inserts an edge. DELETE EDGE Deletes one or more edges.","title":"Edge statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#native_index_statements","text":"Statements Description CREATE INDEX Creates native index for one or more properties of a tag or an edge type. SHOW INDEXES Lists the defined tag or edg type indexes names. DESCRIBE INDEX Gets information about the index. REBUILD INDEX Rebuilds the created tag or edg type index. DROP INDEX Removes an existing index from the current graph space.","title":"Native index statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#full-text_index_statements","text":"Statements Description SIGN IN TEXT SERVICE Signs in to the Elasticsearch clients. SHOW TEXT SEARCH CLIENTS Lists the text search clients. SIGN OUT TEXT SERVICE Signs out all the text search clients. ADD LISTENER ELASTICSEARCH Adds the Listeners for a graph space. SHOW LISTENER Lists the Listeners. REMOVE LISTENER ELASTICSEARCH Removes all the Elasticsearch Listeners for a graph space.","title":"Full-text index statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#subgraph_and_path_statements","text":"Statements Description GET SUBGRAPH Retrieves subgraphs based on the specified conditions.","title":"Subgraph and path statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#query_tuning_statements","text":"Statements Description EXPLAIN Returns the execution plan of an nGQL statement without executing the statement. PROFILE Executes the statement, then outputs the execution plan as well as the execution profile.","title":"Query tuning statements"},{"location":"3.ngql-guide/2.nav.md/1.search-guide/#operation_and_maintenance_statements","text":"Statements Description SUBMIT JOB COMPACT Triggers the long-term RocksDB compact operation. SUBMIT JOB FLUSH Writes the RocksDB memfile in memory to the hard disk. SUBMIT JOB STATS Starts a job that makes the statistics of the current graph space. SHOW JOB Shows the information of a specific job and all its tasks. STOP JOB Stops the jobs that are not finished. RECOVER JOB Re-executes the failed jobs and returns the number of recovered jobs.","title":"Operation and maintenance statements"},{"location":"3.ngql-guide/3.data-types/1.numeric/","text":"Numeric types \u00b6 Integer \u00b6 An integer is declared with keyword int , which is 64-bit signed . The supported range is [-9223372036854775808, 9223372036854775807]. Integer constants support multiple formats: Decimal, for example 123456 Hexadecimal, for example 0xdeadbeaf Octal, for example 01234567 Double floating point \u00b6 Double floating point data type is used for storing double precision floating point values. The keyword used for double floating point data type is double . There are no upper bounds and lower bounds.","title":"Numeric"},{"location":"3.ngql-guide/3.data-types/1.numeric/#numeric_types","text":"","title":"Numeric types"},{"location":"3.ngql-guide/3.data-types/1.numeric/#integer","text":"An integer is declared with keyword int , which is 64-bit signed . The supported range is [-9223372036854775808, 9223372036854775807]. Integer constants support multiple formats: Decimal, for example 123456 Hexadecimal, for example 0xdeadbeaf Octal, for example 01234567","title":"Integer"},{"location":"3.ngql-guide/3.data-types/1.numeric/#double_floating_point","text":"Double floating point data type is used for storing double precision floating point values. The keyword used for double floating point data type is double . There are no upper bounds and lower bounds.","title":"Double floating point"},{"location":"3.ngql-guide/3.data-types/2.boolean/","text":"Boolean \u00b6 A boolean data type is declared with the bool keyword and can only take the values true or false .","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/2.boolean/#boolean","text":"A boolean data type is declared with the bool keyword and can only take the values true or false .","title":"Boolean"},{"location":"3.ngql-guide/3.data-types/3.string/","text":"String \u00b6 The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example \"Shaquille O'Neal\" or '\"This is a double-quoted literal string\"' . Line breaks are not allowed in a string. Embedded escape sequences are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\" Nebula Graph supports two kind of strings: fixed length string and variable length string. For example: nebula> CREATE TAG t1 (p1 FIXED_STRING(10)); -- Fixed length string type nebula> CREATE TAG t2 (p2 string); -- Variable length string type","title":"String"},{"location":"3.ngql-guide/3.data-types/3.string/#string","text":"The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. For example \"Shaquille O'Neal\" or '\"This is a double-quoted literal string\"' . Line breaks are not allowed in a string. Embedded escape sequences are supported within strings, for example: \"\\n\\t\\r\\b\\f\" \"\\110ello world\" Nebula Graph supports two kind of strings: fixed length string and variable length string. For example: nebula> CREATE TAG t1 (p1 FIXED_STRING(10)); -- Fixed length string type nebula> CREATE TAG t2 (p2 string); -- Variable length string type","title":"String"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/","text":"Date and time types \u00b6 This document describes the DATE , TIME , DATETIME , and TIMESTAMP types. Nebula Graph converts the DATE , TIME , DATETIME , and TIMESTAMP values from the current time zone to UTC for storage. Nebula Graph converts back from UTC to the current time zone for retrieval. Combined with YIELD , functions date() , time() , datetime() all accept empty parameters to return the current date, time and datetime. DATE \u00b6 The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 . TIME \u00b6 The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss:usus format. The supported range is 0:0:0:0 to 23:59:59:999999 . DATETIME \u00b6 The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DD hh:mm:ss:usus format. The supported range is -32768-01-01 00:00:00:00 to 32767-12-31 23:59:59:999999 . TIMESTAMP \u00b6 The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of 1970-01-01 00:00:01 UTC to 2262-04-11 23:47:16 UTC. Timestamp is measured in units of seconds. Supported TIMESTAMP inserting methods: Call the now() function. Input TIMESTAMP by using a string. For example: 2019-10-01 10:00:00 . Input TIMESTAMP directly, namely the number of seconds from 1970-01-01 00:00:00 . The underlying storage data type is: int64 . Examples \u00b6 Create a tag named date. nebula> CREATE TAG date(p1 date, p2 time, p3 datetime); Insert a vertex named Date1. nebula> INSERT VERTEX date(p1, p2, p3) VALUES \"Date1\":(date(\"2017-03-04\"), time(\"23:01:00\"), datetime(\"2017-03-04T22:30:40\")); Create a tag named school. nebula> CREATE TAG school(name string , found_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01T08:00:00\" . nebula> INSERT VERTEX school(name, found_time) VALUES \"Stanford\":(\"Stanford\", timestamp(\"1885-10-01T08:00:00\")); Insert a vertex named \"dut\" with the foundation date now. nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now());","title":"Date and time"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date_and_time_types","text":"This document describes the DATE , TIME , DATETIME , and TIMESTAMP types. Nebula Graph converts the DATE , TIME , DATETIME , and TIMESTAMP values from the current time zone to UTC for storage. Nebula Graph converts back from UTC to the current time zone for retrieval. Combined with YIELD , functions date() , time() , datetime() all accept empty parameters to return the current date, time and datetime.","title":"Date and time types"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#date","text":"The DATE type is used for values with a date part but no time part. Nebula Graph retrieves and displays DATE values in the YYYY-MM-DD format. The supported range is -32768-01-01 to 32767-12-31 .","title":"DATE"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#time","text":"The TIME type is used for values with a time part but no date part. Nebula Graph retrieves and displays TIME values in hh:mm:ss:usus format. The supported range is 0:0:0:0 to 23:59:59:999999 .","title":"TIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#datetime","text":"The DATETIME type is used for values that contain both date and time parts. Nebula Graph retrieves and displays DATETIME values in YYYY-MM-DD hh:mm:ss:usus format. The supported range is -32768-01-01 00:00:00:00 to 32767-12-31 23:59:59:999999 .","title":"DATETIME"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#timestamp","text":"The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of 1970-01-01 00:00:01 UTC to 2262-04-11 23:47:16 UTC. Timestamp is measured in units of seconds. Supported TIMESTAMP inserting methods: Call the now() function. Input TIMESTAMP by using a string. For example: 2019-10-01 10:00:00 . Input TIMESTAMP directly, namely the number of seconds from 1970-01-01 00:00:00 . The underlying storage data type is: int64 .","title":"TIMESTAMP"},{"location":"3.ngql-guide/3.data-types/4.date-and-time/#examples","text":"Create a tag named date. nebula> CREATE TAG date(p1 date, p2 time, p3 datetime); Insert a vertex named Date1. nebula> INSERT VERTEX date(p1, p2, p3) VALUES \"Date1\":(date(\"2017-03-04\"), time(\"23:01:00\"), datetime(\"2017-03-04T22:30:40\")); Create a tag named school. nebula> CREATE TAG school(name string , found_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01T08:00:00\" . nebula> INSERT VERTEX school(name, found_time) VALUES \"Stanford\":(\"Stanford\", timestamp(\"1885-10-01T08:00:00\")); Insert a vertex named \"dut\" with the foundation date now. nebula> INSERT VERTEX school(name, found_time) VALUES \"dut\":(\"dut\", now());","title":"Examples"},{"location":"3.ngql-guide/3.data-types/5.null/","text":"NULL \u00b6 You can set the properties for vertices or edges to NULL . Also, you can set NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default. Examples \u00b6 Create a tag named player. Specify the property name with NOT NULL . Skip the property age constraint. nebula> CREATE TAG player(name string NOT NULL, age int); Execution succeeded (time spent 5001/5980 us) The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null); Execution succeeded (time spent 6367/7357 us)","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#null","text":"You can set the properties for vertices or edges to NULL . Also, you can set NOT NULL constraint to make sure that the property values are NOT NULL . If not specified, the property is set to NULL by default.","title":"NULL"},{"location":"3.ngql-guide/3.data-types/5.null/#examples","text":"Create a tag named player. Specify the property name with NOT NULL . Skip the property age constraint. nebula> CREATE TAG player(name string NOT NULL, age int); Execution succeeded (time spent 5001/5980 us) The property name is NOT NULL . The property age is NULL by default. nebula> SHOW CREATE TAG player; +-----------+-----------------------------------+ | Tag | Create Tag | +-----------+-----------------------------------+ | \"student\" | \"CREATE TAG `player` ( | | | `name` string NOT NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\"\" | +-----------+-----------------------------------+ nebula> INSERT VERTEX player(name, age) VALUES \"Kobe\":(\"Kobe\",null); Execution succeeded (time spent 6367/7357 us)","title":"Examples"},{"location":"3.ngql-guide/3.data-types/6.list/","text":"List \u00b6 List is a composite data type. A composite data type cannot be stored as properties. List is a sequence of values. Individual list elements can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with a comma , . Whitespace around elements is ignored in list, thus line breaks, tab stops, and blanks can be used for formatting. CAUTION: A composite data type (i.e. set, map, and list) cannot be stored as properties. Examples \u00b6 nebula> YIELD [1, 2, 3] AS List; +-----------+ | List | +-----------+ | [1, 2, 3] | +-----------+ nebula> YIELD range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ nebula> YIELD range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ nebula> YIELD [n IN range(1,5) WHERE n > 2] AS a; +-----------+ | a | +-----------+ | [3, 4, 5] | +-----------+ nebula> YIELD [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ nebula> YIELD [n IN range(1,5) | n + 10] AS a; +----------------------+ | a | +----------------------+ | [11, 12, 13, 14, 15] | +----------------------+ nebula> YIELD tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ nebula> YIELD [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ nebula> GO FROM \"player100\" OVER follow WHERE follow.degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD follow._dst AS id, follow.degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | +-------------+--------+ | \"player102\" | 90 | +-------------+--------+ nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ -> RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | +------------+ | [142, 133] | +------------+ nebula> YIELD size([1,2,3]); +---------------+ | size([1,2,3]) | +---------------+ | 3 | +---------------+ openCypher compatibility \u00b6 At this time, Nebula Graph does not support using ranges inside the brackets to return ranges of the list. nebula> YIELD range(0,5)[0..3]; [ERROR (-7)]: SyntaxError: syntax error near `3]' In openCypher, out-of-bound single elements returns null . However, in nGQL, out-of-bound single elements returns OUT_OF_RANGE . nebula> YIELD range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+","title":"List"},{"location":"3.ngql-guide/3.data-types/6.list/#list","text":"List is a composite data type. A composite data type cannot be stored as properties. List is a sequence of values. Individual list elements can be accessed by their positions. A list starts with a left square bracket [ and ends with a right square bracket ] . A list contains zero, one, or more expressions. List elements are separated from each other with a comma , . Whitespace around elements is ignored in list, thus line breaks, tab stops, and blanks can be used for formatting. CAUTION: A composite data type (i.e. set, map, and list) cannot be stored as properties.","title":"List"},{"location":"3.ngql-guide/3.data-types/6.list/#examples","text":"nebula> YIELD [1, 2, 3] AS List; +-----------+ | List | +-----------+ | [1, 2, 3] | +-----------+ nebula> YIELD range(1,5)[3]; +---------------+ | range(1,5)[3] | +---------------+ | 4 | +---------------+ nebula> YIELD range(1,5)[-2]; +------------------+ | range(1,5)[-(2)] | +------------------+ | 4 | +------------------+ nebula> YIELD [n IN range(1,5) WHERE n > 2] AS a; +-----------+ | a | +-----------+ | [3, 4, 5] | +-----------+ nebula> YIELD [n IN range(1,5) WHERE n > 2 | n + 10] AS a; +--------------+ | a | +--------------+ | [13, 14, 15] | +--------------+ nebula> YIELD [n IN range(1,5) | n + 10] AS a; +----------------------+ | a | +----------------------+ | [11, 12, 13, 14, 15] | +----------------------+ nebula> YIELD tail([n IN range(1, 5) | 2 * n - 10]) AS a; +-----------------+ | a | +-----------------+ | [-6, -4, -2, 0] | +-----------------+ nebula> YIELD [n IN range(1, 3) WHERE true | n] AS r; +-----------+ | r | +-----------+ | [1, 2, 3] | +-----------+ nebula> GO FROM \"player100\" OVER follow WHERE follow.degree NOT IN [x IN [92, 90] | x + $$.player.age] \\ YIELD follow._dst AS id, follow.degree AS degree; +-------------+--------+ | id | degree | +-------------+--------+ | \"player101\" | 95 | +-------------+--------+ | \"player102\" | 90 | +-------------+--------+ nebula> MATCH p = (n:player{name:\"Tim Duncan\"})-[:follow]->(m) \\ -> RETURN [n IN nodes(p) | n.age + 100] AS r; +------------+ | r | +------------+ | [142, 136] | +------------+ | [142, 133] | +------------+ nebula> YIELD size([1,2,3]); +---------------+ | size([1,2,3]) | +---------------+ | 3 | +---------------+","title":"Examples"},{"location":"3.ngql-guide/3.data-types/6.list/#opencypher_compatibility","text":"At this time, Nebula Graph does not support using ranges inside the brackets to return ranges of the list. nebula> YIELD range(0,5)[0..3]; [ERROR (-7)]: SyntaxError: syntax error near `3]' In openCypher, out-of-bound single elements returns null . However, in nGQL, out-of-bound single elements returns OUT_OF_RANGE . nebula> YIELD range(0,5)[-12]; +-------------------+ | range(0,5)[-(12)] | +-------------------+ | OUT_OF_RANGE | +-------------------+","title":"openCypher compatibility"},{"location":"3.ngql-guide/3.data-types/7.set/","text":"Set \u00b6 Set is a composite data type. A composite data type cannot be stored as properties. Set is a string object that can have zero or more unique values. Individual set elements must be chosen from the list of values 'value1', 'value2', ... CAUTION: A composite data type (i.e. set, map, and list) cannot be stored as properties.","title":"Set"},{"location":"3.ngql-guide/3.data-types/7.set/#set","text":"Set is a composite data type. A composite data type cannot be stored as properties. Set is a string object that can have zero or more unique values. Individual set elements must be chosen from the list of values 'value1', 'value2', ... CAUTION: A composite data type (i.e. set, map, and list) cannot be stored as properties.","title":"Set"},{"location":"3.ngql-guide/3.data-types/8.map/","text":"Map \u00b6 Map is a composite data type. A composite data type cannot be stored as properties. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] .","title":"Map"},{"location":"3.ngql-guide/3.data-types/8.map/#map","text":"Map is a composite data type. A composite data type cannot be stored as properties. Maps are unordered collections of key-value pairs. In maps, the key is a string. The value can have any data type. You can get the map element by using map['key'] .","title":"Map"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/","text":"Type Conversion \u00b6 Converting an expression of a given type to another type is known as type conversion. In nGQL, type conversion is divided into explicit conversion. Explicit type conversion \u00b6 In addition to implicit type conversion, explicit type conversion is also supported in case of semantics compliance. The syntax is similar to the C language: (type_name)expression . For example, the results of YIELD length((string)(123)), (int)\"123\" + 1 are 3, 124 respectively. The results of YIELD (int)(TRUE) is 1 . And YIELD (int)(\"12ab3\") fails in conversion.","title":"Type conversion"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#type_conversion","text":"Converting an expression of a given type to another type is known as type conversion. In nGQL, type conversion is divided into explicit conversion.","title":"Type Conversion"},{"location":"3.ngql-guide/3.data-types/9.type-conversion/#explicit_type_conversion","text":"In addition to implicit type conversion, explicit type conversion is also supported in case of semantics compliance. The syntax is similar to the C language: (type_name)expression . For example, the results of YIELD length((string)(123)), (int)\"123\" + 1 are 3, 124 respectively. The results of YIELD (int)(TRUE) is 1 . And YIELD (int)(\"12ab3\") fails in conversion.","title":"Explicit type conversion"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/","text":"Composite queries \u00b6 Composite queries put data from different queries together. They then use filters, group bys, or sorting before returning the combined return results. A composite query retrieves multiple levels of related information on existing queries and presents data as a single return result. Nebula Graph supports two ways to compose queries (or sub-queries): More than one queries can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. Queries can be piped together by using the pipe operator | . The result of the previous query can be used as the input of the next query. NOTE: Composition queries are not Transactional queries. For example, a query composed of three sub-queries: A | B | C. In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result can be undefined. Currently, there is no rollback. What is written depends on the query executor. Examples \u00b6 Semicolon queries nebula> SHOW TAGS; SHOW EDGES; // Only edges are shown. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); // Multiple vertices are inserted in a compose statement. Pipe queries nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ NOTE: Placeholder $-.id is not supported in Nebula Graph 2.0 or later versions.","title":"Composite queries"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#composite_queries","text":"Composite queries put data from different queries together. They then use filters, group bys, or sorting before returning the combined return results. A composite query retrieves multiple levels of related information on existing queries and presents data as a single return result. Nebula Graph supports two ways to compose queries (or sub-queries): More than one queries can be batched together, separated by semicolons (;). The result of the last query is returned as the result of the batch. Queries can be piped together by using the pipe operator | . The result of the previous query can be used as the input of the next query. NOTE: Composition queries are not Transactional queries. For example, a query composed of three sub-queries: A | B | C. In that A is a read operation, B is a computation operation, and C is a write operation. If any part fails in the execution, the whole result can be undefined. Currently, there is no rollback. What is written depends on the query executor.","title":"Composite queries"},{"location":"3.ngql-guide/4.variable-and-composite-queries/1.composite-queries/#examples","text":"Semicolon queries nebula> SHOW TAGS; SHOW EDGES; // Only edges are shown. nebula> INSERT VERTEX player(name, age) VALUES \"player100\":(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES \"player101\":(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES \"player102\":(\"LaMarcus Aldridge\", 33); // Multiple vertices are inserted in a compose statement. Pipe queries nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ NOTE: Placeholder $-.id is not supported in Nebula Graph 2.0 or later versions.","title":"Examples"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/","text":"User-defined variables \u00b6 User-defined variables allows passing the result of one statement to another. User-defined variables are written as $var_name . The var_name consists of alphanumeric characters. Any other characters are not recommended currently. User-defined variables can only be used in one execution. For example, you can use user-defined variables in composite queries separated by semicolon ; or pipe | . Details about composite queries, see Composite queries . NOTE: A user-defined variable is valid only at the current session and execution. A user-defined variable in one statement CANNOT be used in either other clients or other executions. The statement that defines the user-defined variable and the statement that uses it must be submitted together. When this session ends, the user-defined variable is automatically expired. User-defined variables are case-sensitive. Example \u00b6 nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+ openCypher Compatibility \u00b6 When you refer to a pattern, you need to name it first. The name you give to the pattern is a variable. This is the same as in openCypher. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v . NOTE: This kind of variables are not passed to subsequent queries.","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#user-defined_variables","text":"User-defined variables allows passing the result of one statement to another. User-defined variables are written as $var_name . The var_name consists of alphanumeric characters. Any other characters are not recommended currently. User-defined variables can only be used in one execution. For example, you can use user-defined variables in composite queries separated by semicolon ; or pipe | . Details about composite queries, see Composite queries . NOTE: A user-defined variable is valid only at the current session and execution. A user-defined variable in one statement CANNOT be used in either other clients or other executions. The statement that defines the user-defined variable and the statement that uses it must be submitted together. When this session ends, the user-defined variable is automatically expired. User-defined variables are case-sensitive.","title":"User-defined variables"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#example","text":"nebula> $var = GO FROM \"player100\" OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name AS Team, \\ $^.player.name AS Player; +---------+-------------+ | Team | Player | +---------+-------------+ | Nuggets | Tony Parker | +---------+-------------+","title":"Example"},{"location":"3.ngql-guide/4.variable-and-composite-queries/2.user-defined-variables/#opencypher_compatibility","text":"When you refer to a pattern, you need to name it first. The name you give to the pattern is a variable. This is the same as in openCypher. For example: nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The user-defined variable in the preceding query is v . NOTE: This kind of variables are not passed to subsequent queries.","title":"openCypher Compatibility"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/","text":"Property reference \u00b6 You can refer to the properties of a vertex or an edge in WHERE or YIELD syntax. Property reference for vertex \u00b6 For source vertex \u00b6 $^.tag_name.prop_name Symbol $^ is used to get the property of the source vertex, tag_name is the tag of the vertex, and prop_name specifies the property name. For destination vertex \u00b6 $$.tag_name.prop_name Symbol $$ is used to get the property of the destination vertex, tag_name is the tag of the vertex, and prop_name specifies the property name. Property reference for edge \u00b6 For property \u00b6 Use the following syntax to get the property of an edge. edge_type.prop_name edge_type is the edge type of the edge, and prop_name specifies the property name. For built-in properties \u00b6 There are four built-in properties in each edge: _src : source vertex ID of the edge _dst : destination vertex ID of the edge _type : edge type _rank : the rank value for the edge You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | +--------------+--------+ | \"Tim Duncan\" | 33 | +--------------+--------+ The preceding query returns the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ | 90 | +---------------+ The preceding query returns the degree property of the edge. nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 136 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 136 | 0 | +-------------+-------------+--------------+--------------+ The preceding query returns all the neighbors of vertex \"player100\" over edge type follow , by referencing follow._src as the source vertex ID (which, of course, is \"player100\" ) and follow._dst as the destination vertex ID.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference","text":"You can refer to the properties of a vertex or an edge in WHERE or YIELD syntax.","title":"Property reference"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_vertex","text":"","title":"Property reference for vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_source_vertex","text":"$^.tag_name.prop_name Symbol $^ is used to get the property of the source vertex, tag_name is the tag of the vertex, and prop_name specifies the property name.","title":"For source vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_destination_vertex","text":"$$.tag_name.prop_name Symbol $$ is used to get the property of the destination vertex, tag_name is the tag of the vertex, and prop_name specifies the property name.","title":"For destination vertex"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#property_reference_for_edge","text":"","title":"Property reference for edge"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_property","text":"Use the following syntax to get the property of an edge. edge_type.prop_name edge_type is the edge type of the edge, and prop_name specifies the property name.","title":"For property"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#for_built-in_properties","text":"There are four built-in properties in each edge: _src : source vertex ID of the edge _dst : destination vertex ID of the edge _type : edge type _rank : the rank value for the edge You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path.","title":"For built-in properties"},{"location":"3.ngql-guide/4.variable-and-composite-queries/3.property-reference/#examples","text":"nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +--------------+--------+ | startName | endAge | +--------------+--------+ | \"Tim Duncan\" | 36 | +--------------+--------+ | \"Tim Duncan\" | 33 | +--------------+--------+ The preceding query returns the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 95 | +---------------+ | 90 | +---------------+ The preceding query returns the degree property of the edge. nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 136 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 136 | 0 | +-------------+-------------+--------------+--------------+ The preceding query returns all the neighbors of vertex \"player100\" over edge type follow , by referencing follow._src as the source vertex ID (which, of course, is \"player100\" ) and follow._dst as the destination vertex ID.","title":"Examples"},{"location":"3.ngql-guide/5.operators/1.comparison/","text":"Comparison operators \u00b6 Name Description = Assign a value / Division operator == Equal operator != Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument IS NULL NULL test IS NOT NULL not NULL test Comparison operations result in a value of true and false . == Equal. String comparisons are case-sensitive. Values of different types are not equal. NOTE: The equality operator is == in nGQL and is = in openCypher. nebula> YIELD 'A' == 'a'; +--------+ | (A==a) | +--------+ | false | +--------+ nebula> YIELD '2' == 2; +--------+ | (2==2) | +--------+ | false | +--------+ > Greater than: nebula> YIELD 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ >= Greater than or equal to: nebula> YIELD 2 >= \"2\"; +--------+ | (2>=2) | +--------+ | false | +--------+ nebula> YIELD 2 >= 2; +--------+ | (2>=2) | +--------+ | true | +--------+ < Less than: nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+ <= Less than or equal to: nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+ != Not equal: nebula> YIELD 1 != '1'; +--------+ | (1!=1) | +--------+ | true | +--------+ NULL nebula> TODO","title":"Comparison"},{"location":"3.ngql-guide/5.operators/1.comparison/#comparison_operators","text":"Name Description = Assign a value / Division operator == Equal operator != Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument IS NULL NULL test IS NOT NULL not NULL test Comparison operations result in a value of true and false . == Equal. String comparisons are case-sensitive. Values of different types are not equal. NOTE: The equality operator is == in nGQL and is = in openCypher. nebula> YIELD 'A' == 'a'; +--------+ | (A==a) | +--------+ | false | +--------+ nebula> YIELD '2' == 2; +--------+ | (2==2) | +--------+ | false | +--------+ > Greater than: nebula> YIELD 3 > 2; +-------+ | (3>2) | +-------+ | true | +-------+ >= Greater than or equal to: nebula> YIELD 2 >= \"2\"; +--------+ | (2>=2) | +--------+ | false | +--------+ nebula> YIELD 2 >= 2; +--------+ | (2>=2) | +--------+ | true | +--------+ < Less than: nebula> YIELD 2.0 < 1.9; +---------+ | (2<1.9) | +---------+ | false | +---------+ <= Less than or equal to: nebula> YIELD 0.11 <= 0.11; +--------------+ | (0.11<=0.11) | +--------------+ | true | +--------------+ != Not equal: nebula> YIELD 1 != '1'; +--------+ | (1!=1) | +--------+ | true | +--------+ NULL nebula> TODO","title":"Comparison operators"},{"location":"3.ngql-guide/5.operators/2.boolean/","text":"Boolean operators \u00b6 Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR In nGQL, non-zero numbers are evaluated to true . For the precedence of the operators, refer to Operator Precedence . Implicit type conversion is not supported at this time. To combine numeric and string types with the boolean type, convert them to the boolean type. AND Logical AND: nebula> YIELD (bool)-1 AND TRUE; +-----------------------+ | ((BOOL)-(1) AND true) | +-----------------------+ | true | +-----------------------+ NOT Logical NOT: nebula> YIELD NOT (bool)-1; +---------------+ | !((BOOL)-(1)) | +---------------+ | false | +---------------+ OR Logical OR: nebula> YIELD (bool)1 OR !(bool)1; +-------------------------+ | ((BOOL)1 OR !((BOOL)1)) | +-------------------------+ | true | +-------------------------+ XOR Logical XOR: nebula> YIELD (NOT (bool)0 XOR (bool)0) AND (bool)0 XOR (bool)1 AS ret; +------+ | ret | +------+ | true | +------+","title":"Boolean"},{"location":"3.ngql-guide/5.operators/2.boolean/#boolean_operators","text":"Name Description AND Logical AND NOT Logical NOT OR Logical OR XOR Logical XOR In nGQL, non-zero numbers are evaluated to true . For the precedence of the operators, refer to Operator Precedence . Implicit type conversion is not supported at this time. To combine numeric and string types with the boolean type, convert them to the boolean type. AND Logical AND: nebula> YIELD (bool)-1 AND TRUE; +-----------------------+ | ((BOOL)-(1) AND true) | +-----------------------+ | true | +-----------------------+ NOT Logical NOT: nebula> YIELD NOT (bool)-1; +---------------+ | !((BOOL)-(1)) | +---------------+ | false | +---------------+ OR Logical OR: nebula> YIELD (bool)1 OR !(bool)1; +-------------------------+ | ((BOOL)1 OR !((BOOL)1)) | +-------------------------+ | true | +-------------------------+ XOR Logical XOR: nebula> YIELD (NOT (bool)0 XOR (bool)0) AND (bool)0 XOR (bool)1 AS ret; +------+ | ret | +------+ | true | +------+","title":"Boolean operators"},{"location":"3.ngql-guide/5.operators/4.pipe/","text":"Pipe operator \u00b6 One major difference between nGQL and SQL is how sub-queries are composed. In SQL, to form a statement, sub-queries are nested (embedded). In nGQL the shell style PIPE (|) is introduced. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS dstid, $$.player.name AS Name \\ | GO FROM $-.dstid OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ If no YIELD is used, the destination vertex ID is returned by default. If YIELD is declared explicitly, (the default value) id are not returned. To use pipe, always define alias names in the YIELD statement for the placeholder $-. . For example, the alias names are dstid and Name here.","title":"Pipe"},{"location":"3.ngql-guide/5.operators/4.pipe/#pipe_operator","text":"One major difference between nGQL and SQL is how sub-queries are composed. In SQL, to form a statement, sub-queries are nested (embedded). In nGQL the shell style PIPE (|) is introduced.","title":"Pipe operator"},{"location":"3.ngql-guide/5.operators/4.pipe/#examples","text":"nebula> GO FROM \"player100\" OVER follow \\ YIELD follow._dst AS dstid, $$.player.name AS Name \\ | GO FROM $-.dstid OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ If no YIELD is used, the destination vertex ID is returned by default. If YIELD is declared explicitly, (the default value) id are not returned. To use pipe, always define alias names in the YIELD statement for the placeholder $-. . For example, the alias names are dstid and Name here.","title":"Examples"},{"location":"3.ngql-guide/5.operators/5.property-reference/","text":"Property reference operator \u00b6 You can refer properties in WHERE or YIELD syntax. Reference from vertex \u00b6 For source vertex \u00b6 $^.<tag_name>.<prop_name> The symbol $^ is used to get the property of the source vertex, <tag_name> indicates the tag of the source vertex, and <prop_name> specifies the property name. For destination vertex \u00b6 $$.<tag_name>.<prop_name> The symbol $$ indicates the destination vertex, <tag_name> and <prop_name> are the tag and property of the destination vertex. Reference from edge \u00b6 For property \u00b6 To get properties of an edge, use the following syntax. <edge_type>.<edge_prop> The <edge_type> is the type of the edg. The <edge_prop> is the property of the edge. For Built-in Properties \u00b6 There are four built-in properties in the edge: _src: source vertex ID of the edge _dst: destination ID of the edge _type: edge type _rank: the edge's rank You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path. Examples \u00b6 nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +-----------+--------+ | startName | endAge | +-----------+--------+ | \"Tim\" | 36 | +-----------+--------+ | \"Tim\" | 33 | +-----------+--------+ Use the above query to get the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 96 | +---------------+ | 90 | +---------------+ nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 11 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 11 | 0 | +-------------+-------------+--------------+--------------+ This statement returns all the neighbors of vertex player100 over edge type follow , by referencing follow._src as the starting vertex ID (which, of course, is player100 ) and follow._dst as the ending vertex ID.","title":"Property reference"},{"location":"3.ngql-guide/5.operators/5.property-reference/#property_reference_operator","text":"You can refer properties in WHERE or YIELD syntax.","title":"Property reference operator"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_from_vertex","text":"","title":"Reference from vertex"},{"location":"3.ngql-guide/5.operators/5.property-reference/#for_source_vertex","text":"$^.<tag_name>.<prop_name> The symbol $^ is used to get the property of the source vertex, <tag_name> indicates the tag of the source vertex, and <prop_name> specifies the property name.","title":"For source vertex"},{"location":"3.ngql-guide/5.operators/5.property-reference/#for_destination_vertex","text":"$$.<tag_name>.<prop_name> The symbol $$ indicates the destination vertex, <tag_name> and <prop_name> are the tag and property of the destination vertex.","title":"For destination vertex"},{"location":"3.ngql-guide/5.operators/5.property-reference/#reference_from_edge","text":"","title":"Reference from edge"},{"location":"3.ngql-guide/5.operators/5.property-reference/#for_property","text":"To get properties of an edge, use the following syntax. <edge_type>.<edge_prop> The <edge_type> is the type of the edg. The <edge_prop> is the property of the edge.","title":"For property"},{"location":"3.ngql-guide/5.operators/5.property-reference/#for_built-in_properties","text":"There are four built-in properties in the edge: _src: source vertex ID of the edge _dst: destination ID of the edge _type: edge type _rank: the edge's rank You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path.","title":"For Built-in Properties"},{"location":"3.ngql-guide/5.operators/5.property-reference/#examples","text":"nebula> GO FROM \"player100\" OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; +-----------+--------+ | startName | endAge | +-----------+--------+ | \"Tim\" | 36 | +-----------+--------+ | \"Tim\" | 33 | +-----------+--------+ Use the above query to get the name property of the source vertex and the age property of the destination vertex. nebula> GO FROM \"player100\" OVER follow YIELD follow.degree; +---------------+ | follow.degree | +---------------+ | 96 | +---------------+ | 90 | +---------------+ nebula> GO FROM \"player100\" OVER follow YIELD follow._src, follow._dst, follow._type, follow._rank; +-------------+-------------+--------------+--------------+ | follow._src | follow._dst | follow._type | follow._rank | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player101\" | 11 | 0 | +-------------+-------------+--------------+--------------+ | \"player100\" | \"player102\" | 11 | 0 | +-------------+-------------+--------------+--------------+ This statement returns all the neighbors of vertex player100 over edge type follow , by referencing follow._src as the starting vertex ID (which, of course, is player100 ) and follow._dst as the ending vertex ID.","title":"Examples"},{"location":"3.ngql-guide/5.operators/6.set/","text":"Set operations \u00b6 This document descriptions the set operations, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use the set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph evaluates them from the left to right unless parentheses explicitly specify another order. To use the set operators, always match the return results of the GO clause with the same number and data type. UNION, UNION DISTINCT, and UNION ALL \u00b6 <left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without the duplicate elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion . Example \u00b6 The following statement nebula> GO FROM \"player102\" OVER follow \\ UNION \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns the neighbors' id of vertex \"player102\" and \"player100 (along with edge follow ) without duplication. While nebula> GO FROM \"player102\" OVER follow \\ UNION ALL \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns all the neighbors of vertex \"player102\" and \"player100 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 1 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 2 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ And the following statement nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns the follows: +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different. INTERSECT \u00b6 <left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the <left> and <right> must have the same number of columns and data types. Only the INTERSECT columns of <left> and <right> are returned. For example, the following query nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns Empty set (time spent 5194/6264 us) MINUS \u00b6 <left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A - B). Always pay attention to the order of the <left> and <right> . The set A - B consists of elements that are in A but not in B. For example, the following query nebula> GO FROM \"player100\" OVER follow \\ MINUS \\ GO FROM \"player102\" OVER follow; returns +-------------+ | follow._dst | +-------------+ | \"player102\" | +-------------+ If you reverse the MINUS order, the query nebula> GO FROM \"player102\" OVER follow \\ MINUS \\ GO FROM \"player100\" OVER follow; returns Empty set (time spent 2243/3259 us) Precedence of the SET Operations and Pipe \u00b6 Please note that when a query contains pipe | and set operations, pipe takes precedence. Refer to the Pipe Doc for details. Query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) . For example: nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; +-------------+ | play_dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ The statements in the red bar are executed first. And then the statement in the green box is executed. nebula> (GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; In the above query, the parentheses change the execution priority, and the statements within the parentheses take the precedence.","title":"Set"},{"location":"3.ngql-guide/5.operators/6.set/#set_operations","text":"This document descriptions the set operations, including UNION , UNION ALL , INTERSECT , and MINUS . To combine multiple queries, use the set operators. All set operators have equal precedence. If a nGQL statement contains multiple set operators, Nebula Graph evaluates them from the left to right unless parentheses explicitly specify another order. To use the set operators, always match the return results of the GO clause with the same number and data type.","title":"Set operations"},{"location":"3.ngql-guide/5.operators/6.set/#union_union_distinct_and_union_all","text":"<left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B without the duplicate elements. Operator UNION ALL returns the union of two sets A and B with duplicated elements. The <left> and <right> must have the same number of columns and data types. Different data types are converted according to the Type Conversion .","title":"UNION, UNION DISTINCT, and UNION ALL"},{"location":"3.ngql-guide/5.operators/6.set/#example","text":"The following statement nebula> GO FROM \"player102\" OVER follow \\ UNION \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns the neighbors' id of vertex \"player102\" and \"player100 (along with edge follow ) without duplication. While nebula> GO FROM \"player102\" OVER follow \\ UNION ALL \\ GO FROM \"player100\" OVER follow; +-------------+ | follow._dst | +-------------+ | \"player101\" | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ returns all the neighbors of vertex \"player102\" and \"player100 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 1 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ nebula> GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; -- query 2 +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ And the following statement nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ UNION /* DISTINCT */ \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns the follows: +-------------+--------+-----+ | id | Degree | Age | +-------------+--------+-----+ | \"player101\" | 75 | 36 | -- line 1 +-------------+--------+-----+ | \"player101\" | 96 | 36 | -- line 2 +-------------+--------+-----+ | \"player102\" | 90 | 33 | -- line 3 +-------------+--------+-----+ The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different.","title":"Example"},{"location":"3.ngql-guide/5.operators/6.set/#intersect","text":"<left> INTERSECT <right> Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B). Similar to UNION , the <left> and <right> must have the same number of columns and data types. Only the INTERSECT columns of <left> and <right> are returned. For example, the following query nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age \\ INTERSECT \\ GO FROM \"player100\" OVER follow YIELD follow._dst AS id, follow.degree AS Degree, $$.player.age AS Age; returns Empty set (time spent 5194/6264 us)","title":"INTERSECT"},{"location":"3.ngql-guide/5.operators/6.set/#minus","text":"<left> MINUS <right> Operator MINUS returns the subtraction (or difference) of two sets A and B (denoted by A - B). Always pay attention to the order of the <left> and <right> . The set A - B consists of elements that are in A but not in B. For example, the following query nebula> GO FROM \"player100\" OVER follow \\ MINUS \\ GO FROM \"player102\" OVER follow; returns +-------------+ | follow._dst | +-------------+ | \"player102\" | +-------------+ If you reverse the MINUS order, the query nebula> GO FROM \"player102\" OVER follow \\ MINUS \\ GO FROM \"player100\" OVER follow; returns Empty set (time spent 2243/3259 us)","title":"MINUS"},{"location":"3.ngql-guide/5.operators/6.set/#precedence_of_the_set_operations_and_pipe","text":"Please note that when a query contains pipe | and set operations, pipe takes precedence. Refer to the Pipe Doc for details. Query GO FROM 1 UNION GO FROM 2 | GO FROM 3 is the same as query GO FROM 1 UNION (GO FROM 2 | GO FROM 3) . For example: nebula> GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; +-------------+ | play_dst | +-------------+ | \"player101\" | +-------------+ | \"player102\" | +-------------+ The statements in the red bar are executed first. And then the statement in the green box is executed. nebula> (GO FROM \"player102\" OVER follow YIELD follow._dst AS play_dst \\ UNION \\ GO FROM \"team200\" OVER serve REVERSELY YIELD serve._dst AS play_dst) \\ | GO FROM $-.play_dst OVER follow YIELD follow._dst AS play_dst; In the above query, the parentheses change the execution priority, and the statements within the parentheses take the precedence.","title":"Precedence of the SET Operations and Pipe"},{"location":"3.ngql-guide/5.operators/7.string/","text":"String operators \u00b6 Name Description CONTAINS Perform case-sensitive inclusion searching in strings (NOT) IN Whether a value is within a set of values (NOT) STARTS WITH Perform case-sensitive matching on the beginning of a string (NOT) ENDS WITH Perform case-sensitive matching on the ending of a string Regular expressions Perform regular expression matching on a string NOTE: All the string matchings are case-sensitive. Examples \u00b6 CONTAINS The CONTAINS operator requires string type in both left and right side. nebula> GO FROM \"player101\" OVER serve WHERE $$.team.name CONTAINS \"ets\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Nuggets\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)serve.start_year CONTAINS \"19\" AND \\ $^.player.name CONTAINS \"ny\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Nuggets\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE !($$.team.name CONTAINS \"ets\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; Empty set (time spent 13040/14021 us) IN nebula> YIELD 1 IN [1,2,3]; +----------------+ | (1 IN [1,2,3]) | +----------------+ | true | +----------------+ nebula> YIELD \"Yao\" IN [\"Yi\", \"Tim\", \"Kobe\"]; +------------------------+ | (Yao IN [Yi,Tim,Kobe]) | +------------------------+ | false | +------------------------+ nebula> YIELD NULL in [\"Yi\", \"Tim\", \"Kobe\"]; +-----------------------------+ | (__NULL__ IN [Yi,Tim,Kobe]) | +-----------------------------+ | NULL | +-----------------------------+ (NOT) STARTS WITH nebula> YIELD 'apple' STARTS WITH 'app'; +-------------------------+ | (apple STARTS WITH app) | +-------------------------+ | true | +-------------------------+ nebula> YIELD 'apple' STARTS WITH 'a'; +-----------------------+ | (apple STARTS WITH a) | +-----------------------+ | true | +-----------------------+ nebula> YIELD 'apple' STARTS WITH 'A'; +-----------------------+ | (apple STARTS WITH A) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' STARTS WITH 'b'; +-----------------------+ | (apple STARTS WITH b) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' NOT STARTS WITH 'app'; +-----------------------------+ | (apple NOT STARTS WITH app) | +-----------------------------+ | false | +-----------------------------+ (NOT) ENDS WITH nebula> YIELD 'apple' ENDS WITH 'app'; +-----------------------+ | (apple ENDS WITH app) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' ENDS WITH 'e'; +---------------------+ | (apple ENDS WITH e) | +---------------------+ | true | +---------------------+ nebula> YIELD 'apple' ENDS WITH 'E'; +---------------------+ | (apple ENDS WITH E) | +---------------------+ | false | +---------------------+ nebula> YIELD 'apple' ENDS WITH 'b'; +---------------------+ | (apple ENDS WITH b) | +---------------------+ | false | +---------------------+ Regular expressions Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from st::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> FETCH PROP ON player WHERE player.name =~ 'Tony.*' YIELD player.name; The preceding statement returns the players whose name starts with 'Tony'. nebula> YIELD \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +----------------------------+ | (384748.39=~\\d+(\\.\\d{2})?) | +----------------------------+ | true | +----------------------------+","title":"String"},{"location":"3.ngql-guide/5.operators/7.string/#string_operators","text":"Name Description CONTAINS Perform case-sensitive inclusion searching in strings (NOT) IN Whether a value is within a set of values (NOT) STARTS WITH Perform case-sensitive matching on the beginning of a string (NOT) ENDS WITH Perform case-sensitive matching on the ending of a string Regular expressions Perform regular expression matching on a string NOTE: All the string matchings are case-sensitive.","title":"String operators"},{"location":"3.ngql-guide/5.operators/7.string/#examples","text":"CONTAINS The CONTAINS operator requires string type in both left and right side. nebula> GO FROM \"player101\" OVER serve WHERE $$.team.name CONTAINS \"ets\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Nuggets\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE (STRING)serve.start_year CONTAINS \"19\" AND \\ $^.player.name CONTAINS \"ny\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; +----------------+------------------+----------------+--------------+ | $^.player.name | serve.start_year | serve.end_year | $$.team.name | +----------------+------------------+----------------+--------------+ | \"Tony Parker\" | 1999 | 2018 | \"Nuggets\" | +----------------+------------------+----------------+--------------+ nebula> GO FROM \"player101\" OVER serve WHERE !($$.team.name CONTAINS \"ets\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; Empty set (time spent 13040/14021 us) IN nebula> YIELD 1 IN [1,2,3]; +----------------+ | (1 IN [1,2,3]) | +----------------+ | true | +----------------+ nebula> YIELD \"Yao\" IN [\"Yi\", \"Tim\", \"Kobe\"]; +------------------------+ | (Yao IN [Yi,Tim,Kobe]) | +------------------------+ | false | +------------------------+ nebula> YIELD NULL in [\"Yi\", \"Tim\", \"Kobe\"]; +-----------------------------+ | (__NULL__ IN [Yi,Tim,Kobe]) | +-----------------------------+ | NULL | +-----------------------------+ (NOT) STARTS WITH nebula> YIELD 'apple' STARTS WITH 'app'; +-------------------------+ | (apple STARTS WITH app) | +-------------------------+ | true | +-------------------------+ nebula> YIELD 'apple' STARTS WITH 'a'; +-----------------------+ | (apple STARTS WITH a) | +-----------------------+ | true | +-----------------------+ nebula> YIELD 'apple' STARTS WITH 'A'; +-----------------------+ | (apple STARTS WITH A) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' STARTS WITH 'b'; +-----------------------+ | (apple STARTS WITH b) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' NOT STARTS WITH 'app'; +-----------------------------+ | (apple NOT STARTS WITH app) | +-----------------------------+ | false | +-----------------------------+ (NOT) ENDS WITH nebula> YIELD 'apple' ENDS WITH 'app'; +-----------------------+ | (apple ENDS WITH app) | +-----------------------+ | false | +-----------------------+ nebula> YIELD 'apple' ENDS WITH 'e'; +---------------------+ | (apple ENDS WITH e) | +---------------------+ | true | +---------------------+ nebula> YIELD 'apple' ENDS WITH 'E'; +---------------------+ | (apple ENDS WITH E) | +---------------------+ | false | +---------------------+ nebula> YIELD 'apple' ENDS WITH 'b'; +---------------------+ | (apple ENDS WITH b) | +---------------------+ | false | +---------------------+ Regular expressions Nebula Graph supports filtering by using regular expressions. The regular expression syntax is inherited from st::regex . You can match on regular expressions by using =~ 'regexp' . For example: nebula> FETCH PROP ON player WHERE player.name =~ 'Tony.*' YIELD player.name; The preceding statement returns the players whose name starts with 'Tony'. nebula> YIELD \"384748.39\" =~ \"\\\\d+(\\\\.\\\\d{2})?\"; +----------------------------+ | (384748.39=~\\d+(\\.\\d{2})?) | +----------------------------+ | true | +----------------------------+","title":"Examples"},{"location":"3.ngql-guide/5.operators/9.precedence/","text":"Operator precedence \u00b6 The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) ! *, /, % -, + == , >=, >, <=, <, <>, != AND OR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses. Examples \u00b6 nebula> YIELD 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> YIELD (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+","title":"Precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#operator_precedence","text":"The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. - (negative number) ! *, /, % -, + == , >=, >, <=, <, <>, != AND OR = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses.","title":"Operator precedence"},{"location":"3.ngql-guide/5.operators/9.precedence/#examples","text":"nebula> YIELD 2+3*5; +-----------+ | (2+(3*5)) | +-----------+ | 17 | +-----------+ nebula> YIELD (2+3)*5; +-----------+ | ((2+3)*5) | +-----------+ | 25 | +-----------+","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/","text":"Built-in math functions \u00b6 Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns integral value nearest to the argument, Returnss a number farther away from 0 if the parameter is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) ComPutses the power of the argument. double exp(double x) Returns the value of e raised to the x power. double exp2(double x) Returns 2 raised to the argument. double log(double x) Returns natural logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns sine of the argument. double asin(double x) Returns inverse sine of the argument. double cos(double x) Returns cosine of the argument. double acos(double x) Returns inverse cosine of the argument. double tan(double x) Returns tangent of the argument. double atan(double x) Returns inverse tangent the argument. int rand32() Returns a random 32 bit integer. int rand32(int max) Returns a random 32 bit integer in [0, max). int rand32(int min, int max) Returns a random 32 bit integer in [min, max). int rand64() Returns a random 64 bit integer. int rand64(int max) Returns a random 64 bit integer in [0, max). int rand64(int min, int max) Returns a random 64 bit integer in [min, max). collect() Puts all the collected values to a list. collect_set() Puts all the collected values to a set. int size() Returns the number of elements in a list. map(fun, iter) Returns a map object after applying the given function to each item of a given iterable. int range(int, int, step) Returns a list of integers from the starting integer to the end integer in optional steps. The default step is one.","title":"Math"},{"location":"3.ngql-guide/6.functions-and-expressions/1.math/#built-in_math_functions","text":"Nebula Graph supports the following built-in math functions: Function Description double abs(double x) Returns absolute value of the argument. double floor(double x) Returns the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Returns the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Returns integral value nearest to the argument, Returnss a number farther away from 0 if the parameter is in the middle. double sqrt(double x) Returns the square root of the argument. double cbrt(double x) Returns the cubic root of the argument. double hypot(double x, double y) Returns the hypotenuse of a right-angled triangle. double pow(double x, double y) ComPutses the power of the argument. double exp(double x) Returns the value of e raised to the x power. double exp2(double x) Returns 2 raised to the argument. double log(double x) Returns natural logarithm of the argument. double log2(double x) Returns the base-2 logarithm of the argument. double log10(double x) Returns the base-10 logarithm of the argument. double sin(double x) Returns sine of the argument. double asin(double x) Returns inverse sine of the argument. double cos(double x) Returns cosine of the argument. double acos(double x) Returns inverse cosine of the argument. double tan(double x) Returns tangent of the argument. double atan(double x) Returns inverse tangent the argument. int rand32() Returns a random 32 bit integer. int rand32(int max) Returns a random 32 bit integer in [0, max). int rand32(int min, int max) Returns a random 32 bit integer in [min, max). int rand64() Returns a random 64 bit integer. int rand64(int max) Returns a random 64 bit integer in [0, max). int rand64(int min, int max) Returns a random 64 bit integer in [min, max). collect() Puts all the collected values to a list. collect_set() Puts all the collected values to a set. int size() Returns the number of elements in a list. map(fun, iter) Returns a map object after applying the given function to each item of a given iterable. int range(int, int, step) Returns a list of integers from the starting integer to the end integer in optional steps. The default step is one.","title":"Built-in math functions"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/","text":"Built-in string functions \u00b6 Nebula Graph supports the following built-in string functions: NOTE: Like SQL, the character index (location) for nGQL starts from 1 . However, the character index (i.e. location in a list) for the C language starts from 0 . Function Description int strcasecmp(string a, string b) Compares strings without case sensitivity, when a = b, Returns 0, when a > b Returnsed value is greater than 0, otherwise less than 0. string lower(string a) Returns the argument in lowercase. string upper(string a) Returns the argument in uppercase. int length(string a) Returns the length (int) of given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns the substring in [1, count], if length a is less than count, Returns a. string right(string a, int count) Returns the substring in [size - count + 1, size], if length a is less than count, Returns a. string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length. string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length. string substr(string a, int pos, int count) Extracts a substring from a string, starting at the specified position, extract the specified length characters. int hash() Takes in any data type and encodes it into an integer value. string reverse(string) Returns the reverse of a string. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. Explanations on the return results of function substr : If pos is 0, Returns an empty string. If the absolute value of pos is greater than the string, Returns an empty string. If pos is greater than 0, Returns substring in [pos, pos + count). If pos is less than 0, and set position N as length(a) + pos + 1, Returns substring in [N, N + count). If count is greater than length(a), Returns the whole string.","title":"String"},{"location":"3.ngql-guide/6.functions-and-expressions/2.string/#built-in_string_functions","text":"Nebula Graph supports the following built-in string functions: NOTE: Like SQL, the character index (location) for nGQL starts from 1 . However, the character index (i.e. location in a list) for the C language starts from 0 . Function Description int strcasecmp(string a, string b) Compares strings without case sensitivity, when a = b, Returns 0, when a > b Returnsed value is greater than 0, otherwise less than 0. string lower(string a) Returns the argument in lowercase. string upper(string a) Returns the argument in uppercase. int length(string a) Returns the length (int) of given string in bytes. string trim(string a) Removes leading and trailing spaces. string ltrim(string a) Removes leading spaces. string rtrim(string a) Removes trailing spaces. string left(string a, int count) Returns the substring in [1, count], if length a is less than count, Returns a. string right(string a, int count) Returns the substring in [size - count + 1, size], if length a is less than count, Returns a. string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length. string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length. string substr(string a, int pos, int count) Extracts a substring from a string, starting at the specified position, extract the specified length characters. int hash() Takes in any data type and encodes it into an integer value. string reverse(string) Returns the reverse of a string. string replace(string a, string b, string c) Replaces string b in string a with string c. list split(string a, string b) Splits string a at string b and returns a list of strings. string toString() Takes in any data type and converts it into a string. Explanations on the return results of function substr : If pos is 0, Returns an empty string. If the absolute value of pos is greater than the string, Returns an empty string. If pos is greater than 0, Returns substring in [pos, pos + count). If pos is less than 0, and set position N as length(a) + pos + 1, Returns substring in [N, N + count). If count is greater than length(a), Returns the whole string.","title":"Built-in string functions"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/","text":"Built-in date and time functions \u00b6 Nebula Graph supports the following built-in date and time functions: Function Description int now() Return the current date and time. date date() Return the current date based on the current system. time time() Return the current calendar time of the current time zone. datetime datetime() Return the current datetime based on the current time. The date(), time(), and datetime() functions accept three kind of parameters, namely empty, string, and map.","title":"Date and time"},{"location":"3.ngql-guide/6.functions-and-expressions/3.date-and-time/#built-in_date_and_time_functions","text":"Nebula Graph supports the following built-in date and time functions: Function Description int now() Return the current date and time. date date() Return the current date based on the current system. time time() Return the current calendar time of the current time zone. datetime datetime() Return the current datetime based on the current time. The date(), time(), and datetime() functions accept three kind of parameters, namely empty, string, and map.","title":"Built-in date and time functions"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/","text":"Built-in schema functions \u00b6 Nebula Graph supports the following built-in schema functions: Function Description string id(vertex) Returns the id of a vertex. list tags(vertex) Returns the tags of a vertex. list labels(vertex) Returns the tags of a vertex. map properties() Takes in a vertex or an edge and returns its properties. string type(edge) Returns the edge type of an edge. vertex startNode() Takes in an edge or a path and returns its source vertex ID. string endNode() Takes in an edge or a path and returns its destination vertex ID. int rank(edge) Returns the rank value of an edge.","title":"Schema"},{"location":"3.ngql-guide/6.functions-and-expressions/4.schema/#built-in_schema_functions","text":"Nebula Graph supports the following built-in schema functions: Function Description string id(vertex) Returns the id of a vertex. list tags(vertex) Returns the tags of a vertex. list labels(vertex) Returns the tags of a vertex. map properties() Takes in a vertex or an edge and returns its properties. string type(edge) Returns the edge type of an edge. vertex startNode() Takes in an edge or a path and returns its source vertex ID. string endNode() Takes in an edge or a path and returns its destination vertex ID. int rank(edge) Returns the rank value of an edge.","title":"Built-in schema functions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/","text":"CASE expressions \u00b6 The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD or RETURN clause. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression goes through conditions and returns a result when the first condition is met. Then the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . The following graph is used for the examples in this topic. The simple form of CASE expressions \u00b6 Syntax \u00b6 CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END CAUTION: Always remember to end a CASE expression with END . Parameters Description comparer A value or a valid expression that outputs a value. This value is used to compare with value . value It will be compared with comparer . If they match, then this condition is met. result It is returned by the CASE expression if value matches comparer . default It is returned by the CASE expression if no conditions are met. Examples \u00b6 Example 1: nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 188/583 us) Example 2: nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, \\ CASE $$.player.age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +---------------------+--------------+ | Name | Age_above_35 | +---------------------+--------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+--------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+--------------+ Got 2 rows (time spent 3910/4348 us) The generic form of CASE expressions \u00b6 Syntax \u00b6 CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameters Description condition If condition is evaluated as true, result is returned by the CASE expression. result It is returned by the CASE expression if condition is evaluated as true. default It is returned by the CASE expression if no conditions are met. Examples \u00b6 Example 1: nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 233/693 us) Example 2: nebula> MATCH (v:player) WHERE v.age > 30 \\ RETURN v.name AS Name, \\ CASE \\ WHEN v.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | +---------------------+---------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+---------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+ Got 3 rows (time spent 3859/4326 us) Differences between the simple form and the generic form \u00b6 To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, $$.player.age AS Age, \\ CASE $$.player.age \\ WHEN $$.player.age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +---------------------+-----+--------------+ | Name | Age | Age_above_35 | +---------------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | +---------------------+-----+--------------+ | \"LaMarcus Aldridge\" | 33 | \"No\" | +---------------------+-----+--------------+ Got 2 rows (time spent 2170/2642 us) The preceding GO query is intended to output \"Yes\" when the player age is above 35. However, in this example, when the player age is 36, the actual output is not as expected: It is \"No\" instead of \"Yes\". This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. This condition is not met and \"No\" is returned.","title":"Case expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#case_expressions","text":"The CASE expression uses conditions to filter the result of an nGQL query statement. It is usually used in the YIELD or RETURN clause. nGQL provides two forms of CASE expressions just like openCypher: the simple form and the generic form. The CASE expression goes through conditions and returns a result when the first condition is met. Then the CASE expression stops reading the conditions and returns the result. If no conditions are met, it returns the result in the ELSE clause. If there is no ELSE clause and no conditions are met, it returns NULL . The following graph is used for the examples in this topic.","title":"CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_simple_form_of_case_expressions","text":"","title":"The simple form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax","text":"CASE <comparer> WHEN <value> THEN <result> [WHEN ...] [ELSE <default>] END CAUTION: Always remember to end a CASE expression with END . Parameters Description comparer A value or a valid expression that outputs a value. This value is used to compare with value . value It will be compared with comparer . If they match, then this condition is met. result It is returned by the CASE expression if value matches comparer . default It is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples","text":"Example 1: nebula> YIELD \\ CASE 2+3 \\ WHEN 4 THEN 0 \\ WHEN 5 THEN 1 \\ ELSE -1 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 188/583 us) Example 2: nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, \\ CASE $$.player.age > 35 \\ WHEN true THEN \"Yes\" \\ WHEN false THEN \"No\" \\ ELSE \"Nah\" \\ END \\ AS Age_above_35; +---------------------+--------------+ | Name | Age_above_35 | +---------------------+--------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+--------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+--------------+ Got 2 rows (time spent 3910/4348 us)","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#the_generic_form_of_case_expressions","text":"","title":"The generic form of CASE expressions"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#syntax_1","text":"CASE WHEN <condition> THEN <result> [WHEN ...] [ELSE <default>] END Parameters Description condition If condition is evaluated as true, result is returned by the CASE expression. result It is returned by the CASE expression if condition is evaluated as true. default It is returned by the CASE expression if no conditions are met.","title":"Syntax"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#examples_1","text":"Example 1: nebula> YIELD \\ CASE WHEN 4 > 5 THEN 0 \\ WHEN 3+4==7 THEN 1 \\ ELSE 2 \\ END \\ AS result; +--------+ | result | +--------+ | 1 | +--------+ Got 1 rows (time spent 233/693 us) Example 2: nebula> MATCH (v:player) WHERE v.age > 30 \\ RETURN v.name AS Name, \\ CASE \\ WHEN v.name STARTS WITH \"T\" THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Starts_with_T; +---------------------+---------------+ | Name | Starts_with_T | +---------------------+---------------+ | \"Tim\" | \"Yes\" | +---------------------+---------------+ | \"LaMarcus Aldridge\" | \"No\" | +---------------------+---------------+ | \"Tony Parker\" | \"Yes\" | +---------------------+---------------+ Got 3 rows (time spent 3859/4326 us)","title":"Examples"},{"location":"3.ngql-guide/6.functions-and-expressions/5.case-expressions/#differences_between_the_simple_form_and_the_generic_form","text":"To avoid the misuse of the simple form and the generic form, it is important to understand their differences. The following example can help explain them. nebula> GO FROM \"player100\" OVER follow \\ YIELD $$.player.name AS Name, $$.player.age AS Age, \\ CASE $$.player.age \\ WHEN $$.player.age > 35 THEN \"Yes\" \\ ELSE \"No\" \\ END \\ AS Age_above_35; +---------------------+-----+--------------+ | Name | Age | Age_above_35 | +---------------------+-----+--------------+ | \"Tony Parker\" | 36 | \"No\" | +---------------------+-----+--------------+ | \"LaMarcus Aldridge\" | 33 | \"No\" | +---------------------+-----+--------------+ Got 2 rows (time spent 2170/2642 us) The preceding GO query is intended to output \"Yes\" when the player age is above 35. However, in this example, when the player age is 36, the actual output is not as expected: It is \"No\" instead of \"Yes\". This is because the query uses the CASE expression in the simple form, and a comparison between the values of $$.player.age and $$.player.age > 35 is made. When the player age is 36: The value of $$.player.age is 36 . It is an integer. $$.player.age > 35 is evaluated to true . It is a boolean. The values of $$.player.age and $$.player.age > 35 do not match. This condition is not met and \"No\" is returned.","title":"Differences between the simple form and the generic form"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/","text":"Built-in list functions \u00b6 Nebula Graph supports the following built-in list functions: Function Description empty head(list) Returns the first element of a list. empty last(list) Returns the last element of a list. empty coalesce(list) Returns the first not null value in a list.","title":"List"},{"location":"3.ngql-guide/6.functions-and-expressions/6.list/#built-in_list_functions","text":"Nebula Graph supports the following built-in list functions: Function Description empty head(list) Returns the first element of a list. empty last(list) Returns the last element of a list. empty coalesce(list) Returns the first not null value in a list.","title":"Built-in list functions"},{"location":"3.ngql-guide/7.general-query-statements/2.match/","text":"MATCH \u00b6 The MATCH statement provides the searching ability based on pattern matching. A MATCH statement defines a search pattern (doc TODO) and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. A WHERE clause (doc TODO) is often used together with the pattern as a filter to the search result. The examples in this topic use the nba dataset as the sample dataset. Syntax \u00b6 The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<WHERE clause>] RETURN <output> The workflow of MATCH \u00b6 The MATCH statement uses a native index to locate a source vertex. The vertex can be in any position in a pattern. In other words, in a valid MATCH statement, there must be an indexed property or tag, or a specific VID. For how to index a property, see Create native index . NOTE: The native index for VID is created by default, so you don't need to create an extra index if you want to match on VID. The MATCH statement searches through the pattern to match edges and other vertices. The MATCH statement retrieves data according to the RETURN clause. OpenCypher compatibility For now, nGQL DOES NOT support scanning all vertices and edges with MATCH . For example, MATCH (v) RETURN v . Use patterns in MATCH statements \u00b6 Make sure there is at least one index for the MATCH statement to use. If you want to create an index, but there are already vertices or edges related to the tag, edge type, or property that you want to create the index for, you have to rebuild the index after creation to make it take effect on existing vertices or edges. CAUTION: Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. nebula> CREATE TAG INDEX name ON player(name(20)); // Create an index on the name property. Execution succeeded (time spent 2957/3986 us) nebula> REBUILD TAG INDEX name; // Rebuild the index. +------------+ | New Job Id | +------------+ | 121 | +------------+ Got 1 rows (time spent 2676/3990 us) nebula> SHOW JOB 121; // Make sure the rebuild job succeeded. +----------------+---------------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------------+------------+------------+------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ Got 4 rows (time spent 1186/2998 us) Match a vertex \u00b6 You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) . Match on tag \u00b6 To match on a tag, make sure there is an applicable tag index . For how to create a tag index, see Create tag indexes . NOTE: Tag indexes are different from property indexes. If there is an index for a property of a tag, but no index for the tag, you cannot match on the tag. A vertex tag is specified with :<tag_name> in a pattern. nebula> MATCH (v:player) RETURN v +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +---------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +---------------------------------------------------------------+ | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +---------------------------------------------------------------+ ... Match on vertex property \u00b6 Tag properties are specified with {<prop_name>: <prop_value>} in a pattern after a tag. The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) WHERE v.name == \"Tim Duncan\" RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ OpenCypher compatibility In nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java). In openCypher 9, = is the equality operator. Match on VID \u00b6 You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) WHERE id(v) == 'player101' RETURN v; +---------------------------------------------------+ | v | +---------------------------------------------------+ | (player101) player.name:Tony Parker,player.age:36 | +---------------------------------------------------+ Got 1 rows (time spent 1710/2406 us) To match on multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ Got 3 rows (time spent 3107/3683 us) OpenCypher compatibility For now (till 2.0.0-beta), nGQL supports string-type VIDs only. (TODO: int VID) Match connected vertices \u00b6 You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. nGQL compatibility In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.0, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Marco Belinelli\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Aron Baynes\" | +---------------------+ ... Got 13 rows (time spent 6029/8976 us) And you can add a > or < to the -- symbol to specify the direction of an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2) RETURN v2.name AS Name; +-----------------+ | Name | +-----------------+ | \"Spurs\" | +-----------------+ | \"Tony Parker\" | +-----------------+ | \"Manu Ginobili\" | +-----------------+ Got 3 rows (time spent 2897/5993 us) In the preceding example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. To extend the pattern, add more edges and vertices. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"Tiago Splitter\" | +---------------------+ | \"Dejounte Murray\" | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ ... If you don't need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Rudy Gay\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Kyle Anderson\" | +---------------------+ ... Match paths \u00b6 Connected vertices and edges form a path. You can use a user-defined variable as follows to name a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p; +-----------------------------------------+ | p | +-----------------------------------------+ | (\"player100\")-[:serve@0]->(\"player204\") | +-----------------------------------------+ | (\"player100\")-[:like@0]->(\"player101\") | +-----------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\") | +-----------------------------------------+ Got 3 rows (time spent 3717/4573 us) OpenCypher compatibility In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such a concept. Match edges \u00b6 Besides using -- , --> , or <-- to indicate a nameless edge, you can use a variable in a pair of square brackets to represent a named edge. For example: -[e]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player101\")-[:like@0{likeness: 95}]->(\"player100\") | +---------------------------------------------------------------------------+ | (\"player102\")-[:like@0{likeness: 75}]->(\"player100\") | +---------------------------------------------------------------------------+ | (\"player104\")-[:like@0{likeness: 55}]->(\"player100\") | +---------------------------------------------------------------------------+ ... Match on edge types and properties \u00b6 Just like tags, edge types are specified with :<edge_type> . For example: -[e:serve]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:serve]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\") | +---------------------------------------------------------------------------+ Got 1 rows (time spent 5041/5630 us) And edge type properties are specified with {<prop_name>: <prop_value>} after the :<edge_type> . For example: [e:like{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:like{likeness:95}]->(v2) RETURN e; +------------------------------------------------------+ | e | +------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +------------------------------------------------------+ Got 2 rows (time spent 6080/6728 us) Match on multiple edge types \u00b6 The | symbol can help matching on multiple edge types. For example: [e:like|:serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:like|:serve]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\") | +---------------------------------------------------------------------------+ Got 3 rows (time spent 4264/4976 us) Match multiple edges \u00b6 You can expand a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3; +------------------------------------+-----------------------------------------------------------+ | v2 | v3 | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +------------------------------------+-----------------------------------------------------------+ ... Match fixed-length paths \u00b6 To match a fixed-length path, use the :<edge_type>*<hop> pattern. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*2]->(v2) RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 3 rows (time spent 4863/5591 us) Match variable-length paths \u00b6 You can use the :<edge_type>*[minHop]..<maxHop> pattern to match variable-length paths. Parameter Description minHop Optional. Represents the minimum length of the path. The default value is 1. maxHop Required. Represents the maximum length of the path. OpenCypher compatibility In nGQL, maxHop is required. And .. cannot be omitted. In openCypher, maxHop is optional and default to infinity. When no bounds are given, .. can be omitted. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 11 rows (time spent 5426/6473 us) You can use the DISTINCT keyword to remove duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*1..3]->(v2) \\ RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ Got 4 rows (time spent 4549/5162 us) Match variable-length paths with multiple edge types \u00b6 You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | +-----------------------------------------------------------+ | (\"player215\" :team{name: \"Hornets\"}) | +-----------------------------------------------------------+ Got 5 rows (time spent 3834/4571 us) Common retrieving operations \u00b6 This section shows how to retrieve commonly used items with MATCH statements. Retrieve vertex or edge information \u00b6 Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ Got 1 rows (time spent 1863/2545 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{start_year: 1997, end_year: 2016}]->(\"player204\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +---------------------------------------------------------------------------+ Got 3 rows (time spent 3139/3773 us) Retrieve VIDs \u00b6 Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+ Got 1 rows (time spent 2070/2747 us) Retrieve tags \u00b6 Use the labels() function to retrieve the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ Got 1 rows (time spent 2198/2941 us) To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to retrieve the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+ Got 1 rows (time spent 2609/3481 us) Retrieve a single property on a vertex or an edge \u00b6 Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-------+ | v.age | +-------+ | 42 | +-------+ Got 1 rows (time spent 2261/2973 us) Use AS to specify an alias for a property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-----+ | Age | +-----+ | 42 | +-----+ Got 1 rows (time spent 1762/2321 us) Retrieve all properties on a vertex or an edge \u00b6 Use the properties() function to retrieve all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2); +------------------------------------+ | properties(v2) | +------------------------------------+ | {\"name\":\"Spurs\"} | +------------------------------------+ | {\"name\":\"Tony Parker\", \"age\":36} | +------------------------------------+ | {\"age\":41, \"name\":\"Manu Ginobili\"} | +------------------------------------+ Got 3 rows (time spent 2943/3541 us) Retrieve edge types \u00b6 Use the type() function to retrieve the types of the matched edges. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN type(e); +---------+ | type(e) | +---------+ | \"serve\" | +---------+ | \"like\" | +---------+ | \"like\" | +---------+ Got 3 rows (time spent 3776/4660 us) Retrieve paths \u00b6 Use RETURN <path_name> to retrieve all the information of a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p LIMIT 3; +-------------------------------------------------------------------------------------------+ | p | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player100\")-[:serve@0]->(\"player204\") | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\")-[:serve@0]->(\"player204\") | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\")-[:like@0]->(\"player101\") | +-------------------------------------------------------------------------------------------+ Got 3 rows (time spent 4592/5278 us) Retrieve vertices in a path \u00b6 Use the nodes() function to retrieve all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+ Got 3 rows (time spent 2529/3128 us) Retrieve edges in a path \u00b6 Use the relationships() function to retrieve all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p); +-----------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\")] | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:like@0{likeness: 95}]->(\"player101\")] | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:like@0{likeness: 95}]->(\"player125\")] | +-----------------------------------------------------------------------------+ Got 3 rows (time spent 2715/3363 us) Retrieve path length \u00b6 Use the length() function to retrieve the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +------------------------------------------------------------------+--------+ | Paths | Length | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:serve@0]->(\"player204\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:serve@0]->(\"player215\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player100\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player102\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player125\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\")-[:serve@0]->(\"player204\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:serve@0]->(\"player204\") | 1 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\") | 1 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\") | 1 | +------------------------------------------------------------------+--------+ Got 10 rows (time spent 3774/4516 us)","title":"Match"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match","text":"The MATCH statement provides the searching ability based on pattern matching. A MATCH statement defines a search pattern (doc TODO) and uses it to match data stored in Nebula Graph and to retrieve them in the form defined in the RETURN clause. A WHERE clause (doc TODO) is often used together with the pattern as a filter to the search result. The examples in this topic use the nba dataset as the sample dataset.","title":"MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#syntax","text":"The syntax of MATCH is relatively more flexible compared with that of other query statements such as GO or LOOKUP . But generally, it can be summarized as follows. MATCH <pattern> [<WHERE clause>] RETURN <output>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#the_workflow_of_match","text":"The MATCH statement uses a native index to locate a source vertex. The vertex can be in any position in a pattern. In other words, in a valid MATCH statement, there must be an indexed property or tag, or a specific VID. For how to index a property, see Create native index . NOTE: The native index for VID is created by default, so you don't need to create an extra index if you want to match on VID. The MATCH statement searches through the pattern to match edges and other vertices. The MATCH statement retrieves data according to the RETURN clause. OpenCypher compatibility For now, nGQL DOES NOT support scanning all vertices and edges with MATCH . For example, MATCH (v) RETURN v .","title":"The workflow of MATCH"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#use_patterns_in_match_statements","text":"Make sure there is at least one index for the MATCH statement to use. If you want to create an index, but there are already vertices or edges related to the tag, edge type, or property that you want to create the index for, you have to rebuild the index after creation to make it take effect on existing vertices or edges. CAUTION: Correct use of indexes can speed up queries, but indexes can dramatically reduce the write performance. The performance reduction can be as much as 90% or even more. DO NOT use indexes in production environments unless you are fully aware of their influences on your service. nebula> CREATE TAG INDEX name ON player(name(20)); // Create an index on the name property. Execution succeeded (time spent 2957/3986 us) nebula> REBUILD TAG INDEX name; // Rebuild the index. +------------+ | New Job Id | +------------+ | 121 | +------------+ Got 1 rows (time spent 2676/3990 us) nebula> SHOW JOB 121; // Make sure the rebuild job succeeded. +----------------+---------------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------------+------------+------------+------------+ | 121 | \"REBUILD_TAG_INDEX\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1607073046 | 1607073046 | +----------------+---------------------+------------+------------+------------+ Got 4 rows (time spent 1186/2998 us)","title":"Use patterns in MATCH statements"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_a_vertex","text":"You can use a user-defined variable in a pair of parentheses to represent a vertex in a pattern. For example: (v) .","title":"Match a vertex"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_tag","text":"To match on a tag, make sure there is an applicable tag index . For how to create a tag index, see Create tag indexes . NOTE: Tag indexes are different from property indexes. If there is an index for a property of a tag, but no index for the tag, you cannot match on the tag. A vertex tag is specified with :<tag_name> in a pattern. nebula> MATCH (v:player) RETURN v +---------------------------------------------------------------+ | v | +---------------------------------------------------------------+ | (\"player102\" :player{age: 33, name: \"LaMarcus Aldridge\"}) | +---------------------------------------------------------------+ | (\"player106\" :player{age: 25, name: \"Kyle Anderson\"}) | +---------------------------------------------------------------+ | (\"player115\" :player{age: 40, name: \"Kobe Bryant\"}) | +---------------------------------------------------------------+ ...","title":"Match on tag"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_vertex_property","text":"Tag properties are specified with {<prop_name>: <prop_value>} in a pattern after a tag. The following example uses the name property to match a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ The WHERE clause can do the same thing: nebula> MATCH (v:player) WHERE v.name == \"Tim Duncan\" RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ OpenCypher compatibility In nGQL, == is the equality operator and = is the assignment operator (as in C++ or Java). In openCypher 9, = is the equality operator.","title":"Match on vertex property"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_vid","text":"You can use the VID to match a vertex. The id() function can retrieve the VID of a vertex. nebula> MATCH (v) WHERE id(v) == 'player101' RETURN v; +---------------------------------------------------+ | v | +---------------------------------------------------+ | (player101) player.name:Tony Parker,player.age:36 | +---------------------------------------------------+ Got 1 rows (time spent 1710/2406 us) To match on multiple VIDs, use WHERE id(v) IN [vid_list] . nebula> MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] RETURN v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ Got 3 rows (time spent 3107/3683 us) OpenCypher compatibility For now (till 2.0.0-beta), nGQL supports string-type VIDs only. (TODO: int VID)","title":"Match on VID"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_connected_vertices","text":"You can use the -- symbol to represent edges of both directions and match vertices connected by these edges. nGQL compatibility In nGQL 1.x, the -- symbol is used for inline comments. Starting from nGQL 2.0, the -- symbol represents an incoming or outgoing edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})--(v2) RETURN v2.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Marco Belinelli\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Aron Baynes\" | +---------------------+ ... Got 13 rows (time spent 6029/8976 us) And you can add a > or < to the -- symbol to specify the direction of an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2) RETURN v2.name AS Name; +-----------------+ | Name | +-----------------+ | \"Spurs\" | +-----------------+ | \"Tony Parker\" | +-----------------+ | \"Manu Ginobili\" | +-----------------+ Got 3 rows (time spent 2897/5993 us) In the preceding example, --> represents an edge that starts from v and points to v2 . To v , this is an outgoing edge, and to v2 this is an incoming edge. To extend the pattern, add more edges and vertices. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->(v2)<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"Tiago Splitter\" | +---------------------+ | \"Dejounte Murray\" | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ ... If you don't need to refer to a vertex, you can omit the variable representing it in the parentheses. nebula> MATCH (v:player{name:\"Tim Duncan\"})-->()<--(v3) RETURN v3.name AS Name; +---------------------+ | Name | +---------------------+ | \"Tony Parker\" | +---------------------+ | \"LaMarcus Aldridge\" | +---------------------+ | \"Rudy Gay\" | +---------------------+ | \"Danny Green\" | +---------------------+ | \"Kyle Anderson\" | +---------------------+ ...","title":"Match connected vertices"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_paths","text":"Connected vertices and edges form a path. You can use a user-defined variable as follows to name a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-->(v2) RETURN p; +-----------------------------------------+ | p | +-----------------------------------------+ | (\"player100\")-[:serve@0]->(\"player204\") | +-----------------------------------------+ | (\"player100\")-[:like@0]->(\"player101\") | +-----------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\") | +-----------------------------------------+ Got 3 rows (time spent 3717/4573 us) OpenCypher compatibility In nGQL, the @ symbol represents the rank of an edge, but openCypher has no such a concept.","title":"Match paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_edges","text":"Besides using -- , --> , or <-- to indicate a nameless edge, you can use a variable in a pair of square brackets to represent a named edge. For example: -[e]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player101\")-[:like@0{likeness: 95}]->(\"player100\") | +---------------------------------------------------------------------------+ | (\"player102\")-[:like@0{likeness: 75}]->(\"player100\") | +---------------------------------------------------------------------------+ | (\"player104\")-[:like@0{likeness: 55}]->(\"player100\") | +---------------------------------------------------------------------------+ ...","title":"Match edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_edge_types_and_properties","text":"Just like tags, edge types are specified with :<edge_type> . For example: -[e:serve]- . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:serve]-(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\") | +---------------------------------------------------------------------------+ Got 1 rows (time spent 5041/5630 us) And edge type properties are specified with {<prop_name>: <prop_value>} after the :<edge_type> . For example: [e:like{likeness:95}] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:like{likeness:95}]->(v2) RETURN e; +------------------------------------------------------+ | e | +------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +------------------------------------------------------+ Got 2 rows (time spent 6080/6728 us)","title":"Match on edge types and properties"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_on_multiple_edge_types","text":"The | symbol can help matching on multiple edge types. For example: [e:like|:serve] . nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e:like|:serve]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\") | +---------------------------------------------------------------------------+ Got 3 rows (time spent 4264/4976 us)","title":"Match on multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_multiple_edges","text":"You can expand a pattern to match multiple edges in a path. nebula> MATCH (v:player{name:\"Tim Duncan\"})-[]->(v2)<-[e:serve]-(v3) RETURN v2, v3; +------------------------------------+-----------------------------------------------------------+ | v2 | v3 | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +------------------------------------+-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | (\"player103\" :player{age: 32, name: \"Rudy Gay\"}) | +------------------------------------+-----------------------------------------------------------+ ...","title":"Match multiple edges"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_fixed-length_paths","text":"To match a fixed-length path, use the :<edge_type>*<hop> pattern. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*2]->(v2) RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 3 rows (time spent 4863/5591 us)","title":"Match fixed-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths","text":"You can use the :<edge_type>*[minHop]..<maxHop> pattern to match variable-length paths. Parameter Description minHop Optional. Represents the minimum length of the path. The default value is 1. maxHop Required. Represents the maximum length of the path. OpenCypher compatibility In nGQL, maxHop is required. And .. cannot be omitted. In openCypher, maxHop is optional and default to infinity. When no bounds are given, .. can be omitted. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*1..3]->(v2) \\ RETURN v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ Got 11 rows (time spent 5426/6473 us) You can use the DISTINCT keyword to remove duplicate results. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like*1..3]->(v2) \\ RETURN DISTINCT v2 AS Friends; +-----------------------------------------------------------+ | Friends | +-----------------------------------------------------------+ | (\"player101\" :player{name: \"Tony Parker\", age: 36}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ Got 4 rows (time spent 4549/5162 us)","title":"Match variable-length paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#match_variable-length_paths_with_multiple_edge_types","text":"You can specify multiple edge types in a fixed-length or variable-length pattern. In this case, hop , minHop , and maxHop take effect on all edge types. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e:like|serve*2]->(v2) \\ RETURN DISTINCT v2; +-----------------------------------------------------------+ | v2 | +-----------------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +-----------------------------------------------------------+ | (\"player102\" :player{name: \"LaMarcus Aldridge\", age: 33}) | +-----------------------------------------------------------+ | (\"player125\" :player{name: \"Manu Ginobili\", age: 41}) | +-----------------------------------------------------------+ | (\"player204\" :team{name: \"Spurs\"}) | +-----------------------------------------------------------+ | (\"player215\" :team{name: \"Hornets\"}) | +-----------------------------------------------------------+ Got 5 rows (time spent 3834/4571 us)","title":"Match variable-length paths with multiple edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#common_retrieving_operations","text":"This section shows how to retrieve commonly used items with MATCH statements.","title":"Common retrieving operations"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vertex_or_edge_information","text":"Use RETURN {<vertex_name> | <edge_name>} to retrieve all the information of a vertex or an edge. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v; +----------------------------------------------------+ | v | +----------------------------------------------------+ | (\"player100\" :player{name: \"Tim Duncan\", age: 42}) | +----------------------------------------------------+ Got 1 rows (time spent 1863/2545 us) nebula> MATCH (v:player{name:\"Tim Duncan\"})-[e]->(v2) RETURN e; +---------------------------------------------------------------------------+ | e | +---------------------------------------------------------------------------+ | (\"player100\")-[:serve@0{start_year: 1997, end_year: 2016}]->(\"player204\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player101\") | +---------------------------------------------------------------------------+ | (\"player100\")-[:like@0{likeness: 95}]->(\"player125\") | +---------------------------------------------------------------------------+ Got 3 rows (time spent 3139/3773 us)","title":"Retrieve vertex or edge information"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vids","text":"Use the id() function to retrieve VIDs. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN id(v); +-------------+ | id(v) | +-------------+ | \"player100\" | +-------------+ Got 1 rows (time spent 2070/2747 us)","title":"Retrieve VIDs"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_tags","text":"Use the labels() function to retrieve the list of tags on a vertex. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v); +------------+ | labels(v) | +------------+ | [\"player\"] | +------------+ Got 1 rows (time spent 2198/2941 us) To retrieve the nth element in the labels(v) list, use labels(v)[n-1] . The following example shows how to use labels(v)[0] to retrieve the first tag in the list. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN labels(v)[0]; +--------------+ | labels(v)[0] | +--------------+ | \"player\" | +--------------+ Got 1 rows (time spent 2609/3481 us)","title":"Retrieve tags"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_a_single_property_on_a_vertex_or_an_edge","text":"Use RETURN {<vertex_name> | <edge_name>}.<property> to retrieve a single property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-------+ | v.age | +-------+ | 42 | +-------+ Got 1 rows (time spent 2261/2973 us) Use AS to specify an alias for a property. nebula> MATCH (v:player{name:\"Tim Duncan\"}) RETURN v.age AS Age; +-----+ | Age | +-----+ | 42 | +-----+ Got 1 rows (time spent 1762/2321 us)","title":"Retrieve a single property on a vertex or an edge"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_all_properties_on_a_vertex_or_an_edge","text":"Use the properties() function to retrieve all properties on a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN properties(v2); +------------------------------------+ | properties(v2) | +------------------------------------+ | {\"name\":\"Spurs\"} | +------------------------------------+ | {\"name\":\"Tony Parker\", \"age\":36} | +------------------------------------+ | {\"age\":41, \"name\":\"Manu Ginobili\"} | +------------------------------------+ Got 3 rows (time spent 2943/3541 us)","title":"Retrieve all properties on a vertex or an edge"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_edge_types","text":"Use the type() function to retrieve the types of the matched edges. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[e]->() RETURN type(e); +---------+ | type(e) | +---------+ | \"serve\" | +---------+ | \"like\" | +---------+ | \"like\" | +---------+ Got 3 rows (time spent 3776/4660 us)","title":"Retrieve edge types"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_paths","text":"Use RETURN <path_name> to retrieve all the information of a vertex or an edge. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*3]->() RETURN p LIMIT 3; +-------------------------------------------------------------------------------------------+ | p | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player100\")-[:serve@0]->(\"player204\") | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\")-[:serve@0]->(\"player204\") | +-------------------------------------------------------------------------------------------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\")-[:like@0]->(\"player101\") | +-------------------------------------------------------------------------------------------+ Got 3 rows (time spent 4592/5278 us)","title":"Retrieve paths"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_vertices_in_a_path","text":"Use the nodes() function to retrieve all vertices in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN nodes(p); +---------------------------------------------------------------------------------------------------------------------+ | nodes(p) | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player204\" :team{name: \"Spurs\"})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player101\" :player{name: \"Tony Parker\", age: 36})] | +---------------------------------------------------------------------------------------------------------------------+ | [(\"player100\" :star{} :player{age: 42, name: \"Tim Duncan\"}), (\"player125\" :player{name: \"Manu Ginobili\", age: 41})] | +---------------------------------------------------------------------------------------------------------------------+ Got 3 rows (time spent 2529/3128 us)","title":"Retrieve vertices in a path"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_edges_in_a_path","text":"Use the relationships() function to retrieve all edges in a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[]->(v2) RETURN relationships(p); +-----------------------------------------------------------------------------+ | relationships(p) | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:serve@0{end_year: 2016, start_year: 1997}]->(\"player204\")] | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:like@0{likeness: 95}]->(\"player101\")] | +-----------------------------------------------------------------------------+ | [(\"player100\")-[:like@0{likeness: 95}]->(\"player125\")] | +-----------------------------------------------------------------------------+ Got 3 rows (time spent 2715/3363 us)","title":"Retrieve edges in a path"},{"location":"3.ngql-guide/7.general-query-statements/2.match/#retrieve_path_length","text":"Use the length() function to retrieve the length of a path. nebula> MATCH p=(v:player{name:\"Tim Duncan\"})-[*..2]->(v2) \\ RETURN p AS Paths, length(p) AS Length; +------------------------------------------------------------------+--------+ | Paths | Length | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:serve@0]->(\"player204\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:serve@0]->(\"player215\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player100\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player102\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\")-[:like@0]->(\"player125\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\")-[:serve@0]->(\"player204\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\")-[:like@0]->(\"player100\") | 2 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:serve@0]->(\"player204\") | 1 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player101\") | 1 | +------------------------------------------------------------------+--------+ | (\"player100\")-[:like@0]->(\"player125\") | 1 | +------------------------------------------------------------------+--------+ Got 10 rows (time spent 3774/4516 us)","title":"Retrieve path length"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/","text":"LOOKUP \u00b6 The LOOKUP statement retrieves data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type. Prerequisites \u00b6 Before using the LOOKUP statement, make sure that relative indexes are created. For how to create indexes, see CREATE INDEX . Syntax \u00b6 LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] [YIELD <return_list>] <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...] The WHERE clause filters data with the specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE (doc TODO). The YIELD clause specifies the results to be returned and the format of the results. It is not supported when there is no WHERE clause. If there is a WHERE clause but no YIELD clause: The Vertex ID is returned when LOOKUP a tag. The source vertex ID, destination vertex ID, and rank of the edge is returned when LOOKUP an edge type. Limitations of using WHERE in LOOKUP \u00b6 The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, expressions with field names on both sides of the operator are not supported, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. Range scan is not supported in the string-type index. The OR and XOR operations are not supported. Retrieve Vertices \u00b6 The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name, age); nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name== \"Kobe Bryant\" YIELD player.name AS name | \\ GO FROM $-.VertexID OVER serve YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------ Retrieve Edges \u00b6 The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree | \\ GO FROM $-.DstVID OVER serve YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ---------------------------------------------------------------- List vertices or edges with a tag or an edge type \u00b6 To list vertices or edges with a tag or an edge type, at least one index must exist on the tag or the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG player(name string,age int); Execution succeeded (time spent 3235/3865 us) nebula> CREATE TAG INDEX player_index on player(); Execution succeeded (time spent 3486/4124 us) nebula> INSERT VERTEX player(name,age) VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 1695/2268 us) nebula> LOOKUP ON player; +-------------+ | _vid | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ Got 2 rows (time spent 1514/2070 us) The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. nebula)> CREATE EDGE like(likeness int); Execution succeeded (time spent 3710/4483 us) nebula)> CREATE EDGE INDEX like_index on like(); Execution succeeded (time spent 3422/4026 us) nebula)> INSERT EDGE like(likeness) values \"player100\"->\"player101\":(95); Execution succeeded (time spent 1638/2351 us) nebula)> LOOKUP ON like; +-------------+----------+-------------+ | _src | _ranking | _dst | +-------------+----------+-------------+ | \"player100\" | 0 | \"player101\" | +-------------+----------+-------------+ Got 1 rows (time spent 1163/1748 us) Count the numbers of vertices or edges \u00b6 The following example shows how to count the number of vertices tagged with player and edges of the like edge type. nebula> LOOKUP ON player | YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 2 | +---------------+ Got 1 rows (time spent 1158/1864 us) nebula> LOOKUP ON like | YIELD COUNT(*) AS Like_Number; +-------------+ | Like_Number | +-------------+ | 1 | +-------------+ Got 1 rows (time spent 1190/1970 us) FAQ \u00b6 Error code 411 \u00b6 [ ERROR ( -8 )] : Unknown error ( 411 ) : Error code 411 shows there is no valid index for the current WHERE filter. Nebula Graph uses the left matching mode to select indexes. That is, columns in the WHERE filter must be in the first N columns of the index. For example: nebula> CREATE TAG INDEX example_index ON TAG t(p1, p2, p3); -- Create an index for the first 3 properties of tag t nebula> LOOKUP ON t WHERE p2 == 1 and p3 == 1; -- Not supported nebula> LOOKUP ON t WHERE p1 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1 and p3 == 1; -- Supported No valid index found \u00b6 No valid index found If your query filter contains a string type field, Nebula Graph selects the index that matches all the fields. For example: nebula> CREATE TAG t1 (c1 string, c2 int); nebula> CREATE TAG INDEX i1 ON t1 (c1, c2); nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\"; -- Index i1 is invalid nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\" and t1.c2 == 1; -- Index i1 is valid","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#lookup","text":"The LOOKUP statement retrieves data based on indexes. You can use LOOKUP for the following purposes: Search for the specific data based on conditions defined by the WHERE clause. List vertices with a tag: retrieve the VID of all vertices with a tag. List edges with an edge type: retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges with an edge type. Count the number of vertices or edges with a tag or an edge type.","title":"LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#prerequisites","text":"Before using the LOOKUP statement, make sure that relative indexes are created. For how to create indexes, see CREATE INDEX .","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#syntax","text":"LOOKUP ON {<vertex_tag> | <edge_type>} [WHERE <expression> [AND <expression> ...]] [YIELD <return_list>] <return_list> <prop_name> [AS <col_alias>] [, <prop_name> [AS <prop_alias>] ...] The WHERE clause filters data with the specified conditions. Both AND and OR are supported between different expressions. For more information, see WHERE (doc TODO). The YIELD clause specifies the results to be returned and the format of the results. It is not supported when there is no WHERE clause. If there is a WHERE clause but no YIELD clause: The Vertex ID is returned when LOOKUP a tag. The source vertex ID, destination vertex ID, and rank of the edge is returned when LOOKUP an edge type.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#limitations_of_using_where_in_lookup","text":"The WHERE clause in a LOOKUP statement does not support the following operations: $- and $^ . In relational expressions, expressions with field names on both sides of the operator are not supported, such as tagName.prop1> tagName.prop2 . Nested AliasProp expressions in operation expressions and function expressions are not supported. Range scan is not supported in the string-type index. The OR and XOR operations are not supported.","title":"Limitations of using WHERE in LOOKUP"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_vertices","text":"The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name, age); nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name== \"Kobe Bryant\" YIELD player.name AS name | \\ GO FROM $-.VertexID OVER serve YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------","title":"Retrieve Vertices"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#retrieve_edges","text":"The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree | \\ GO FROM $-.DstVID OVER serve YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ----------------------------------------------------------------","title":"Retrieve Edges"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#list_vertices_or_edges_with_a_tag_or_an_edge_type","text":"To list vertices or edges with a tag or an edge type, at least one index must exist on the tag or the edge type, or its property. For example, if there is a player tag with a name property and an age property, to retrieve the VID of all vertices tagged with player , there has to be an index on the player tag itself, the name property, or the age property. The following example shows how to retrieve the VID of all vertices tagged with player . nebula> CREATE TAG player(name string,age int); Execution succeeded (time spent 3235/3865 us) nebula> CREATE TAG INDEX player_index on player(); Execution succeeded (time spent 3486/4124 us) nebula> INSERT VERTEX player(name,age) VALUES \"player100\":(\"Tim Duncan\", 42), \"player101\":(\"Tony Parker\", 36); Execution succeeded (time spent 1695/2268 us) nebula> LOOKUP ON player; +-------------+ | _vid | +-------------+ | \"player100\" | +-------------+ | \"player101\" | +-------------+ Got 2 rows (time spent 1514/2070 us) The following example shows how to retrieve the source Vertex IDs, destination vertex IDs, and ranks of all edges of the like edge type. nebula)> CREATE EDGE like(likeness int); Execution succeeded (time spent 3710/4483 us) nebula)> CREATE EDGE INDEX like_index on like(); Execution succeeded (time spent 3422/4026 us) nebula)> INSERT EDGE like(likeness) values \"player100\"->\"player101\":(95); Execution succeeded (time spent 1638/2351 us) nebula)> LOOKUP ON like; +-------------+----------+-------------+ | _src | _ranking | _dst | +-------------+----------+-------------+ | \"player100\" | 0 | \"player101\" | +-------------+----------+-------------+ Got 1 rows (time spent 1163/1748 us)","title":"List vertices or edges with a tag or an edge type"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#count_the_numbers_of_vertices_or_edges","text":"The following example shows how to count the number of vertices tagged with player and edges of the like edge type. nebula> LOOKUP ON player | YIELD COUNT(*) AS Player_Number; +---------------+ | Player_Number | +---------------+ | 2 | +---------------+ Got 1 rows (time spent 1158/1864 us) nebula> LOOKUP ON like | YIELD COUNT(*) AS Like_Number; +-------------+ | Like_Number | +-------------+ | 1 | +-------------+ Got 1 rows (time spent 1190/1970 us)","title":"Count the numbers of vertices or edges"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#faq","text":"","title":"FAQ"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#error_code_411","text":"[ ERROR ( -8 )] : Unknown error ( 411 ) : Error code 411 shows there is no valid index for the current WHERE filter. Nebula Graph uses the left matching mode to select indexes. That is, columns in the WHERE filter must be in the first N columns of the index. For example: nebula> CREATE TAG INDEX example_index ON TAG t(p1, p2, p3); -- Create an index for the first 3 properties of tag t nebula> LOOKUP ON t WHERE p2 == 1 and p3 == 1; -- Not supported nebula> LOOKUP ON t WHERE p1 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1; -- Supported nebula> LOOKUP ON t WHERE p1 == 1 and p2 == 1 and p3 == 1; -- Supported","title":"Error code 411"},{"location":"3.ngql-guide/7.general-query-statements/5.lookup/#no_valid_index_found","text":"No valid index found If your query filter contains a string type field, Nebula Graph selects the index that matches all the fields. For example: nebula> CREATE TAG t1 (c1 string, c2 int); nebula> CREATE TAG INDEX i1 ON t1 (c1, c2); nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\"; -- Index i1 is invalid nebula> LOOKUP ON t1 WHERE t1.c1 == \"a\" and t1.c2 == 1; -- Index i1 is valid","title":"No valid index found"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/","text":"SHOW CHARSET \u00b6 The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent. Syntax \u00b6 SHOW CHARSET Example \u00b6 nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Got 1 rows (time spent 527/1269 us) The output of SHOW CHARSET is explained as follows: Column Description Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#show_charset","text":"The SHOW CHARSET statement shows the available character sets. Currently available types are utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four-byte characters. Therefore utf8 and utf8mb4 are equivalent.","title":"SHOW CHARSET"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#syntax","text":"SHOW CHARSET","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/1.show-charset/#example","text":"nebula> SHOW CHARSET; +---------+-----------------+-------------------+--------+ | Charset | Description | Default collation | Maxlen | +---------+-----------------+-------------------+--------+ | \"utf8\" | \"UTF-8 Unicode\" | \"utf8_bin\" | 4 | +---------+-----------------+-------------------+--------+ Got 1 rows (time spent 527/1269 us) The output of SHOW CHARSET is explained as follows: Column Description Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/","text":"SHOW ROLES \u00b6 The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space. If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user doesn't have a role, Nebula Graph shows no role information. For more information about user roles, see GRANT ROLE (doc TODO). Syntax \u00b6 SHOW ROLES IN <space_name> Example \u00b6 nebula> SHOW ROLES in nba; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+ Got 1 rows (time spent 789/1594 us)","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#show_roles","text":"The SHOW ROLES statement shows the roles that are assigned to a user account. The return message differs according to the role of the user who is running this statement: If the user is a GOD or ADMIN and is granted access to the specified graph space, Nebula Graph shows all roles in this graph space. If the user is a DBA , USER , or GUEST and is granted access to the specified graph space, Nebula Graph shows the user's own role in this graph space. If the user doesn't have a role, Nebula Graph shows no role information. For more information about user roles, see GRANT ROLE (doc TODO).","title":"SHOW ROLES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#syntax","text":"SHOW ROLES IN <space_name>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/10.show-roles/#example","text":"nebula> SHOW ROLES in nba; +---------+-----------+ | Account | Role Type | +---------+-----------+ | \"user1\" | \"ADMIN\" | +---------+-----------+ Got 1 rows (time spent 789/1594 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/","text":"SHOW SNAPSHOTS \u00b6 The SHOW SNAPSHOTS statement shows all the snapshots. For how to create a snapshot and back up data, see CREATE SNAPSHOT (TODO). Role requirement \u00b6 Only the root user who has the GOD role can use this statement. Syntax \u00b6 SHOW SNAPSHOTS Example \u00b6 nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:44500, storaged1:44500, storaged2:44500\" | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:44500, storaged1:44500, storaged2:44500\" | +--------------------------------+---------+-----------------------------------------------------+ Got 2 rows (time spent 762/1434 us)","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#show_snapshots","text":"The SHOW SNAPSHOTS statement shows all the snapshots. For how to create a snapshot and back up data, see CREATE SNAPSHOT (TODO).","title":"SHOW SNAPSHOTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#role_requirement","text":"Only the root user who has the GOD role can use this statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#syntax","text":"SHOW SNAPSHOTS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/11.show-snapshots/#example","text":"nebula> SHOW SNAPSHOTS; +--------------------------------+---------+-----------------------------------------------------+ | Name | Status | Hosts | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_13_55\" | \"VALID\" | \"storaged0:44500, storaged1:44500, storaged2:44500\" | +--------------------------------+---------+-----------------------------------------------------+ | \"SNAPSHOT_2020_12_16_11_14_10\" | \"VALID\" | \"storaged0:44500, storaged1:44500, storaged2:44500\" | +--------------------------------+---------+-----------------------------------------------------+ Got 2 rows (time spent 762/1434 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/","text":"SHOW SPACES \u00b6 The SHOW SPACES statement shows the graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE . Syntax \u00b6 SHOW SPACES Example \u00b6 nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"docs\" | +--------+ | \"nba\" | +--------+ Got 2 rows (time spent 968/1893 us)","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#show_spaces","text":"The SHOW SPACES statement shows the graph spaces in Nebula Graph. For how to create a graph space, see CREATE SPACE .","title":"SHOW SPACES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#syntax","text":"SHOW SPACES","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/12.show-spaces/#example","text":"nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"docs\" | +--------+ | \"nba\" | +--------+ Got 2 rows (time spent 968/1893 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/","text":"SHOW STATS \u00b6 The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics list the following information: The number of vertices and edges in the graph space The number of vertices with each tag The number of edges of each edge type Prerequisites \u00b6 You have successfully run the SUBMIT JOB STATS statement in the graph space you want to collect statistics. For more information, see SUBMIT JOB STATS . NOTE: The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again. Syntax \u00b6 SHOW STATS Example \u00b6 nebula> USE nba; Execution succeeded (time spent 1075/1646 us) --Start a `STATS` job. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ Got 1 rows (time spent 2058/2609 us) --Make sure the job is finished. nebula> SHOW JOB 98; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 98 | \"STATS\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ Got 4 rows (time spent 1233/1924 us) --Check the statistics. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | +---------+------------+-------+ | \"Tag\" | \"team\" | 30 | +---------+------------+-------+ | \"Edge\" | \"like\" | 81 | +---------+------------+-------+ | \"Edge\" | \"serve\" | 152 | +---------+------------+-------+ | \"Space\" | \"vertices\" | 81 | +---------+------------+-------+ | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows (time spent 996/1637 us)","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#show_stats","text":"The SHOW STATS statement shows the statistics of the graph space collected by the latest STATS job. The statistics list the following information: The number of vertices and edges in the graph space The number of vertices with each tag The number of edges of each edge type","title":"SHOW STATS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#prerequisites","text":"You have successfully run the SUBMIT JOB STATS statement in the graph space you want to collect statistics. For more information, see SUBMIT JOB STATS . NOTE: The result of the SHOW STATS statement is based on the last executed SUBMIT JOB STATS statement. If you want to update the result, run SUBMIT JOB STATS again.","title":"Prerequisites"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#syntax","text":"SHOW STATS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/14.show-stats/#example","text":"nebula> USE nba; Execution succeeded (time spent 1075/1646 us) --Start a `STATS` job. nebula> SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 98 | +------------+ Got 1 rows (time spent 2058/2609 us) --Make sure the job is finished. nebula> SHOW JOB 98; +----------------+---------------+------------+------------+------------+ | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | +----------------+---------------+------------+------------+------------+ | 98 | \"STATS\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 0 | \"storaged2\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 1 | \"storaged0\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ | 2 | \"storaged1\" | \"FINISHED\" | 1606552675 | 1606552675 | +----------------+---------------+------------+------------+------------+ Got 4 rows (time spent 1233/1924 us) --Check the statistics. nebula> SHOW STATS; +---------+------------+-------+ | Type | Name | Count | +---------+------------+-------+ | \"Tag\" | \"player\" | 51 | +---------+------------+-------+ | \"Tag\" | \"team\" | 30 | +---------+------------+-------+ | \"Edge\" | \"like\" | 81 | +---------+------------+-------+ | \"Edge\" | \"serve\" | 152 | +---------+------------+-------+ | \"Space\" | \"vertices\" | 81 | +---------+------------+-------+ | \"Space\" | \"edges\" | 233 | +---------+------------+-------+ Got 6 rows (time spent 996/1637 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/","text":"SHOW TAGS/EDGES \u00b6 The SHOW TAGS or SHOW EDGES statement shows all tags or edge types in the current graph space. Syntax \u00b6 SHOW {TAGS | EDGES} Examples \u00b6 Show tags: nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"star\" | +----------+ | \"team\" | +----------+ Got 3 rows (time spent 1461/2114 us) Show edge types\uff1a nebula> SHOW EDGES; +---------+ | Name | +---------+ | \"like\" | +---------+ | \"serve\" | +---------+ Got 2 rows (time spent 1039/1687 us)","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#show_tagsedges","text":"The SHOW TAGS or SHOW EDGES statement shows all tags or edge types in the current graph space.","title":"SHOW TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#syntax","text":"SHOW {TAGS | EDGES}","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/15.show-tags-edges/#examples","text":"Show tags: nebula> SHOW TAGS; +----------+ | Name | +----------+ | \"player\" | +----------+ | \"star\" | +----------+ | \"team\" | +----------+ Got 3 rows (time spent 1461/2114 us) Show edge types\uff1a nebula> SHOW EDGES; +---------+ | Name | +---------+ | \"like\" | +---------+ | \"serve\" | +---------+ Got 2 rows (time spent 1039/1687 us)","title":"Examples"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/","text":"SHOW USERS \u00b6 The SHOW USERS statement shows the user information. Role requirement \u00b6 Only the root user who has the GOD role can use this statement. Syntax \u00b6 SHOW USERS Example \u00b6 nebula> SHOW USERS; +---------+ | Account | +---------+ | \"root\" | +---------+ | \"user1\" | +---------+ Got 2 rows (time spent 964/1691 us)","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#show_users","text":"The SHOW USERS statement shows the user information.","title":"SHOW USERS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#role_requirement","text":"Only the root user who has the GOD role can use this statement.","title":"Role requirement"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#syntax","text":"SHOW USERS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/16.show-users/#example","text":"nebula> SHOW USERS; +---------+ | Account | +---------+ | \"root\" | +---------+ | \"user1\" | +---------+ Got 2 rows (time spent 964/1691 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/","text":"SHOW COLLATION \u00b6 The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin, and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive. Syntax \u00b6 SHOW COLLATION Example \u00b6 nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Got 1 rows (time spent 413/1034 us) The output of SHOW CHARSET is described as follows: Column Description Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#show_collation","text":"The SHOW COLLATION statement shows the collations supported by Nebula Graph. Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin, and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive.","title":"SHOW COLLATION"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#syntax","text":"SHOW COLLATION","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/2.show-collation/#example","text":"nebula> SHOW COLLATION; +------------+---------+ | Collation | Charset | +------------+---------+ | \"utf8_bin\" | \"utf8\" | +------------+---------+ Got 1 rows (time spent 413/1034 us) The output of SHOW CHARSET is described as follows: Column Description Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/3.show-configs/","text":"SHOW CONFIGS \u00b6 The SHOW CONFIGS statement lists the mutable configurations of the Graph Service, Meta Service, or Storage Service. For how to update the configurations through nGQL, see CONFIGS syntax (doc TODO). Syntax \u00b6 SHOW CONFIGS [GRAPH|META|STORAGE] Option Description GRAPH Shows the configuration of the Graph Service. META Shows the configuration of the Meta Service. STORAGE Shows the configuration of the Meta Service. If no service name is set in the statement, Nebula Graph shows the mutable configurations of all services. Example \u00b6 nebula> SHOW CONFIGS GRAPH; +---------+---------------------------+-------+-----------+-------+ | module | name | type | mode | value | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"v\" | \"int\" | \"MUTABLE\" | 0 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"minloglevel\" | \"int\" | \"MUTABLE\" | 0 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"slow_op_threshhold_ms\" | \"int\" | \"MUTABLE\" | 50 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"heartbeat_interval_secs\" | \"int\" | \"MUTABLE\" | 3 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"meta_client_retry_times\" | \"int\" | \"MUTABLE\" | 3 | +---------+---------------------------+-------+-----------+-------+ Got 6 rows (time spent 1216/1880 us) The output of SHOW CONFIGS is explained as follows: Column Description module The Nebula Graph service name. name The parameter name. type The data type of the value. mode Shows whether the parameter can be modified or not. value The value of the parameter. For more information about the Nebula Graph configurations, see: Graph configuration (doc TODO) Storage configuration (doc TODO)","title":"SHOW CONFIGS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/3.show-configs/#show_configs","text":"The SHOW CONFIGS statement lists the mutable configurations of the Graph Service, Meta Service, or Storage Service. For how to update the configurations through nGQL, see CONFIGS syntax (doc TODO).","title":"SHOW CONFIGS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/3.show-configs/#syntax","text":"SHOW CONFIGS [GRAPH|META|STORAGE] Option Description GRAPH Shows the configuration of the Graph Service. META Shows the configuration of the Meta Service. STORAGE Shows the configuration of the Meta Service. If no service name is set in the statement, Nebula Graph shows the mutable configurations of all services.","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/3.show-configs/#example","text":"nebula> SHOW CONFIGS GRAPH; +---------+---------------------------+-------+-----------+-------+ | module | name | type | mode | value | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"v\" | \"int\" | \"MUTABLE\" | 0 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"minloglevel\" | \"int\" | \"MUTABLE\" | 0 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"slow_op_threshhold_ms\" | \"int\" | \"MUTABLE\" | 50 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"heartbeat_interval_secs\" | \"int\" | \"MUTABLE\" | 3 | +---------+---------------------------+-------+-----------+-------+ | \"GRAPH\" | \"meta_client_retry_times\" | \"int\" | \"MUTABLE\" | 3 | +---------+---------------------------+-------+-----------+-------+ Got 6 rows (time spent 1216/1880 us) The output of SHOW CONFIGS is explained as follows: Column Description module The Nebula Graph service name. name The parameter name. type The data type of the value. mode Shows whether the parameter can be modified or not. value The value of the parameter. For more information about the Nebula Graph configurations, see: Graph configuration (doc TODO) Storage configuration (doc TODO)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/","text":"SHOW CREATE SPACE \u00b6 The SHOW CREATE SPACE statement shows the basic information of the specified graph space, such as the nGQL for creating the graph space, the partition number, the replica number. For details about the graph space information, see CREATE SPACE . Syntax \u00b6 SHOW CREATE SPACE <space_name> Example \u00b6 nebula> SHOW CREATE SPACE nba; +-------+--------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +-------+--------------------------------------------------------------------------------------------------------------------------------+ | \"nba\" | \"CREATE SPACE `nba` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32))\" | +-------+--------------------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 1747/2562 us)","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#show_create_space","text":"The SHOW CREATE SPACE statement shows the basic information of the specified graph space, such as the nGQL for creating the graph space, the partition number, the replica number. For details about the graph space information, see CREATE SPACE .","title":"SHOW CREATE SPACE"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#syntax","text":"SHOW CREATE SPACE <space_name>","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/4.show-create-space/#example","text":"nebula> SHOW CREATE SPACE nba; +-------+--------------------------------------------------------------------------------------------------------------------------------+ | Space | Create Space | +-------+--------------------------------------------------------------------------------------------------------------------------------+ | \"nba\" | \"CREATE SPACE `nba` (partition_num = 10, replica_factor = 1, charset = utf8, collate = utf8_bin, vid_type = FIXED_STRING(32))\" | +-------+--------------------------------------------------------------------------------------------------------------------------------+ Got 1 rows (time spent 1747/2562 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/","text":"SHOW CREATE TAGS/EDGES \u00b6 The SHOW CREATE TAG or SHOW CREATE EDGE statement shows the basic information of the specified tag or edge type. For details about the tag or edge type information, see CREATE TAG (doc TODO) and CREATE EDGE (doc TODO). Syntax \u00b6 SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} Example \u00b6 nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\" | +----------+-----------------------------------+","title":"SHOW CREATE TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#show_create_tagsedges","text":"The SHOW CREATE TAG or SHOW CREATE EDGE statement shows the basic information of the specified tag or edge type. For details about the tag or edge type information, see CREATE TAG (doc TODO) and CREATE EDGE (doc TODO).","title":"SHOW CREATE TAGS/EDGES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#syntax","text":"SHOW CREATE {TAG <tag_name> | EDGE <edge_name>}","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/5.show-create-tags-edges/#example","text":"nebula> SHOW CREATE TAG player; +----------+-----------------------------------+ | Tag | Create Tag | +----------+-----------------------------------+ | \"player\" | \"CREATE TAG `player` ( | | | `name` string NULL, | | | `age` int64 NULL | | | ) ttl_duration = 0, ttl_col = \"\" | +----------+-----------------------------------+","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/","text":"SHOW HOSTS \u00b6 The SHOW HOSTS statement lists storage hosts registered by the Meta Service. Syntax \u00b6 SHOW HOSTS Example \u00b6 nebula> SHOW HOSTS; +-------------+-------+----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged0\" | 44500 | \"ONLINE\" | 8 | \"docs:5, nba:3\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged1\" | 44500 | \"ONLINE\" | 9 | \"nba:4, docs:5\" | \"docs:5, nba:4\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged2\" | 44500 | \"ONLINE\" | 8 | \"nba:3, docs:5\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ Got 3 rows (time spent 866/1411 us)","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#show_hosts","text":"The SHOW HOSTS statement lists storage hosts registered by the Meta Service.","title":"SHOW HOSTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#syntax","text":"SHOW HOSTS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/6.show-hosts/#example","text":"nebula> SHOW HOSTS; +-------------+-------+----------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged0\" | 44500 | \"ONLINE\" | 8 | \"docs:5, nba:3\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged1\" | 44500 | \"ONLINE\" | 9 | \"nba:4, docs:5\" | \"docs:5, nba:4\" | +-------------+-------+----------+--------------+---------------------+------------------------+ | \"storaged2\" | 44500 | \"ONLINE\" | 8 | \"nba:3, docs:5\" | \"docs:5, nba:3\" | +-------------+-------+----------+--------------+---------------------+------------------------+ Got 3 rows (time spent 866/1411 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/","text":"SHOW INDEX STATUS \u00b6 The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes. You can find out whether a native index is successfully rebuilt or not. Syntax \u00b6 SHOW {TAG | EDGE} INDEX STATUS Example \u00b6 nebula> SHOW TAG INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"like_index_0\" | \"FINISHED\" | +----------------+--------------+ | \"like1\" | \"FINISHED\" | +----------------+--------------+ Got 2 rows (time spent 1456/2122 us) Related topics \u00b6 Job manager and the JOB statements REBUILD NATIVE INDEX","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#show_index_status","text":"The SHOW INDEX STATUS statement shows the status of jobs that rebuild native indexes. You can find out whether a native index is successfully rebuilt or not.","title":"SHOW INDEX STATUS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#syntax","text":"SHOW {TAG | EDGE} INDEX STATUS","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#example","text":"nebula> SHOW TAG INDEX STATUS; +----------------+--------------+ | Name | Index Status | +----------------+--------------+ | \"like_index_0\" | \"FINISHED\" | +----------------+--------------+ | \"like1\" | \"FINISHED\" | +----------------+--------------+ Got 2 rows (time spent 1456/2122 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/7.show-index-status/#related_topics","text":"Job manager and the JOB statements REBUILD NATIVE INDEX","title":"Related topics"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/","text":"SHOW INDEXES \u00b6 The SHOW INDEXES statement shows the names of existing native indexes. Syntax \u00b6 SHOW {TAG | EDGE} INDEXES Example \u00b6 nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"play_age_0\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"star\" | +------------------+ Got 4 rows (time spent 1450/2087 us)","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#show_indexes","text":"The SHOW INDEXES statement shows the names of existing native indexes.","title":"SHOW INDEXES"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#syntax","text":"SHOW {TAG | EDGE} INDEXES","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/8.show-indexes/#example","text":"nebula> SHOW TAG INDEXES; +------------------+ | Names | +------------------+ | \"play_age_0\" | +------------------+ | \"player_index_0\" | +------------------+ | \"player_index_1\" | +------------------+ | \"star\" | +------------------+ Got 4 rows (time spent 1450/2087 us)","title":"Example"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/","text":"SHOW PARTS \u00b6 The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space. Syntax \u00b6 SHOW PARTS [<part_id>] Examples \u00b6 Show the information of all partitions: nebula> SHOW PARTS; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 2 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 3 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 4 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 5 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 6 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 7 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 8 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 9 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 10 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 10 rows (time spent 2317/3512 us) Show the information of partition 1: nebula> SHOW PARTS 1; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 1 rows (time spent 1055/1678 us)","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#show_parts","text":"The SHOW PARTS statement shows the information of a specified partition or all partitions in a graph space.","title":"SHOW PARTS"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#syntax","text":"SHOW PARTS [<part_id>]","title":"Syntax"},{"location":"3.ngql-guide/7.general-query-statements/6.show/9.show-parts/#examples","text":"Show the information of all partitions: nebula> SHOW PARTS; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 2 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 3 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 4 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 5 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 6 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 7 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 8 | \"storaged2:44500\" | \"storaged2:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 9 | \"storaged0:44500\" | \"storaged0:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ | 10 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 10 rows (time spent 2317/3512 us) Show the information of partition 1: nebula> SHOW PARTS 1; +--------------+-------------------+-------------------+-------+ | Partition ID | Leader | Peers | Losts | +--------------+-------------------+-------------------+-------+ | 1 | \"storaged1:44500\" | \"storaged1:44500\" | \"\" | +--------------+-------------------+-------------------+-------+ Got 1 rows (time spent 1055/1678 us)","title":"Examples"},{"location":"3.ngql-guide/9.space-statements/1.create-space/","text":"CREATE SPACE \u00b6 CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, charset = <charset>, collate = <collate>, vid_type = {FIXED_STRING(<max_string_length>))] | INT64} The CREATE SPACE statement creates a new graph space with the given name. A SPACE is a region that provides physically isolated graphs in Nebula Graph. An error occurs if a graph space with the same name exists if you did not specify IF NOT EXISTS . IF NOT EXISTS \u00b6 You can use the IF NOT EXISTS keywords when creating graph spaces. These keywords automatically detects if the related graph space exists. If it does not exist, a new one is created. Otherwise, no graph space is created. NOTE : The graph space existence detection here only compares the graph space name (excluding properties). Graph space name \u00b6 The graph_space_name uniquely identifies a graph space in a Nebula Graph instance. Customized graph space options \u00b6 You can set four optional options for a new graph space: partition_num Specifies the number of partitions in each replica. The suggested number is five times the number of the hard disks in the cluster. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in a production environment and 1 in a test environment. Always set the replica to an odd number for the need of quorum-based voting. charset Short for character set. A character set is a set of symbols and encodings. The default value is utf8 . collate A set of rules for comparing characters in a character set. The default value is utf8_bin . vid_type Specifies the data type of vertex IDs (VIDs) in a graph space. The default value is FIXED_STRING(8) . This shows that the default VID data type is FIXED_STRING and the default length is 8 . In Nebula Graph 2.0.0-RC, only FIXED_STRING(N) and INT64 are supported. N represents the maximum length of the VIDs. You must set the fixed_string to a positive integer. If your VID length is greater than the maximum VID length, Nebula Graph throws an error. To set the integer VID for vertices, specify the vid_type to INT64 . If no option is given, Nebula Graph creates the graph space with the default options. Example \u00b6 nebula> CREATE SPACE my_space_1; -- create a graph space with default options nebula> CREATE SPACE my_space_2(partition_num=10); -- create a graph space with customized partition number nebula> CREATE SPACE my_space_3(replica_factor=1); -- create a graph space with customized replica factor nebula> CREATE SPACE my_space_4(vid_type = FIXED_STRING(30)); -- create a graph space with customized VID maximum length Check partition distribution \u00b6 On some large clusters, the partition distribution is possibly unbalanced because of the different startup time. You can run the command to do a check of the machine distribution. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 2 | test:1, nba:5 | nba:5, test:1 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ To balance the request loads, use the following command. nebula> BALANCE LEADER;","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#create_space","text":"CREATE SPACE [IF NOT EXISTS] <graph_space_name> [(partition_num = <partition_number>, replica_factor = <replica_number>, charset = <charset>, collate = <collate>, vid_type = {FIXED_STRING(<max_string_length>))] | INT64} The CREATE SPACE statement creates a new graph space with the given name. A SPACE is a region that provides physically isolated graphs in Nebula Graph. An error occurs if a graph space with the same name exists if you did not specify IF NOT EXISTS .","title":"CREATE SPACE"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#if_not_exists","text":"You can use the IF NOT EXISTS keywords when creating graph spaces. These keywords automatically detects if the related graph space exists. If it does not exist, a new one is created. Otherwise, no graph space is created. NOTE : The graph space existence detection here only compares the graph space name (excluding properties).","title":"IF NOT EXISTS"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#graph_space_name","text":"The graph_space_name uniquely identifies a graph space in a Nebula Graph instance.","title":"Graph space name"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#customized_graph_space_options","text":"You can set four optional options for a new graph space: partition_num Specifies the number of partitions in each replica. The suggested number is five times the number of the hard disks in the cluster. For example, if you have 3 hard disks in the cluster, we recommend that you set 15 partitions. replica_factor Specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in a production environment and 1 in a test environment. Always set the replica to an odd number for the need of quorum-based voting. charset Short for character set. A character set is a set of symbols and encodings. The default value is utf8 . collate A set of rules for comparing characters in a character set. The default value is utf8_bin . vid_type Specifies the data type of vertex IDs (VIDs) in a graph space. The default value is FIXED_STRING(8) . This shows that the default VID data type is FIXED_STRING and the default length is 8 . In Nebula Graph 2.0.0-RC, only FIXED_STRING(N) and INT64 are supported. N represents the maximum length of the VIDs. You must set the fixed_string to a positive integer. If your VID length is greater than the maximum VID length, Nebula Graph throws an error. To set the integer VID for vertices, specify the vid_type to INT64 . If no option is given, Nebula Graph creates the graph space with the default options.","title":"Customized graph space options"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#example","text":"nebula> CREATE SPACE my_space_1; -- create a graph space with default options nebula> CREATE SPACE my_space_2(partition_num=10); -- create a graph space with customized partition number nebula> CREATE SPACE my_space_3(replica_factor=1); -- create a graph space with customized replica factor nebula> CREATE SPACE my_space_4(vid_type = FIXED_STRING(30)); -- create a graph space with customized VID maximum length","title":"Example"},{"location":"3.ngql-guide/9.space-statements/1.create-space/#check_partition_distribution","text":"On some large clusters, the partition distribution is possibly unbalanced because of the different startup time. You can run the command to do a check of the machine distribution. nebula> SHOW HOSTS; +-----------+-------+--------+--------------+---------------------+------------------------+ | Host | Port | Status | Leader count | Leader distribution | Partition distribution | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged0 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged1 | 44500 | ONLINE | 2 | test:1, nba:5 | nba:5, test:1 | +-----------+-------+--------+--------------+---------------------+------------------------+ | storaged2 | 44500 | ONLINE | 1 | nba:5 | nba:5 | +-----------+-------+--------+--------------+---------------------+------------------------+ To balance the request loads, use the following command. nebula> BALANCE LEADER;","title":"Check partition distribution"},{"location":"3.ngql-guide/9.space-statements/2.use-space/","text":"USE \u00b6 USE <graph_space_name> The USE statement specifies a graph space as the current working space for subsequent queries. To manage multiple graph spaces, use the USE statement. The USE statement requires some privileges. (TODO: authentication doc) The graph space remains the same unless another USE statement is executed. nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; Different from SQL, making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. Graph spaces are FULLY ISOLATED from each other. Unlike MySQL, you can only use one graph space at a time in Nebula Graph. But in MySQL, you can select two tables from different databases in one statement.","title":"USE SPACE"},{"location":"3.ngql-guide/9.space-statements/2.use-space/#use","text":"USE <graph_space_name> The USE statement specifies a graph space as the current working space for subsequent queries. To manage multiple graph spaces, use the USE statement. The USE statement requires some privileges. (TODO: authentication doc) The graph space remains the same unless another USE statement is executed. nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; Different from SQL, making a graph space as the working graph space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. Graph spaces are FULLY ISOLATED from each other. Unlike MySQL, you can only use one graph space at a time in Nebula Graph. But in MySQL, you can select two tables from different databases in one statement.","title":"USE"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/","text":"SHOW SPACES \u00b6 SHOW SPACES The SHOW SPACES statement lists the all the graph spaces in a Nebula Graph instance. For example: nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"nba\" | +--------+ To create graph spaces, see Create Space document .","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/3.show-spaces/#show_spaces","text":"SHOW SPACES The SHOW SPACES statement lists the all the graph spaces in a Nebula Graph instance. For example: nebula> SHOW SPACES; +--------+ | Name | +--------+ | \"nba\" | +--------+ To create graph spaces, see Create Space document .","title":"SHOW SPACES"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/","text":"DESCRIBE SPACE \u00b6 DESC[RIBE] SPACE <graph_space_name> The DESCRIBE SPACE statement returns information about a graph space. The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES document . You can use DESC instead of DESCRIBE for short. Example \u00b6 Get information about a graph space. nebula> DESCRIBE SPACE nba; +----+-------+------------------+----------------+---------+------------+--------------------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | +----+-------+------------------+----------------+---------+------------+--------------------+ | 8 | \"nba\" | 15 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(30)\" | +----+-------+------------------+----------------+---------+------------+--------------------+","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#describe_space","text":"DESC[RIBE] SPACE <graph_space_name> The DESCRIBE SPACE statement returns information about a graph space. The DESCRIBE SPACE statement is different from the SHOW SPACES statement. For details about SHOW SPACES , see SHOW SPACES document . You can use DESC instead of DESCRIBE for short.","title":"DESCRIBE SPACE"},{"location":"3.ngql-guide/9.space-statements/4.describe-space/#example","text":"Get information about a graph space. nebula> DESCRIBE SPACE nba; +----+-------+------------------+----------------+---------+------------+--------------------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | +----+-------+------------------+----------------+---------+------------+--------------------+ | 8 | \"nba\" | 15 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(30)\" | +----+-------+------------------+----------------+---------+------------+--------------------+","title":"Example"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/","text":"DROP SPACE \u00b6 DROP SPACE [IF EXISTS] <graph_space_name> The DROP SPACE statement deletes everything in the related graph space. You must have the DROP privilege for the related graph space.(TODO: authentication doc) You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detects if the related graph space exists. If it exists, it is deleted. Otherwise, no graph space is deleted. Other graph spaces stay unchanged. The DROP SPACE statement does not immediately remove all the files and directories in the storage engine (and release disk space). The deletion depends on the implementation of different storage engines. NOTE: Be very careful with this statement.","title":"DROP SPACE"},{"location":"3.ngql-guide/9.space-statements/5.drop-space/#drop_space","text":"DROP SPACE [IF EXISTS] <graph_space_name> The DROP SPACE statement deletes everything in the related graph space. You must have the DROP privilege for the related graph space.(TODO: authentication doc) You can use the IF EXISTS keywords when dropping spaces. These keywords automatically detects if the related graph space exists. If it exists, it is deleted. Otherwise, no graph space is deleted. Other graph spaces stay unchanged. The DROP SPACE statement does not immediately remove all the files and directories in the storage engine (and release disk space). The deletion depends on the implementation of different storage engines. NOTE: Be very careful with this statement.","title":"DROP SPACE"},{"location":"4.deployment-and-installation/1.resource-preparations/","text":"Prepare resources for compiling, installing, and running Nebula Graph \u00b6 This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster. Reading guide \u00b6 If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to compile Nebula Graph? What do I need to run Nebula Graph in a test environment? What do I need to run Nebula Graph in a production environment? How much memory and disk space do I need to reserve for my Nebula Graph cluster? How to optimize the configuration for HDD and Gigabit Networks? Requirements for compiling the Nebula Graph source code \u00b6 Hardware requirements for compiling Nebula Graph \u00b6 Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD Supported operating systems for compiling Nebula Graph \u00b6 For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 2.6.32 or above. Software requirements for compiling Nebula Graph \u00b6 You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.12 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlid-devel Any stable version - gcc 7.1.0 or above You can run gcc -v to check the gcc version. gcc-c++ Any stable version - cmake 3.5.0 or above You can run cmake --version to check the cmake version. gettext Any stable version - curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage. Prepare software for compiling Nebula Graph \u00b6 This section guides you through the downloading and installation of software required for compiling Nebula Graph. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. ```bash $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core # For CentOS 8+, RedHat 8+, and Fedora, install libstdc++-static, libasan as well $ yum install -y libstdc++-static libasan ``` For Debian and Ubuntu users, run the following commands. ```bash $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext ``` Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and cmake are in the right version, then you are all set. If they are not, follow the sub-steps as follows. 1. Clone the nebula-common repository to your host. ```bash $ git clone https://github.com/vesoft-inc/nebula-common.git ``` The source code of Nebula Graph versions like 2.0 alpha or 2.0 beta is stored in particular branches. You can use the `--branch` or `-b` option to specify the branch to be cloned. For example, for 2.0 alpha, run the following command. ```bash $ git clone --branch v2.0.0-alpha https://github.com/vesoft-inc/nebula-common.git ``` 2. Make nebula-common the current working directory. ```bash $ cd nebula-common ``` 3. Run the following commands to install and enable CMake and GCC. ```bash # Install CMake. $ ./third-party/install-cmake.sh cmake-install CMake has been installed to prefix=cmake-install Run 'source cmake-install/bin/enable-cmake.sh' to make it ready to use. Run 'source cmake-install/bin/disable-cmake.sh' to disable it. # Enable CMake $ source cmake-install/bin/enable-cmake.sh # Install GCC. Installing GCC to /opt requires root privilege, you can change it to other locations. $ ./third-party/install-gcc.sh --prefix=/opt GCC-7.5.0 has been installed to /opt/vesoft/toolset/gcc/7.5.0 Performing usability tests Performing regular C++14 tests...OK Performing LeakSanitizer tests...OK Run 'source /opt/vesoft/toolset/gcc/7.5.0/enable' to start using. Run 'source /opt/vesoft/toolset/gcc/7.5.0/disable' to stop using. #Enable GCC. Please note that the path and specific version might be different from your environment. $ source /opt/vesoft/toolset/gcc/7.5.0/enable ``` Requirements and suggestions for installing Nebula Graph in test environments \u00b6 Hardware requirements for test environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD Supported operating systems for test environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see Optimize Linux kernel configuration [TODO]. Suggested service architecture for test environments \u00b6 Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1 Requirements and suggestions for installing Nebula Graph in production environments \u00b6 Hardware requirements for production environments \u00b6 Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD Supported operating systems for production environments \u00b6 For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see Optimize Linux kernel configuration [TODO]. Suggested service architecture for production environments \u00b6 Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a copy of the metadata. Usually, you only need 3 metad processes. The number of storaged processes does not affect the number of graph space copies. You can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: WARNING : Do not deploy a cluster across IDCs. Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1 Capacity requirements for running a Nebula Graph cluster \u00b6 You can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Disk space for a cluster Bytes * * 6 * 120% Memory for a cluster Bytes [ * 15 + * ( * ) + ] * 120% Number of partitions for a graph space - * Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. NOTE : You can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases. Question 3: What is the disk_partition_num_multiplier ? Answer: disk_partition_num_multiplier is a value between 2 to 10, the better performance of the hard disk, the larger the value. Use 2 for HDD. Optimize the configuration for HDD \u00b6 [This part might be moved to the configuration doc map later.] Nebula Graph is intended for NVMe SSD, but if you don't have a choice, optimizing the configuration as follows may better accommodate HDD. etc/nebula-storage.conf: --raft_rpc_timeout_ms = 5000 ~ 10000 --rocksdb_batch_size = 4096 ~ 16384 --heartbeat_interval_secs = 30 ~ 60 --raft_heartbeat_interval_secs = 30 ~ 60 etc/nebula-meta.conf: --heartbeat_interval_secs is the same as etc/nebula-storage.conf Spark Writer: ra te : { t imeou t : 5000 t o 10000 } go-importer: batchSize : 10 to 50 concurrency : 1 to 10 channelBufferSize : 100 to 500","title":"Resource preparations"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_resources_for_compiling_installing_and_running_nebula_graph","text":"This topic describes the requirements and suggestions for compiling and installing Nebula Graph, as well as how to estimate the resource you need to reserve for running a Nebula Graph cluster.","title":"Prepare resources for compiling, installing, and running Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#reading_guide","text":"If you are reading this topic with the questions listed below, click them to jump to their answers. What do I need to compile Nebula Graph? What do I need to run Nebula Graph in a test environment? What do I need to run Nebula Graph in a production environment? How much memory and disk space do I need to reserve for my Nebula Graph cluster? How to optimize the configuration for HDD and Gigabit Networks?","title":"Reading guide"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_for_compiling_the_nebula_graph_source_code","text":"","title":"Requirements for compiling the Nebula Graph source code"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_compiling_nebula_graph","text":"Item Requirement CPU architecture x86_64 Memory 4 GB Disk 10 GB, SSD","title":"Hardware requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_compiling_nebula_graph","text":"For now, we can only compile Nebula Graph in the Linux system. We recommend that you use any Linux system with kernel version 2.6.32 or above.","title":"Supported operating systems for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#software_requirements_for_compiling_nebula_graph","text":"You must have the correct version of the software listed below to compile Nebula Graph. If they are not as required or you are not sure, follow the steps in Prepare software for compiling Nebula Graph to get them ready. Software Version Note glibc 2.12 or above You can run ldd --version to check the glibc version. make Any stable version - m4 Any stable version - git Any stable version - wget Any stable version - unzip Any stable version - xz Any stable version - readline-devel Any stable version - ncurses-devel Any stable version - zlid-devel Any stable version - gcc 7.1.0 or above You can run gcc -v to check the gcc version. gcc-c++ Any stable version - cmake 3.5.0 or above You can run cmake --version to check the cmake version. gettext Any stable version - curl Any stable version - redhat-lsb-core Any stable version - libstdc++-static Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. libasan Any stable version Only needed in CentOS 8+, RedHat 8+, and Fedora systems. Other third-party software will be automatically downloaded and installed to the build directory at the configure (cmake) stage.","title":"Software requirements for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#prepare_software_for_compiling_nebula_graph","text":"This section guides you through the downloading and installation of software required for compiling Nebula Graph. Install dependencies. For CentOS, RedHat, and Fedora users, run the following commands. ```bash $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core # For CentOS 8+, RedHat 8+, and Fedora, install libstdc++-static, libasan as well $ yum install -y libstdc++-static libasan ``` For Debian and Ubuntu users, run the following commands. ```bash $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext ``` Check if the GCC and cmake on your host are in the right version. See Software requirements for compiling Nebula Graph for the required versions. $ g++ --version $ cmake --version If your GCC and cmake are in the right version, then you are all set. If they are not, follow the sub-steps as follows. 1. Clone the nebula-common repository to your host. ```bash $ git clone https://github.com/vesoft-inc/nebula-common.git ``` The source code of Nebula Graph versions like 2.0 alpha or 2.0 beta is stored in particular branches. You can use the `--branch` or `-b` option to specify the branch to be cloned. For example, for 2.0 alpha, run the following command. ```bash $ git clone --branch v2.0.0-alpha https://github.com/vesoft-inc/nebula-common.git ``` 2. Make nebula-common the current working directory. ```bash $ cd nebula-common ``` 3. Run the following commands to install and enable CMake and GCC. ```bash # Install CMake. $ ./third-party/install-cmake.sh cmake-install CMake has been installed to prefix=cmake-install Run 'source cmake-install/bin/enable-cmake.sh' to make it ready to use. Run 'source cmake-install/bin/disable-cmake.sh' to disable it. # Enable CMake $ source cmake-install/bin/enable-cmake.sh # Install GCC. Installing GCC to /opt requires root privilege, you can change it to other locations. $ ./third-party/install-gcc.sh --prefix=/opt GCC-7.5.0 has been installed to /opt/vesoft/toolset/gcc/7.5.0 Performing usability tests Performing regular C++14 tests...OK Performing LeakSanitizer tests...OK Run 'source /opt/vesoft/toolset/gcc/7.5.0/enable' to start using. Run 'source /opt/vesoft/toolset/gcc/7.5.0/disable' to stop using. #Enable GCC. Please note that the path and specific version might be different from your environment. $ source /opt/vesoft/toolset/gcc/7.5.0/enable ```","title":"Prepare software for compiling Nebula Graph"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_test_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_test_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 4 Memory 8 GB Disk 100 GB, SSD","title":"Hardware requirements for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_test_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a test environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see Optimize Linux kernel configuration [TODO].","title":"Supported operating systems for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_test_environments","text":"Process Suggested number metad (the metadata service process) 1 storaged (the storage service process) 1 or more graphd (the query engine service process) 1 or more For example, for a single-machine environment, you can deploy 1 metad, 1 storaged, and 1 graphd processes in the machine. For a more common environment, such as a cluster of 3 machines (named as A, B, and C), you can deploy Nebula Graph as follows: Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B None 1 1 C None 1 1","title":"Suggested service architecture for test environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#requirements_and_suggestions_for_installing_nebula_graph_in_production_environments","text":"","title":"Requirements and suggestions for installing Nebula Graph in production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#hardware_requirements_for_production_environments","text":"Item Requirement CPU architecture x86_64 Number of CPU core 48 Memory 96 GB Disk 2 * 900 GB, NVMe SSD","title":"Hardware requirements for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#supported_operating_systems_for_production_environments","text":"For now, we can only install Nebula Graph in the Linux system. To install Nebula Graph in a production environment, we recommend that you use any Linux system with kernel version 3.9 or above. You can adjust some of the kernel parameters to better accommodate the need for running Nebula Graph. For more information, see Optimize Linux kernel configuration [TODO].","title":"Supported operating systems for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#suggested_service_architecture_for_production_environments","text":"Process Suggested number metad (the metadata service process) 3 storaged (the storage service process) 3 or more graphd (the query engine service process) 3 or more Each metad process automatically creates and maintains a copy of the metadata. Usually, you only need 3 metad processes. The number of storaged processes does not affect the number of graph space copies. You can deploy multiple processes on a single machine. For example, on a cluster of 5 machines (named as A, B, C, D, and E), you can deploy Nebula Graph as follows: WARNING : Do not deploy a cluster across IDCs. Machine name Number of metad Number of storaged Number of graphd A 1 1 1 B 1 1 1 C 1 1 1 D None 1 1 E None 1 1","title":"Suggested service architecture for production environments"},{"location":"4.deployment-and-installation/1.resource-preparations/#capacity_requirements_for_running_a_nebula_graph_cluster","text":"You can estimate the memory, disk space, and partition number needed for a Nebula Graph cluster of 3 replicas as follows. Resource Unit How to estimate Disk space for a cluster Bytes * * 6 * 120% Memory for a cluster Bytes [ * 15 + * ( * ) + ] * 120% Number of partitions for a graph space - * Question 1: Why do we multiply the disk space and memory by 120%? Answer: The extra 20% is for buffer. Question 2: How to get the number of RocksDB instances? Answer: Each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance. Count the number of directories to get the RocksDB instance number. NOTE : You can decrease the memory size occupied by the bloom filter by adding --enable_partitioned_index_filter=true in etc/nebula-storaged.conf . But it may decrease the read performance in some random-seek cases. Question 3: What is the disk_partition_num_multiplier ? Answer: disk_partition_num_multiplier is a value between 2 to 10, the better performance of the hard disk, the larger the value. Use 2 for HDD.","title":"Capacity requirements for running a Nebula Graph cluster"},{"location":"4.deployment-and-installation/1.resource-preparations/#optimize_the_configuration_for_hdd","text":"[This part might be moved to the configuration doc map later.] Nebula Graph is intended for NVMe SSD, but if you don't have a choice, optimizing the configuration as follows may better accommodate HDD. etc/nebula-storage.conf: --raft_rpc_timeout_ms = 5000 ~ 10000 --rocksdb_batch_size = 4096 ~ 16384 --heartbeat_interval_secs = 30 ~ 60 --raft_heartbeat_interval_secs = 30 ~ 60 etc/nebula-meta.conf: --heartbeat_interval_secs is the same as etc/nebula-storage.conf Spark Writer: ra te : { t imeou t : 5000 t o 10000 } go-importer: batchSize : 10 to 50 concurrency : 1 to 10 channelBufferSize : 100 to 500","title":"Optimize the configuration for HDD"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/","text":"Install Nebula Graph by compiling the source code \u00b6 Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features. Prerequisites \u00b6 You have prepared the necessary resources described in Prepare resources for compiling, installing, and running Nebula Graph . You can access the Internet from the host you plan to install Nebula Graph. How to install \u00b6 Use Git to clone the source code of Nebula Graph to your host. To install the latest version of Nebula Graph 2.x, run the following command to download the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula-graph.git To install a specific version of Nebula Graph 2.x, use the --branch <branch_name> option to specify the correct branch. For example, to install 2.0.0 beta, run the following command. $ git clone --branch v2.0.0-beta https://github.com/vesoft-inc/nebula-graph.git Make the nebula-graph directory the current working directory. $ cd nebula-graph Create a build directory and make it the current working directory. $ mkdir build && cd build Generate the Makefile with CMake. NOTE : The installation path is /user/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=/your/install/path/ CMake variable in the following command. For more information about CMake variables, see CMake variables . If you are installing the latest version of Nebula Graph 2.x and has cloned the master branch in step 1, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. If you are installing a specific version of Nebula Graph 2.x and has cloned the corresponding branch in step 1, use the -DNEBULA_COMMON_REPO_TAG and -DNEBULA_STORAGE_REPO_TAG options to specify the correct branches of the nebula-common and nebula-storage repositories. For example, to install 2.0.0 beta, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release \\ -DNEBULA_COMMON_REPO_TAG = v2.0.0-beta -DNEBULA_STORAGE_REPO_TAG = v2.0.0-beta .. Compile Nebula Graph. Assuming cores is the number of CPU cores and mem_gb is the memory size (in GB), to appropriately speed up the compiling, you can use the value of the smaller one between cores and mem_gb/2 as the value of N in the following command. $ make -j { N } Install Nebula Graph. $ make install-all CMake variables \u00b6 Usage of CMake variables: $ cmake -DVariable = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings. ENABLE_BUILD_STORAGE \u00b6 Starting from the 2.0 pre-release, Nebula Graph supports separated compute and storage. The ENABLE_BUILD_STORAGE variable is set to OFF by default so that the storage service is not installed together with the graph service. If you are deploying Nebula Graph on a single host for testing, you can set ENABLE_BUILD_STORAGE to ON to download and install the storage service automatically. CMAKE_INSTALL_PREFIX \u00b6 CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula . ENABLE_WERROR \u00b6 ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed. ENABLE_TESTING \u00b6 ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF . ENABLE_ASAN \u00b6 ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers. CMAKE_BUILD_TYPE \u00b6 Nebula Graph supports the following building types: Debug , the default value of CMAKE_BUILD_TYPE , indicates building Nebula Graph with the debug info but not the optimization options. Release , indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo , indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel , indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info. CMAKE_C_COMPILER/CMAKE_CXX_COMPILER \u00b6 Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = /path/to/gcc/bin/gcc -DCMAKE_CXX_COMPILER = /path/to/gcc/bin/g++ .. $ cmake -DCMAKE_C_COMPILER = /path/to/clang/bin/clang -DCMAKE_CXX_COMPILER = /path/to/clang/bin/clang++ .. ENABLE_CCACHE \u00b6 ENABLE_CCACHE is ON by default and ccache is used to speed up the compiling of Nebula Graph. To disable ccache , set ENABLE_CCACHE to OFF . On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation . NEBULA_THIRDPARTY_ROOT \u00b6 NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party . What to do next \u00b6 Start and stop Nebula Graph [TODO] Connect to Nebula Graph Try Nebula Graph CRUD","title":"Install Nebula\u00a0Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#install_nebula_graph_by_compiling_the_source_code","text":"Installing Nebula Graph from the source code allows you to customize the compiling and installation settings and test the latest features.","title":"Install Nebula Graph by compiling the source code"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#prerequisites","text":"You have prepared the necessary resources described in Prepare resources for compiling, installing, and running Nebula Graph . You can access the Internet from the host you plan to install Nebula Graph.","title":"Prerequisites"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#how_to_install","text":"Use Git to clone the source code of Nebula Graph to your host. To install the latest version of Nebula Graph 2.x, run the following command to download the source code from the master branch. $ git clone https://github.com/vesoft-inc/nebula-graph.git To install a specific version of Nebula Graph 2.x, use the --branch <branch_name> option to specify the correct branch. For example, to install 2.0.0 beta, run the following command. $ git clone --branch v2.0.0-beta https://github.com/vesoft-inc/nebula-graph.git Make the nebula-graph directory the current working directory. $ cd nebula-graph Create a build directory and make it the current working directory. $ mkdir build && cd build Generate the Makefile with CMake. NOTE : The installation path is /user/local/nebula by default. To customize it, add the -DCMAKE_INSTALL_PREFIX=/your/install/path/ CMake variable in the following command. For more information about CMake variables, see CMake variables . If you are installing the latest version of Nebula Graph 2.x and has cloned the master branch in step 1, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release .. If you are installing a specific version of Nebula Graph 2.x and has cloned the corresponding branch in step 1, use the -DNEBULA_COMMON_REPO_TAG and -DNEBULA_STORAGE_REPO_TAG options to specify the correct branches of the nebula-common and nebula-storage repositories. For example, to install 2.0.0 beta, run the following command. $ cmake -DENABLE_BUILD_STORAGE = on -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release \\ -DNEBULA_COMMON_REPO_TAG = v2.0.0-beta -DNEBULA_STORAGE_REPO_TAG = v2.0.0-beta .. Compile Nebula Graph. Assuming cores is the number of CPU cores and mem_gb is the memory size (in GB), to appropriately speed up the compiling, you can use the value of the smaller one between cores and mem_gb/2 as the value of N in the following command. $ make -j { N } Install Nebula Graph. $ make install-all","title":"How to install"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_variables","text":"Usage of CMake variables: $ cmake -DVariable = <value> ... The following CMake variables can be used at the configure (cmake) stage to adjust the compiling settings.","title":"CMake variables"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_build_storage","text":"Starting from the 2.0 pre-release, Nebula Graph supports separated compute and storage. The ENABLE_BUILD_STORAGE variable is set to OFF by default so that the storage service is not installed together with the graph service. If you are deploying Nebula Graph on a single host for testing, you can set ENABLE_BUILD_STORAGE to ON to download and install the storage service automatically.","title":"ENABLE_BUILD_STORAGE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_install_prefix","text":"CMAKE_INSTALL_PREFIX specifies the path where the service modules, scripts, configuration files are installed. The default path is /usr/local/nebula .","title":"CMAKE_INSTALL_PREFIX"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_werror","text":"ENABLE_WERROR is ON by default and it makes all warnings into errors. You can set it to OFF if needed.","title":"ENABLE_WERROR"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_testing","text":"ENABLE_TESTING is ON by default and unit tests are built with the Nebula Graph services. If you just need the service modules, set it to OFF .","title":"ENABLE_TESTING"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_asan","text":"ENABLE_ASAN is OFF by default and the building of ASan (AddressSanitizer), a memory error detector, is disabled. To enable it, set ENABLE_ASAN to ON . This variable is intended for Nebula Graph developers.","title":"ENABLE_ASAN"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_build_type","text":"Nebula Graph supports the following building types: Debug , the default value of CMAKE_BUILD_TYPE , indicates building Nebula Graph with the debug info but not the optimization options. Release , indicates building Nebula Graph with the optimization options but not the debug info. RelWithDebInfo , indicates building Nebula Graph with the optimization options and the debug info. MinSizeRel , indicates building Nebula Graph with the optimization options for controlling the code size but not the debug info.","title":"CMAKE_BUILD_TYPE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#cmake_c_compilercmake_cxx_compiler","text":"Usually, CMake locates and uses a C/C++ compiler installed in the host automatically. But if your compiler is not installed at the standard path, or if you want to use a different one, run the command as follows to specify the installation path of the target compiler: $ cmake -DCMAKE_C_COMPILER = /path/to/gcc/bin/gcc -DCMAKE_CXX_COMPILER = /path/to/gcc/bin/g++ .. $ cmake -DCMAKE_C_COMPILER = /path/to/clang/bin/clang -DCMAKE_CXX_COMPILER = /path/to/clang/bin/clang++ ..","title":"CMAKE_C_COMPILER/CMAKE_CXX_COMPILER"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#enable_ccache","text":"ENABLE_CCACHE is ON by default and ccache is used to speed up the compiling of Nebula Graph. To disable ccache , set ENABLE_CCACHE to OFF . On some platforms, the ccache installation hooks up or precedes the compiler. In such a case, you have to set an environment variable export CCACHE_DISABLE=true or add a line disable=true in ~/.ccache/ccache.conf as well. For more information, see the ccache official documentation .","title":"ENABLE_CCACHE"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#nebula_thirdparty_root","text":"NEBULA_THIRDPARTY_ROOT specifies the path where the third party software is installed. By default it is /opt/vesoft/third-party .","title":"NEBULA_THIRDPARTY_ROOT"},{"location":"4.deployment-and-installation/2.compile-and-install-nebula-graph/1.install-nebula-graph-by-compiling-the-source-code/#what_to_do_next","text":"Start and stop Nebula Graph [TODO] Connect to Nebula Graph Try Nebula Graph CRUD","title":"What to do next"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/","text":"Full-text index restrictions \u00b6 This document holds the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: The maximum indexing string length is 256 bytes. To index data that is longer than 256 bytes, store your data in a reverse order. Full-text index can not be applied to more than one property at a time (similar to a composite index). The WHERE clause in full-text search statement LOOKUP does not support logical expressions AND and OR . Full-text index can not be applied to multiple tags search. Ideographic language Chinese does not have word delimiters. Therefore, the built-in full-text parser cannot determine where words begin and end in Chinese. Install the elasticsearch-analysis-ik to parse Chinese. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search the null properties. Rebuilding or altering Elasticsearch indexes is not supported at this time. Pipe is not supported in the LOOKUP statement, excluding the examples in our document. Full-text search only works on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, a error is caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect.","title":"Full-text restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/1.text-based-index-restrictions/#full-text_index_restrictions","text":"This document holds the restrictions for full-text indexes. Please read the restrictions very carefully before using the full-text indexes. For now, full-text search has the following limitations: The maximum indexing string length is 256 bytes. To index data that is longer than 256 bytes, store your data in a reverse order. Full-text index can not be applied to more than one property at a time (similar to a composite index). The WHERE clause in full-text search statement LOOKUP does not support logical expressions AND and OR . Full-text index can not be applied to multiple tags search. Ideographic language Chinese does not have word delimiters. Therefore, the built-in full-text parser cannot determine where words begin and end in Chinese. Install the elasticsearch-analysis-ik to parse Chinese. Sorting for the returned results of the full-text search is not supported. Data is returned in the order of data insertion. Full-text index can not search the null properties. Rebuilding or altering Elasticsearch indexes is not supported at this time. Pipe is not supported in the LOOKUP statement, excluding the examples in our document. Full-text search only works on single terms. Full-text indexes are not deleted together with the graph space. Make sure that you start the Elasticsearch cluster and Nebula Graph at the same time. If not, the data writing on the Elasticsearch cluster can be incomplete. Do not contain ' or \\ in the vertex or edge values. If not, a error is caused in the Elasticsearch cluster storage. It may take a while for Elasticsearch to create indexes. If Nebula Graph warns no index is found, wait for the index to take effect.","title":"Full-text index restrictions"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/","text":"Deploy full-text index \u00b6 Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed. Before you start \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Deploy Elasticsearch cluster \u00b6 To deploy an Elasticsearch cluster, see the Elasticsearch documentation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document . Sign in to the text search clients \u00b6 SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch. For example: nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); Elasticsearch does not have username or password by default. If you configured a username and password, you need to specify in the SIGN IN statement. Show text search clients \u00b6 SHOW TEXT SEARCH CLIENTS Use the SHOW TEXT SEARCH CLIENTS statement to list the text search clients. For example: nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ Sign out to the text search clients \u00b6 SIGN OUT TEXT SERVICE Use the SIGN OUT TEXT SERVICE to sign out all the text search clients. For example: nebula> SIGN OUT TEXT SERVICE;","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_full-text_index","text":"Nebula Graph full-text indexes are powered by Elasticsearch . This means that you can use Elasticsearch full-text query language to retrieve what you want. Full-text indexes are managed through built-in procedures. They can be created only for variable STRING and FIXED_STRING properties when the listener cluster and the Elasticsearch cluster are deployed.","title":"Deploy full-text index"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#before_you_start","text":"Before you start using the full-text index, please make sure that you know the restrictions .","title":"Before you start"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#deploy_elasticsearch_cluster","text":"To deploy an Elasticsearch cluster, see the Elasticsearch documentation . When the Elasticsearch cluster is started, add the template file for the Nebula Graph full-text index. Take the following sample template for example: { \"template\" : \"nebula*\" , \"settings\" : { \"index\" : { \"number_of_shards\" : 3 , \"number_of_replicas\" : 1 } }, \"mappings\" : { \"properties\" : { \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } } } } Make sure that you specify the following fields in strict accordance with the preceding template format: \"template\" : \"nebula*\" \"tag_id\" : { \"type\" : \"long\" }, \"column_id\" : { \"type\" : \"text\" }, \"value\" :{ \"type\" : \"keyword\" } You can configure the Elasticsearch to meet your business needs. To customize the Elasticsearch, see Elasticsearch Document .","title":"Deploy Elasticsearch cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_in_to_the_text_search_clients","text":"SIGN IN TEXT SERVICE [(<elastic_ip:port> [,<username>, <password>]), (<elastic_ip:port>), ...] When the Elasticsearch cluster is deployed, use the SIGN IN statement to sign in to the Elasticsearch clients. Multiple elastic_ip:port pairs are separated with commas. You must use the IPs and the port number in the configuration file for the Elasticsearch. For example: nebula> SIGN IN TEXT SERVICE (127.0.0.1:9200); Elasticsearch does not have username or password by default. If you configured a username and password, you need to specify in the SIGN IN statement.","title":"Sign in to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#show_text_search_clients","text":"SHOW TEXT SEARCH CLIENTS Use the SHOW TEXT SEARCH CLIENTS statement to list the text search clients. For example: nebula> SHOW TEXT SEARCH CLIENTS; +-------------+------+ | Host | Port | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+ | \"127.0.0.1\" | 9200 | +-------------+------+","title":"Show text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/2.deploy-es/#sign_out_to_the_text_search_clients","text":"SIGN OUT TEXT SERVICE Use the SIGN OUT TEXT SERVICE to sign out all the text search clients. For example: nebula> SIGN OUT TEXT SERVICE;","title":"Sign out to the text search clients"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/","text":"Deploy Raft Listener for Nebula Storage service \u00b6 Before you start \u00b6 Before you start using the full-text index, please make sure that you know the restrictions . Full-text index data is written to the Elasticsearch cluster asynchronously. To use the full-text index search, you must run one or more nebula-storaged processes as the Raft Listener (short for Listener). The Listener is a separate process and receives requests from Nebula Graph clients. It manages the traffic of these requests to the database server. NOTE: The version of the listener must be the same as or later than the version of all Nebula Graph services. Configure Listener \u00b6 The Listener writes the data into the Elasticsearch cluster. Configure the following parameters for the Listener. For example: ########## basics ########## # The file to host the process id --pid_file=pids_listener/nebula-storaged.pid # The directory to host logging files, which must already exists --log_dir=logs_listener ########## networking ########## # Meta server address --meta_server_addrs=192.168.8.5:68833 # Local ip --local_ip=192.168.8.5 # Storage daemon listening port --port=66780 # HTTP service ip --ws_ip=192.168.8.5 # HTTP service port --ws_http_port=65219 # HTTP2 service port --ws_h2_port=65211 Use the real IP in the configuration file. There is a sample Listener configuration file for your reference. Modify the preceding configurations in the Listener configuration file. Run the following command to start the Listener: ./bin/nebula-storaged --flagfile /path/to/your/listener/nebula-storaged-listener.conf Add full-text Listeners \u00b6 ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] When Listeners are started, use the ADD LISTENER ELASTICSEARCH statement to add the Listeners for a graph space. Multiple listener_ip:port pairs are separated with commas. You must use the real IPs. For example: nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; NOTE: Listeners are bound to the partitions. You must add all the Listeners in one sentence or you receive an error. Show full-text Listeners \u00b6 SHOW LISTENER Use the SHOW LISTENER statement to list the Listeners. For example: nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ Remove full-text Listeners \u00b6 REMOVE LISTENER ELASTICSEARCH Use the REMOVE LISTENER ELASTICSEARCH statement to remove all the Elasticsearch Listeners for a graph space. For example: nebula> REMOVE LISTENER ELASTICSEARCH; When you are done with the deployment of the Elasticsearch cluster and the Listener, full-text indexes are created automatically on the Elasticsearch. You can do full-text search now. For more information, see Full-text search .","title":"Deploy Raft Listener cluster"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#deploy_raft_listener_for_nebula_storage_service","text":"","title":"Deploy Raft Listener for Nebula Storage service"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#before_you_start","text":"Before you start using the full-text index, please make sure that you know the restrictions . Full-text index data is written to the Elasticsearch cluster asynchronously. To use the full-text index search, you must run one or more nebula-storaged processes as the Raft Listener (short for Listener). The Listener is a separate process and receives requests from Nebula Graph clients. It manages the traffic of these requests to the database server. NOTE: The version of the listener must be the same as or later than the version of all Nebula Graph services.","title":"Before you start"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#configure_listener","text":"The Listener writes the data into the Elasticsearch cluster. Configure the following parameters for the Listener. For example: ########## basics ########## # The file to host the process id --pid_file=pids_listener/nebula-storaged.pid # The directory to host logging files, which must already exists --log_dir=logs_listener ########## networking ########## # Meta server address --meta_server_addrs=192.168.8.5:68833 # Local ip --local_ip=192.168.8.5 # Storage daemon listening port --port=66780 # HTTP service ip --ws_ip=192.168.8.5 # HTTP service port --ws_http_port=65219 # HTTP2 service port --ws_h2_port=65211 Use the real IP in the configuration file. There is a sample Listener configuration file for your reference. Modify the preceding configurations in the Listener configuration file. Run the following command to start the Listener: ./bin/nebula-storaged --flagfile /path/to/your/listener/nebula-storaged-listener.conf","title":"Configure Listener"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#add_full-text_listeners","text":"ADD LISTENER ELASTICSEARCH <listener_ip:port> [,<listener_ip:port>, ...] When Listeners are started, use the ADD LISTENER ELASTICSEARCH statement to add the Listeners for a graph space. Multiple listener_ip:port pairs are separated with commas. You must use the real IPs. For example: nebula> ADD LISTENER ELASTICSEARCH 192.168.8.5:46780; NOTE: Listeners are bound to the partitions. You must add all the Listeners in one sentence or you receive an error.","title":"Add full-text Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#show_full-text_listeners","text":"SHOW LISTENER Use the SHOW LISTENER statement to list the Listeners. For example: nebula> SHOW LISTENER; +--------+-----------------+-----------------------+----------+ | PartId | Type | Host | Status | +--------+-----------------+-----------------------+----------+ | 1 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 2 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+ | 3 | \"ELASTICSEARCH\" | \"[192.168.8.5:46780]\" | \"ONLINE\" | +--------+-----------------+-----------------------+----------+","title":"Show full-text Listeners"},{"location":"4.deployment-and-installation/6.deploy-text-based-index/3.deploy-listener/#remove_full-text_listeners","text":"REMOVE LISTENER ELASTICSEARCH Use the REMOVE LISTENER ELASTICSEARCH statement to remove all the Elasticsearch Listeners for a graph space. For example: nebula> REMOVE LISTENER ELASTICSEARCH; When you are done with the deployment of the Elasticsearch cluster and the Listener, full-text indexes are created automatically on the Elasticsearch. You can do full-text search now. For more information, see Full-text search .","title":"Remove full-text Listeners"},{"location":"7.data-security/2.backup-restore/1.what-is-br/","text":"What is Backup & Restore \u00b6 Backup & Restore (BR in short) is a Command-Line Interface (CLI) tool for you to back up data of graph spaces of Nebula Graph and to restore data from the backup files. Features \u00b6 Supports storing backup files in local disks (SSD or HDD), Alibaba Cloud OSS, and Amazon S3. Supports backing up data of one or multiple graph spaces. Limitations \u00b6 Supports Nebula Graph v2.0.0-RC and later versions only. Supports full backup, but not incremental backup. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. SSH login is required for backup and restoration. Does not support the Nebula Graph services deployed with Docker Compose. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. The restoration process is performed OFFLINE. Implementation \u00b6 You can use the BR to do these: Backing up a cluster and storing its data in a local or cloud storage system. Restoring data to a cluster from a local or cloud storage system. This section introduces how backup and restoration are implemented in the BR. Backup \u00b6 To back up data, the BR sends a backup request to the leader metad process to trigger the backup process as follows: The SSH login from the BR machine to the meta and the storage servers is verified. Besides, if a remote storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR sends a request to create backup files. The leader metad process is locked. NOTE : From now on, you cannot run any DDL statement of nGQL until Step 9. The leader metad process blocks writing to the specified graph spaces. NOTE : From now on, you cannot run any DML statement of nGQL in the specified graph spaces until Step 7. But this process has no effect on the DQL statements in these graph spaces, and you can do whatever you want in other graph spaces. The leader metad process sends a request to the storaged processes for the snapshot file names. The leader metad process scans local RocksDB files and output SST files. The leader metad process cancels blocking writing to the specified graph spaces. NOTE : From now on, you can run DML statements in the specified graph spaces. The leader metad process sends responses to the BR with the metadata including: the thrift format, partition information of the graph spaces, and the Raft log commit ID of each partition, and the snapshot information including the catalog of the snapshots of each storaged process, their SST file names of the meta server, and the backup file names. The leader metad process is unlocked. > NOTE : From now on, you can run any DDL statement in the specified graph spaces. The account on the BR machine logs on via SSH to the meta server where the leader locates and to all the storage servers, and backs up files. If Amazon S3 or Alibaba Cloud OSS is used, the BR calls commands to upload the files to the cloud storage system. > NOTE : This step causes massive disk reads. We recommend that a 10 Gigabit Network is applied. If a networking error occurs during this step, the backup process fails and you must do the backup operation again. For now, the backup process cannot be resumed from the broken point. The BR sends a request to clean the snapshots from meta server and storage servers, and the backup process is done. This figure shows how the backup is implemented. When backup files are generated, the file names are generated automatically. A folder name is in the format of BACKUP_YY_MM_DD_HH_mm_SS , of which, BACKUP indicates the files are backup files. YY_MM_DD_HH_mm_SS indicates the timestamp when the files are generated. Restore \u00b6 CAUTION : During the restoration process, the data on the target cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. The restoration process is implemented as follows: The SSH login from the BR to the meta and the storage servers is verified. Besides, if a cloud storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR downloads the metadata (but not data) of the backup files from the remote storage system or other external storage systems. The BR verifies the topology of the clusters. The BR stops the Meta Service and the Storage Service remotely. The account on the BR machine logs on via SSH to the meta and storage servers to remove the existing data files. When data files are removed, the account on the BR machine logs on via SSH to the meta and storage servers and downloads the backup files from the cloud storage system or other external storage systems. When the backup files are downloaded, the BR starts the Meta Service. The BR calls the br restore command to change the partition information of the specified metad processes. The BR starts the Storage Service, and the restoration process is done. This figure shows how the restoration process is implemented. How to use \u00b6 To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"What is Backup & Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#what_is_backup_restore","text":"Backup & Restore (BR in short) is a Command-Line Interface (CLI) tool for you to back up data of graph spaces of Nebula Graph and to restore data from the backup files.","title":"What is Backup &amp; Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#features","text":"Supports storing backup files in local disks (SSD or HDD), Alibaba Cloud OSS, and Amazon S3. Supports backing up data of one or multiple graph spaces.","title":"Features"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#limitations","text":"Supports Nebula Graph v2.0.0-RC and later versions only. Supports full backup, but not incremental backup. Supports restoration of data on clusters of the same topologies only, which means both clusters must have exactly the same number of hosts. SSH login is required for backup and restoration. Does not support the Nebula Graph services deployed with Docker Compose. During the backup process, both DDL and DML statements in the specified graph spaces are blocked. We recommend that you do the operation within the low peak period of the business, for example, from 2:00 AM to 5:00 AM. The restoration process is performed OFFLINE.","title":"Limitations"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#implementation","text":"You can use the BR to do these: Backing up a cluster and storing its data in a local or cloud storage system. Restoring data to a cluster from a local or cloud storage system. This section introduces how backup and restoration are implemented in the BR.","title":"Implementation"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#backup","text":"To back up data, the BR sends a backup request to the leader metad process to trigger the backup process as follows: The SSH login from the BR machine to the meta and the storage servers is verified. Besides, if a remote storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR sends a request to create backup files. The leader metad process is locked. NOTE : From now on, you cannot run any DDL statement of nGQL until Step 9. The leader metad process blocks writing to the specified graph spaces. NOTE : From now on, you cannot run any DML statement of nGQL in the specified graph spaces until Step 7. But this process has no effect on the DQL statements in these graph spaces, and you can do whatever you want in other graph spaces. The leader metad process sends a request to the storaged processes for the snapshot file names. The leader metad process scans local RocksDB files and output SST files. The leader metad process cancels blocking writing to the specified graph spaces. NOTE : From now on, you can run DML statements in the specified graph spaces. The leader metad process sends responses to the BR with the metadata including: the thrift format, partition information of the graph spaces, and the Raft log commit ID of each partition, and the snapshot information including the catalog of the snapshots of each storaged process, their SST file names of the meta server, and the backup file names. The leader metad process is unlocked. > NOTE : From now on, you can run any DDL statement in the specified graph spaces. The account on the BR machine logs on via SSH to the meta server where the leader locates and to all the storage servers, and backs up files. If Amazon S3 or Alibaba Cloud OSS is used, the BR calls commands to upload the files to the cloud storage system. > NOTE : This step causes massive disk reads. We recommend that a 10 Gigabit Network is applied. If a networking error occurs during this step, the backup process fails and you must do the backup operation again. For now, the backup process cannot be resumed from the broken point. The BR sends a request to clean the snapshots from meta server and storage servers, and the backup process is done. This figure shows how the backup is implemented. When backup files are generated, the file names are generated automatically. A folder name is in the format of BACKUP_YY_MM_DD_HH_mm_SS , of which, BACKUP indicates the files are backup files. YY_MM_DD_HH_mm_SS indicates the timestamp when the files are generated.","title":"Backup"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#restore","text":"CAUTION : During the restoration process, the data on the target cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. The restoration process is implemented as follows: The SSH login from the BR to the meta and the storage servers is verified. Besides, if a cloud storage system such as Amazon S3 or Alibaba Cloud OSS is necessary, their client installation and configuration are verified. The BR downloads the metadata (but not data) of the backup files from the remote storage system or other external storage systems. The BR verifies the topology of the clusters. The BR stops the Meta Service and the Storage Service remotely. The account on the BR machine logs on via SSH to the meta and storage servers to remove the existing data files. When data files are removed, the account on the BR machine logs on via SSH to the meta and storage servers and downloads the backup files from the cloud storage system or other external storage systems. When the backup files are downloaded, the BR starts the Meta Service. The BR calls the br restore command to change the partition information of the specified metad processes. The BR starts the Storage Service, and the restoration process is done. This figure shows how the restoration process is implemented.","title":"Restore"},{"location":"7.data-security/2.backup-restore/1.what-is-br/#how_to_use","text":"To use the BR, follow these steps: Compile BR . Use BR to back up data . Use BR to restore data from backup files .","title":"How to use"},{"location":"7.data-security/2.backup-restore/2.compile-br/","text":"Compile BR \u00b6 For now, the BR is not provided as a package. You can compile the BR. NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . Prerequisites \u00b6 To compile the BR, do a check of these: Go 1.14 or a later version is installed. make is installed. Procedure \u00b6 To compile the BR, follow these steps: Clone the nebula-storage repository to your machine. git clone https://github.com/vesoft-inc/nebula-storage.git Change to the br diretory. cd nebula-storage/util/br Compile the BR. make build && make test When the BR is compiled successfully, you can find the br binary file under the nebula-storage/util/br/bin/ directory.","title":"Compile BR"},{"location":"7.data-security/2.backup-restore/2.compile-br/#compile_br","text":"For now, the BR is not provided as a package. You can compile the BR. NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files .","title":"Compile BR"},{"location":"7.data-security/2.backup-restore/2.compile-br/#prerequisites","text":"To compile the BR, do a check of these: Go 1.14 or a later version is installed. make is installed.","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/2.compile-br/#procedure","text":"To compile the BR, follow these steps: Clone the nebula-storage repository to your machine. git clone https://github.com/vesoft-inc/nebula-storage.git Change to the br diretory. cd nebula-storage/util/br Compile the BR. make build && make test When the BR is compiled successfully, you can find the br binary file under the nebula-storage/util/br/bin/ directory.","title":"Procedure"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/","text":"Use BR to back up data \u00b6 After the BR is compiled, you can back up data of specified graph spaces. This article introduces how to use the BR to back up data. Prerequisites \u00b6 To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . The Nebula Graph services are running and we recommend that the backup is performed when the application request traffic is very low. Get the names of the specified graph spaces. In this example, nba is used. Get the Nebula Graph installation directory. In this example, /usr/local/nebula/ is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration files. Your account on the BR machine can log on via SSH to the meta and the storage servers without a password. Here is a configuration reference . In this example, such an account named nebula on the BR machine is used. If you use Amazon S3 or Alibaba Cloud OSS to store the backup files, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If you store the backup files locally, create a directory with the same absolute path on the meta and the storage servers and the BR machine for the backup files and get the absolute path. In this example, /home/user/backup/ is used. NOTE : In the production environment, we recommend that you mount Network File System (NFS) storage to the meta and the storage servers and the BR machine for local backup, or use S3 or OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files . Procedure \u00b6 To back up data of the specified graph spaces: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of this metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH without a password user : \"nebula\" #- # If more than one metad process runs, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account on the BR machine that is authorized to log on to the storage server via SSH without a password user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the store directory for the backup files. # If the backup files are stored locally, set backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used, set # backend: \"oss://nebulabackup\" # If Amazon S3 is used, set # backend: \"s3://nebulabackup\" # Set the graph spaces to be backed up. # If more than one graph spaces are necessary, set # space_names: [\"space_name1\", \"space_name2\", ..., \"space_name\"] space_name : [ \"nba\" ] Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to back up data. ./br backup full --config \"/absolute/path/to/the/backup/configuration/file.yaml\" In this command: backup full : Backs up data. --config \"/absolute/path/to/the/backup/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the backup process, if the leader changes, an error occurs. You can clean the temporary files with the br cleanup command as in Step 4, and then run the br backup command again. When the backup is successful, you can find a backup folder with a name in the BACKUP_YY_MM_DD_HH_mm_SS format in the backup store directory on the BR machine and all the servers of the cluster. In this example, in the /home/user/backup/ directory, you can find a folder named BACKUP_2020_11_30_20_47_44 . All these backup files on all the machines are required for data restoration. (Optional) By default, all the snapshots will be deleted when the backup is done. If errors occur during the deletion of these files, run this command to delete them. ./br cleanup --backup_name [ backup file name ] --meta 192 .168.8.161:9559 In this command: - cleanup : Deletes all the temporary files on the meta and the storage servers. - --backup_name BACKUP_YY_MM_DD_HH_mm_SS : Sets a backup folder name, indicating the command is run to delete the temporary files that were generated when this backup folder was generated. - --meta <IP address:port> : Sets the IP address and the port of a meta server. Next to do \u00b6 After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Use BR to back up data"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#use_br_to_back_up_data","text":"After the BR is compiled, you can back up data of specified graph spaces. This article introduces how to use the BR to back up data.","title":"Use BR to back up data"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#prerequisites","text":"To back up data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . NOTE : If you want to store the backup files locally, we recommend that you compile the BR on one meta server of the target Nebula Graph cluster where you will perform data restoration. For more information, see Restore data from backup files . The Nebula Graph services are running and we recommend that the backup is performed when the application request traffic is very low. Get the names of the specified graph spaces. In this example, nba is used. Get the Nebula Graph installation directory. In this example, /usr/local/nebula/ is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration files. Your account on the BR machine can log on via SSH to the meta and the storage servers without a password. Here is a configuration reference . In this example, such an account named nebula on the BR machine is used. If you use Amazon S3 or Alibaba Cloud OSS to store the backup files, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If you store the backup files locally, create a directory with the same absolute path on the meta and the storage servers and the BR machine for the backup files and get the absolute path. In this example, /home/user/backup/ is used. NOTE : In the production environment, we recommend that you mount Network File System (NFS) storage to the meta and the storage servers and the BR machine for local backup, or use S3 or OSS for remote backup. When you restore the data from local files, you must manually move these backup files to a specified directory, which causes redundant data and troubles. For more information, see Restore data from backup files .","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#procedure","text":"To back up data of the specified graph spaces: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of this metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH without a password user : \"nebula\" #- # If more than one metad process runs, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account on the BR machine that is authorized to log on to the storage server via SSH without a password user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more # addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the store directory for the backup files. # If the backup files are stored locally, set backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used, set # backend: \"oss://nebulabackup\" # If Amazon S3 is used, set # backend: \"s3://nebulabackup\" # Set the graph spaces to be backed up. # If more than one graph spaces are necessary, set # space_names: [\"space_name1\", \"space_name2\", ..., \"space_name\"] space_name : [ \"nba\" ] Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to back up data. ./br backup full --config \"/absolute/path/to/the/backup/configuration/file.yaml\" In this command: backup full : Backs up data. --config \"/absolute/path/to/the/backup/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the backup process, if the leader changes, an error occurs. You can clean the temporary files with the br cleanup command as in Step 4, and then run the br backup command again. When the backup is successful, you can find a backup folder with a name in the BACKUP_YY_MM_DD_HH_mm_SS format in the backup store directory on the BR machine and all the servers of the cluster. In this example, in the /home/user/backup/ directory, you can find a folder named BACKUP_2020_11_30_20_47_44 . All these backup files on all the machines are required for data restoration. (Optional) By default, all the snapshots will be deleted when the backup is done. If errors occur during the deletion of these files, run this command to delete them. ./br cleanup --backup_name [ backup file name ] --meta 192 .168.8.161:9559 In this command: - cleanup : Deletes all the temporary files on the meta and the storage servers. - --backup_name BACKUP_YY_MM_DD_HH_mm_SS : Sets a backup folder name, indicating the command is run to delete the temporary files that were generated when this backup folder was generated. - --meta <IP address:port> : Sets the IP address and the port of a meta server.","title":"Procedure"},{"location":"7.data-security/2.backup-restore/3.br-backup-data/#next_to_do","text":"After the backup files are generated, you can use the BR to restore them for Nebula Graph. For more information, see Use BR to restore data .","title":"Next to do"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/","text":"Use BR to restore data \u00b6 If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This article introduces how to use the BR to restore data from backup files. NOTE : The restoration process is performed OFFLINE. CAUTION : During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster. Prerequisites \u00b6 To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. Get the backup folder names to do the restoration. In this example, BACKUP_2020_12_21_01_17_53 is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration file. Your account on the BR machine can log on to the meta and the storage servers via SSH without a password. Here is a configuration reference . This account must have the write permission to the installation directory of Nebula Graph. In this example, such an account named nebula on the BR machine is used. If the backup files are stored on Alibaba Cloud OSS or Amazon S3, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If the backup files are stored locally on the servers, create a directory with the same absolute path on the BR machine and all the servers of the target Nebula Graph cluster, and then manually move these backup files to this directory. Such file movement causes redundant data and troubles. Procedure \u00b6 To restore data from some backup files: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH user : \"nebula\" #- # If more than one metad processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account of the BR machine that is authorized to log on to the storage server via SSH user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the directory where the backup files are located. # If the backup files are stored locally backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used # backend: \"oss://nebulabackup\" # If Amazon S3 is used # backend: \"s3://nebulabackup\" # Set the backup files to be restored backup_name : \"BACKUP_2020_12_21_01_17_53\" Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to restore data. ./br restore full --config \"/absolute/path/to/the/restore/configuration/file.yaml\" In this command: - restore full : Restores data. - --config \"/absolute/path/to/the/restore/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the restoration process, if the leader changes, an error occurs. To prevent data corruption, when an error occurs, you must run the br restore command to perform the restoration again. When the restoration is successful, you can find the data in the <nebula_installation_path>/data/storage directory under the Nebula Graph installation directory. Wait about several seconds until the metadata and the schema are synchronized, and then verify the data. For example, on the nebula-console, run SHOW STATS to verify the number of vertices and edges in the restored graph space. NOTE : After restoration, if no records are returned for the USE <space_name> statement, we recommend that you restart the Graph Service. if the Storage Error: part: 2, error code: -3. error occurs when you query the restored data, do a check of the status of the Storage Service. If necessary, restart the Storage Service.","title":"Use BR to restore data"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#use_br_to_restore_data","text":"If you use the BR to back up data, you can use it to restore the data to Nebula Graph. This article introduces how to use the BR to restore data from backup files. NOTE : The restoration process is performed OFFLINE. CAUTION : During the restoration process, the data on the target Nebula Graph cluster is removed and then is replaced with the data from the backup files. If necessary, back up the data on the target cluster.","title":"Use BR to restore data"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#prerequisites","text":"To restore data with the BR, do a check of these: The BR is compiled. For more information, see Compile BR . No application is connected to the target Nebula Graph cluster. Make sure that the target and the source Nebula Graph clusters have the same topology, which means that they have exactly the same number of hosts. Get the backup folder names to do the restoration. In this example, BACKUP_2020_12_21_01_17_53 is used. From the nebula-metad.conf and nebula-storaged.conf files, get the IP addresses and ports of the meta and the storage servers. Both files are in the <nebula_installation_path>/nebula/etc directory. In this example, For the meta server: 192.168.8.161:9559 For the storage server: 192.168.8.161:9779 NOTE : Make sure that the actual IP addresses instead of 127.0.0.1 are used in the configuration file. Your account on the BR machine can log on to the meta and the storage servers via SSH without a password. Here is a configuration reference . This account must have the write permission to the installation directory of Nebula Graph. In this example, such an account named nebula on the BR machine is used. If the backup files are stored on Alibaba Cloud OSS or Amazon S3, make sure that the S3 CLI client or ossutil is installed and configured on the meta servers, the storage servers, and the BR machine. For more information, see Amazon S3 CLI Documentation and Alibaba Cloud ossutil Documentation . NOTE : Run ln -s /<ossutil_tool_installation_path>/<ossutil64 or ossutil> /usr/local/bin/ossutil to make the ossutil command effective. If the backup files are stored locally on the servers, create a directory with the same absolute path on the BR machine and all the servers of the target Nebula Graph cluster, and then manually move these backup files to this directory. Such file movement causes redundant data and troubles.","title":"Prerequisites"},{"location":"7.data-security/2.backup-restore/4.br-restore-data/#procedure","text":"To restore data from some backup files: Edit the configuration file as follows. You can find an example configuration in the nebula-storage/util/br/ directory. meta_nodes : - # Set the IP address and the port of one meta server addrs : \"192.168.8.161:9559\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the metad process data : \"/usr/local/nebula/data/meta\" # Set the account of the BR machine that is authorized to log on to the meta server via SSH user : \"nebula\" #- # If more than one metad processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" #- addrs: \"192.168.8.161:9559\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/meta\" # user: \"nebula\" storage_nodes : - # Set the IP address and the port of one storage server addrs : \"192.168.8.161:9779\" # Set the absolute path of the Nebula Graph installation directory root : \"/usr/local/nebula/\" # Set the absolute path of the data directory of the storaged process data : \"/usr/local/nebula/data/storage\" # Set the account of the BR machine that is authorized to log on to the storage server via SSH user : \"nebula\" #- If more than one storaged processes run, refer to the preceding configuration to add more #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" #- addrs: \"192.168.8.161:9779\" # root: \"/usr/local/nebula/\" # data: \"/usr/local/nebula/data/storage\" # user: \"nebula\" # Set the directory where the backup files are located. # If the backup files are stored locally backend : \"local:///absolute/path/to/the/store/directory\" # If Alibaba Cloud OSS is used # backend: \"oss://nebulabackup\" # If Amazon S3 is used # backend: \"s3://nebulabackup\" # Set the backup files to be restored backup_name : \"BACKUP_2020_12_21_01_17_53\" Run the command to change to the nebula-storage/util/br/bin/ directory. cd nebula-storage/util/util/br/bin/ Run the command to restore data. ./br restore full --config \"/absolute/path/to/the/restore/configuration/file.yaml\" In this command: - restore full : Restores data. - --config \"/absolute/path/to/the/restore/configuration/file.yaml\" : Sets the absolute path of the configuration file. NOTE : During the restoration process, if the leader changes, an error occurs. To prevent data corruption, when an error occurs, you must run the br restore command to perform the restoration again. When the restoration is successful, you can find the data in the <nebula_installation_path>/data/storage directory under the Nebula Graph installation directory. Wait about several seconds until the metadata and the schema are synchronized, and then verify the data. For example, on the nebula-console, run SHOW STATS to verify the number of vertices and edges in the restored graph space. NOTE : After restoration, if no records are returned for the USE <space_name> statement, we recommend that you restart the Graph Service. if the Storage Error: part: 2, error code: -3. error occurs when you query the restored data, do a check of the status of the Storage Service. If necessary, restart the Storage Service.","title":"Procedure"},{"location":"nebula-exchange/ex-ug-compile/","text":"Compile Exchange \u00b6 To compile Exchange, follow these steps: Run these commands to install Nebula Java Client v2.x. $ git clone https://github.com/vesoft-inc/nebula-java.git $ cd nebula-java $ mvn clean install -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true NOTE : After the installation, you can see the /com/vesoft/client/2.0.0-beta/client-2.0.0-beta.jar in your local Maven repository. Run these commands to package Nebula Exchange v2.x. $ git clone https://github.com/vesoft-inc/nebula-spark-utils.git $ cd nebula-spark-utils/nebula-exchange $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After the compiling, you can see the structure of the Exchange directory as follows. . \u251c\u2500\u2500 README-CN.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pom.xml \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 main \u2502 \u2514\u2500\u2500 test \u2514\u2500\u2500 target \u251c\u2500\u2500 classes \u251c\u2500\u2500 classes.timestamp \u251c\u2500\u2500 maven-archiver \u251c\u2500\u2500 nebula-exchange-2.x.y-javadoc.jar \u251c\u2500\u2500 nebula-exchange-2.x.y-sources.jar \u251c\u2500\u2500 nebula-exchange-2.x.y.jar \u251c\u2500\u2500 original-nebula-exchange-2.x.y.jar \u2514\u2500\u2500 site In the target directory, you can see the exchange-2.x.y.jar file. NOTE : The version of the JAR file depends on the releases of Nebula Java Client. You can find the latest versions on the Releases page of the nebula-spark-utils repository . To import data, you can refer to the example configuration in the target/classes/application.conf , target/classes/server_application.conf , and target/classes/stream_application.conf files.","title":"Compile Exchange"},{"location":"nebula-exchange/ex-ug-compile/#compile_exchange","text":"To compile Exchange, follow these steps: Run these commands to install Nebula Java Client v2.x. $ git clone https://github.com/vesoft-inc/nebula-java.git $ cd nebula-java $ mvn clean install -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true NOTE : After the installation, you can see the /com/vesoft/client/2.0.0-beta/client-2.0.0-beta.jar in your local Maven repository. Run these commands to package Nebula Exchange v2.x. $ git clone https://github.com/vesoft-inc/nebula-spark-utils.git $ cd nebula-spark-utils/nebula-exchange $ mvn clean package -Dmaven.test.skip = true -Dgpg.skip -Dmaven.javadoc.skip = true After the compiling, you can see the structure of the Exchange directory as follows. . \u251c\u2500\u2500 README-CN.md \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pom.xml \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 main \u2502 \u2514\u2500\u2500 test \u2514\u2500\u2500 target \u251c\u2500\u2500 classes \u251c\u2500\u2500 classes.timestamp \u251c\u2500\u2500 maven-archiver \u251c\u2500\u2500 nebula-exchange-2.x.y-javadoc.jar \u251c\u2500\u2500 nebula-exchange-2.x.y-sources.jar \u251c\u2500\u2500 nebula-exchange-2.x.y.jar \u251c\u2500\u2500 original-nebula-exchange-2.x.y.jar \u2514\u2500\u2500 site In the target directory, you can see the exchange-2.x.y.jar file. NOTE : The version of the JAR file depends on the releases of Nebula Java Client. You can find the latest versions on the Releases page of the nebula-spark-utils repository . To import data, you can refer to the example configuration in the target/classes/application.conf , target/classes/server_application.conf , and target/classes/stream_application.conf files.","title":"Compile Exchange"},{"location":"nebula-exchange/ex-ug-toc/","text":"Nebula Exchange v2.x User Guide \u00b6 About Nebula Exchange What is Nebula Exchange Limitations Glossary FAQ Compile Exchange Use Exchange Import data from CSV files Import data from JSON files Import data from HIVE [Import data from Apache Parquet][DOC_TO_DO] [Import data from Apache ORC][DOC_TO_DO] [Import data from Neo4j][DOC_TO_DO] [Import data from HBase][DOC_TO_DO] [Import data from MySQL][DOC_TO_DO] [Import data from Kafka][DOC_TO_DO] [Import data from Pulsar][DOC_TO_DO] [Import SST files][DOC_TO_DO] Parameter reference Spark related parameters Nebula Graph related parameters Import command parameters","title":"Nebula Exchange v2.x User Guide"},{"location":"nebula-exchange/ex-ug-toc/#nebula_exchange_v2x_user_guide","text":"About Nebula Exchange What is Nebula Exchange Limitations Glossary FAQ Compile Exchange Use Exchange Import data from CSV files Import data from JSON files Import data from HIVE [Import data from Apache Parquet][DOC_TO_DO] [Import data from Apache ORC][DOC_TO_DO] [Import data from Neo4j][DOC_TO_DO] [Import data from HBase][DOC_TO_DO] [Import data from MySQL][DOC_TO_DO] [Import data from Kafka][DOC_TO_DO] [Import data from Pulsar][DOC_TO_DO] [Import SST files][DOC_TO_DO] Parameter reference Spark related parameters Nebula Graph related parameters Import command parameters","title":"Nebula Exchange v2.x User Guide"},{"location":"nebula-exchange/about-exchange/ex-ug-faq/","text":"FAQ \u00b6 What version of Nebula Graph does Exchange v2.x support? Read Limitations to get the latest information about supported Nebula Graph versions. What are the differences between Exchange v1.x and Exchange v2.x? Compared with Exchange v1.x, Exchange v2.x has these new features: Importing vertex data with String type IDs. Importing data of the Null, Date, DateTime, and Time types. Importing data from other Hive sources besides Hive on Spark. Recording and retrying the INSERT statement after failures during data import. For more information, see Exchange README . What is the difference between Exchange and Spark Writer? Both are Spark applications, and Exchange is based on Spark Writer. Both of them are designed for the migration of data into a Nebula Graph cluster in a distributed environment, but the later maintenance work will focus on Exchange. Compared with Spark Writer, Exchange has the following improvements: Supporting more data sources, such as MySQL, Neo4j, HIVE, HBase, Kafka, and Pulsar. Some problems with Spark Writer were fixed. For example, by default Spark reads source data from HDFS as strings, which is probably different from your graph schema defined in Nebula Graph. Exchange supports automatically matching and converting data types. With it, when a non-string data type is defined in Nebula Graph, Exchange converts the strings into data of the required data type.","title":"FAQ"},{"location":"nebula-exchange/about-exchange/ex-ug-faq/#faq","text":"What version of Nebula Graph does Exchange v2.x support? Read Limitations to get the latest information about supported Nebula Graph versions. What are the differences between Exchange v1.x and Exchange v2.x? Compared with Exchange v1.x, Exchange v2.x has these new features: Importing vertex data with String type IDs. Importing data of the Null, Date, DateTime, and Time types. Importing data from other Hive sources besides Hive on Spark. Recording and retrying the INSERT statement after failures during data import. For more information, see Exchange README . What is the difference between Exchange and Spark Writer? Both are Spark applications, and Exchange is based on Spark Writer. Both of them are designed for the migration of data into a Nebula Graph cluster in a distributed environment, but the later maintenance work will focus on Exchange. Compared with Spark Writer, Exchange has the following improvements: Supporting more data sources, such as MySQL, Neo4j, HIVE, HBase, Kafka, and Pulsar. Some problems with Spark Writer were fixed. For example, by default Spark reads source data from HDFS as strings, which is probably different from your graph schema defined in Nebula Graph. Exchange supports automatically matching and converting data types. With it, when a non-string data type is defined in Nebula Graph, Exchange converts the strings into data of the required data type.","title":"FAQ"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/","text":"Limitations \u00b6 This article introduces the limitations of Exchange v2.x. Supported Nebula Graph versions \u00b6 Exchange v2.x supports Nebula Graph v2.x only. If you are using Nebula Graph v1.x, please use Nebula Exchange v1.x . Supported operation systems \u00b6 You can use Exchange v2.x in these operation systems: CentOS 7 macOS NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Software dependencies \u00b6 To make sure that Exchange v2.x works properly, make sure that these software applications are installed in your machine: Apache Spark: 2.3.0 or later versions Java: 1.8 Scala: 2.10.7, 2.11.12, or 2.12.10 In these scenarios, Hadoop Distributed File System (HDFS) must be deployed: Importing data from HDFS to Nebula Graph Importing SST files into Nebula Graph","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#limitations","text":"This article introduces the limitations of Exchange v2.x.","title":"Limitations"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#supported_nebula_graph_versions","text":"Exchange v2.x supports Nebula Graph v2.x only. If you are using Nebula Graph v1.x, please use Nebula Exchange v1.x .","title":"Supported Nebula Graph versions"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#supported_operation_systems","text":"You can use Exchange v2.x in these operation systems: CentOS 7 macOS NOTE : Importing SST files with Exchange v2.x is supported in Linux only.","title":"Supported operation systems"},{"location":"nebula-exchange/about-exchange/ex-ug-limitations/#software_dependencies","text":"To make sure that Exchange v2.x works properly, make sure that these software applications are installed in your machine: Apache Spark: 2.3.0 or later versions Java: 1.8 Scala: 2.10.7, 2.11.12, or 2.12.10 In these scenarios, Hadoop Distributed File System (HDFS) must be deployed: Importing data from HDFS to Nebula Graph Importing SST files into Nebula Graph","title":"Software dependencies"},{"location":"nebula-exchange/about-exchange/ex-ug-terms/","text":"Glossary \u00b6 This article gives explanations of some necessary terminologies in this user guide. Nebula Exchange: Referred to as Exchange v2.x or Exchange in this user guide. It is a Spark application based on Apache Spark\u2122 for batch or stream processing data migration. It supports converting data from different sources into vertex and edge data that can be recognized by Nebula Graph, and then concurrently importing data into Nebula Graph. Apache Spark\u2122: A fast and general computing engine designed for large-scale data processing. It is an open-source project of Apache Software Foundation. Driver Program: Referred to as driver in this user guide. It is a program that runs the main function of an application and creates a new SparkContext instance.","title":"Glossary"},{"location":"nebula-exchange/about-exchange/ex-ug-terms/#glossary","text":"This article gives explanations of some necessary terminologies in this user guide. Nebula Exchange: Referred to as Exchange v2.x or Exchange in this user guide. It is a Spark application based on Apache Spark\u2122 for batch or stream processing data migration. It supports converting data from different sources into vertex and edge data that can be recognized by Nebula Graph, and then concurrently importing data into Nebula Graph. Apache Spark\u2122: A fast and general computing engine designed for large-scale data processing. It is an open-source project of Apache Software Foundation. Driver Program: Referred to as driver in this user guide. It is a program that runs the main function of an application and creates a new SparkContext instance.","title":"Glossary"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/","text":"What is Nebula Exchange \u00b6 Nebula Exchange v2.x (Exchange v2.x or Exchange in short) is an Apache Spark\u2122 application. It can be used to migrate data from a cluster in a distributed environment to a Nebula Graph v2.x cluster. It supports processing different formats of batch data and streaming data. Exchange is composed of Reader, Processor, and Writer. Reader reads data of different sources and creates DataFrame. Processor traverses every row of the DataFrame and obtains the values for each column according to the mapping of the fields in the configuration file. And then after the specified rows of data to be batch processed are traversed, Writer writes the obtained data into Nebula Graph concurrently. This figure shows how the data is transformed and transferred in Exchange. Scenarios \u00b6 You can use Exchange in these scenarios: Converting streaming data from Kafka or Pulsar platforms to vertex or edge data of property graphs and importing them into Nebula Graph. For example, log files, online shopping data, in-game player activities, social networking information, financial trading services, geospatial services, or telemetry data from connected devices or instruments in the data center. Converting batch data (such as data in a certain period of time) from a relational database (such as MySQL) or a distributed file system (such as HDFS) into vertex or edge data of property graphs, and importing them into Nebula Graph. Converting a large amount of data into SST files and then importing them into Nebula Graph. Features \u00b6 Exchange has these features: Adaptable: Exchange supports importing data from different sources into Nebula Graph, which is convenient for you to migrate data. SST files supported: Exchange supports converting data from different sources into SST files for data import. NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Resuming broken transfer: Exchange supports resuming an interrupted transfer from a broken point during the data import process, which saves your time and improves efficiency. NOTE : Exchange v2.x supports resuming broken transfer for Neo4j only. Asynchronous: Exchange enables you to set an insertion statement for the source and sends it to the Graph Service of Nebula Graph for data insertion. Flexible: Exchange supports importing multiple types of vertices and edges of different sources or formats simultaneously. Statistics: Exchange uses the accumulator in Apache Spark\u2122 to count the successes and failures during the insertion process. Easy to use and user-friendly: Exchange supports HOCON (Human-Optimized Config Object Notation) configuration file format, which is object-oriented, and easy to understand and operate. Supported data sources \u00b6 You can use Exchange v2.x to convert data of these sources into vertex and edge data and then import them to Nebula Graph v2.x: Data of different formats stored on HDFS, including: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data warehouses: HIVE Graph databases: Neo4j 2.4.5-M1. Resuming transfer from a broken point is supported for Neo4j data only. Relational databases: MySQL Stream processing platforms: Apache Kafka\u00ae Messaging and streaming platforms: Apache Pulsar 2.4.5","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#what_is_nebula_exchange","text":"Nebula Exchange v2.x (Exchange v2.x or Exchange in short) is an Apache Spark\u2122 application. It can be used to migrate data from a cluster in a distributed environment to a Nebula Graph v2.x cluster. It supports processing different formats of batch data and streaming data. Exchange is composed of Reader, Processor, and Writer. Reader reads data of different sources and creates DataFrame. Processor traverses every row of the DataFrame and obtains the values for each column according to the mapping of the fields in the configuration file. And then after the specified rows of data to be batch processed are traversed, Writer writes the obtained data into Nebula Graph concurrently. This figure shows how the data is transformed and transferred in Exchange.","title":"What is Nebula Exchange"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#scenarios","text":"You can use Exchange in these scenarios: Converting streaming data from Kafka or Pulsar platforms to vertex or edge data of property graphs and importing them into Nebula Graph. For example, log files, online shopping data, in-game player activities, social networking information, financial trading services, geospatial services, or telemetry data from connected devices or instruments in the data center. Converting batch data (such as data in a certain period of time) from a relational database (such as MySQL) or a distributed file system (such as HDFS) into vertex or edge data of property graphs, and importing them into Nebula Graph. Converting a large amount of data into SST files and then importing them into Nebula Graph.","title":"Scenarios"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#features","text":"Exchange has these features: Adaptable: Exchange supports importing data from different sources into Nebula Graph, which is convenient for you to migrate data. SST files supported: Exchange supports converting data from different sources into SST files for data import. NOTE : Importing SST files with Exchange v2.x is supported in Linux only. Resuming broken transfer: Exchange supports resuming an interrupted transfer from a broken point during the data import process, which saves your time and improves efficiency. NOTE : Exchange v2.x supports resuming broken transfer for Neo4j only. Asynchronous: Exchange enables you to set an insertion statement for the source and sends it to the Graph Service of Nebula Graph for data insertion. Flexible: Exchange supports importing multiple types of vertices and edges of different sources or formats simultaneously. Statistics: Exchange uses the accumulator in Apache Spark\u2122 to count the successes and failures during the insertion process. Easy to use and user-friendly: Exchange supports HOCON (Human-Optimized Config Object Notation) configuration file format, which is object-oriented, and easy to understand and operate.","title":"Features"},{"location":"nebula-exchange/about-exchange/ex-ug-what-is-exchange/#supported_data_sources","text":"You can use Exchange v2.x to convert data of these sources into vertex and edge data and then import them to Nebula Graph v2.x: Data of different formats stored on HDFS, including: Apache Parquet Apache ORC JSON CSV Apache HBase\u2122 Data warehouses: HIVE Graph databases: Neo4j 2.4.5-M1. Resuming transfer from a broken point is supported for Neo4j data only. Relational databases: MySQL Stream processing platforms: Apache Kafka\u00ae Messaging and streaming platforms: Apache Pulsar 2.4.5","title":"Supported data sources"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/","text":"Import command parameters \u00b6 When the configuration file is ready, replace master-node-url and exchange-2.x.y.jar in this command and run it to import the data from the specified source into Nebula Graph. $SPARK_HOME /bin/spark-submit --master \"master-node-url\" --class com.vesoft.nebula.exchange.Exchange target/exchange-2.x.y.jar -c /path/to/conf/application.conf This table lists all the parameters in the preceding command. Parameters Required? Default Description --master Yes None Specifies the URL of the Master node of the specified Spark cluster. For more information, see master-urls in Spark Documentation \u3002 --class Yes None Specifies the entry point of Exchange. -c / --config Yes None Specifies the path of the Exchange configuration file. -h / --hive No false If you want to import data from HIVE, add this parameter. -D / --dry No false Before data import, add this parameter to do a check of the format of the configuration file, but not the configuration of tags and edges . Do not use this parameter when you import data.","title":"Import command parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-para-import-command/#import_command_parameters","text":"When the configuration file is ready, replace master-node-url and exchange-2.x.y.jar in this command and run it to import the data from the specified source into Nebula Graph. $SPARK_HOME /bin/spark-submit --master \"master-node-url\" --class com.vesoft.nebula.exchange.Exchange target/exchange-2.x.y.jar -c /path/to/conf/application.conf This table lists all the parameters in the preceding command. Parameters Required? Default Description --master Yes None Specifies the URL of the Master node of the specified Spark cluster. For more information, see master-urls in Spark Documentation \u3002 --class Yes None Specifies the entry point of Exchange. -c / --config Yes None Specifies the path of the Exchange configuration file. -h / --hive No false If you want to import data from HIVE, add this parameter. -D / --dry No false Before data import, add this parameter to do a check of the format of the configuration file, but not the configuration of tags and edges . Do not use this parameter when you import data.","title":"Import command parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-nebulagraph/","text":"Nebula Graph related parameters \u00b6 To import data, you must set parameters for Nebula Graph. This table lists all the Nebula Graph related parameters. For more information, see the examples . Parameters Default Data Type Required? Description nebula.address.graph None list[string] Yes Specifies the addresses and ports used by the Graph Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.address.meta None list[string] Yes Specifies the addresses and ports used by the Meta Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.user user string Yes Specifies an account of Nebula Graph. The default value is user . If authentication is enabled in Nebula Graph: - If no account is created, use root . - If a specified account is created and given the write permission to a graph space, you can use this account. nebula.pswd password string Yes Specifies the password of the specified account. The default password for the user account is password . If authentication is enabled in Nebula Graph: - For the root account, use nebula . - For another account, use the specified password. nebula.space None string Yes Specifies the name of the graph space to import data. nebula.connection.timeout 3000 int No Specifies the period of timeout for Thrift connection. Unit: ms. nebula.connection.retry 3 int No Specifies the number of retries for Thrift connection. nebula.execution.retry 3 int No Specifies the number of execution retries of an nGQL statements nebula.error.max 32 int No Specifies the maximum number of failures during the import process. When the specified number of failures occur, the submitted Spark job stops automatically. nebula.error.output None string Yes Specifies a logging directory on the Nebula Graph cluster for the error message.","title":"Nebula Graph related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-nebulagraph/#nebula_graph_related_parameters","text":"To import data, you must set parameters for Nebula Graph. This table lists all the Nebula Graph related parameters. For more information, see the examples . Parameters Default Data Type Required? Description nebula.address.graph None list[string] Yes Specifies the addresses and ports used by the Graph Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.address.meta None list[string] Yes Specifies the addresses and ports used by the Meta Service of Nebula Graph. Multiple addresses must be separated with commas. In the format of \"ip1:port\",\"ip2:port\",\"ip3:port\" . nebula.user user string Yes Specifies an account of Nebula Graph. The default value is user . If authentication is enabled in Nebula Graph: - If no account is created, use root . - If a specified account is created and given the write permission to a graph space, you can use this account. nebula.pswd password string Yes Specifies the password of the specified account. The default password for the user account is password . If authentication is enabled in Nebula Graph: - For the root account, use nebula . - For another account, use the specified password. nebula.space None string Yes Specifies the name of the graph space to import data. nebula.connection.timeout 3000 int No Specifies the period of timeout for Thrift connection. Unit: ms. nebula.connection.retry 3 int No Specifies the number of retries for Thrift connection. nebula.execution.retry 3 int No Specifies the number of execution retries of an nGQL statements nebula.error.max 32 int No Specifies the maximum number of failures during the import process. When the specified number of failures occur, the submitted Spark job stops automatically. nebula.error.output None string Yes Specifies a logging directory on the Nebula Graph cluster for the error message.","title":"Nebula Graph related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-spark/","text":"Spark related parameters \u00b6 To import data, you must set parameters for Spark. This table lists some generally-used parameters. For more Spark-related parameters, see Apache Spark documentation . For more information, see the examples . Parameters Default Data type Required? Description spark.app.name Nebula Exchange 2.0 string No Specifies the name of the Spark Driver Program. spark.driver.cores 1 int No Specifies the number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G string No Specifies the limit of the total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. Should be at least 1M, or 0 for unlimited. spark.cores.max None int No When the driver program runs on a standalone deployed cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on the standalone cluster manager of Spark, or infinite (all available cores) on Mesos.","title":"Spark related parameters"},{"location":"nebula-exchange/parameter-reference/ex-ug-paras-spark/#spark_related_parameters","text":"To import data, you must set parameters for Spark. This table lists some generally-used parameters. For more Spark-related parameters, see Apache Spark documentation . For more information, see the examples . Parameters Default Data type Required? Description spark.app.name Nebula Exchange 2.0 string No Specifies the name of the Spark Driver Program. spark.driver.cores 1 int No Specifies the number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G string No Specifies the limit of the total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. Should be at least 1M, or 0 for unlimited. spark.cores.max None int No When the driver program runs on a standalone deployed cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on the standalone cluster manager of Spark, or infinite (all available cores) on Mesos.","title":"Spark related parameters"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/","text":"Import data from CSV files \u00b6 This article uses an example to show how to use Exchange to import data from CSV files stored on HDFS into Nebula Graph. If you want to import data from local CSV files into Nebula Graph v2.x, see Nebula Importer . Dataset \u00b6 In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from CSV files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Analyze the data in the CSV files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named csv and create a schema. -- Create a graph space named csv CREATE SPACE csv(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the csv graph space USE csv; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph Database . Step 2. Prepare CSV files \u00b6 Do a check of these: The CSV files are processed to meet the requirements of the schema. For more information, see Quick Start of Nebula Graph Studio . > NOTE : Exchange supports importing CSV files with or without headers. The CSV files must be stored in HDFS and get the file storage path. Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for CSV files. In this example, a new configuration file is named csv_ application.conf . In this file, the vertex and edge related configuration is introduced in the comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\". graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: csv connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the course tag { # Specifies a tag name defined in Nebula Graph. name: course type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/course.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c1] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex must be one column of the CSV file. vertex: _c1 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: _c1, # policy: \"hash\" } # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the user tag { name: user type: { source: csv sink: client } path: \"hdfs://namenode_ip:port/path/to/user.csv\" # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [userId] # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [userId] # The value of vertex.field must be one column of the CSV file. vertex: userId separator: \",\" header: true batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/actions.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c3, _c4, _c5, _c6, _c7, _c8] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # Specifies the columns as the source of the IDs of the source and target vertices. source: _c1 target: _c2 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use source.policy or target.policy for mapping. #target: { # field: _c2 # policy: \"hash\" #} # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] # If more edge types are necessary, refer to the preceding configuration to add more. } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from CSV files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#import_data_from_csv_files","text":"This article uses an example to show how to use Exchange to import data from CSV files stored on HDFS into Nebula Graph. If you want to import data from local CSV files into Nebula Graph v2.x, see Nebula Importer .","title":"Import data from CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#dataset","text":"In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository.","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#prerequisites","text":"To import data from CSV files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_1_create_a_schema_in_nebula_graph","text":"Analyze the data in the CSV files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named csv and create a schema. -- Create a graph space named csv CREATE SPACE csv(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the csv graph space USE csv; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph Database .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_2_prepare_csv_files","text":"Do a check of these: The CSV files are processed to meet the requirements of the schema. For more information, see Quick Start of Nebula Graph Studio . > NOTE : Exchange supports importing CSV files with or without headers. The CSV files must be stored in HDFS and get the file storage path.","title":"Step 2. Prepare CSV files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for CSV files. In this example, a new configuration file is named csv_ application.conf . In this file, the vertex and edge related configuration is introduced in the comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\". graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: csv connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the course tag { # Specifies a tag name defined in Nebula Graph. name: course type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/course.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c1] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex must be one column of the CSV file. vertex: _c1 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: _c1, # policy: \"hash\" } # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the user tag { name: user type: { source: csv sink: client } path: \"hdfs://namenode_ip:port/path/to/user.csv\" # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [userId] # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [userId] # The value of vertex.field must be one column of the CSV file. vertex: userId separator: \",\" header: true batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. csv is used. source: csv # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc_to_do). sink: client } # Specifies the HDFS path of the CSV file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/actions.csv\" # If the CSV file has no header, use [_c0, _c1, _c2, ..., _cn] to # represent its header and to indicate columns as the source of the property values. fields: [_c0, _c3, _c4, _c5, _c6, _c7, _c8] # If the CSV file has a header, use the actual column names. # Specifies property names defined in Nebula Graph. # fields for the CSV file and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # Specifies the columns as the source of the IDs of the source and target vertices. source: _c1 target: _c2 # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use source.policy or target.policy for mapping. #target: { # field: _c2 # policy: \"hash\" #} # Specifies the separator. The default value is commas. separator: \",\" # If the CSV file has a header, set header to true. # If the CSV file has no header, set header to false (default value). header: false # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] # If more edge types are necessary, refer to the preceding configuration to add more. }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from CSV files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/csv_application.conf","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-from-csv/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/","text":"Import data from HIVE \u00b6 This article uses an example to show how to use Exchange to import data from HIVE into Nebula Graph. Dataset \u00b6 In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. In this example, the dataset is stored in a database named mooc in HIVE, and the information of all vertices and edges is stored in the users , courses , and actions tables. Here are the structures of all the tables. scala > sql ( \"describe mooc.users\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | userid | string | null | + --------+---------+-------+ scala > sql ( \"describe mooc.courses\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | courseid | bigint | null | | coursename | string | null | + ----------+---------+-------+ scala > sql ( \"describe mooc.actions\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | actionid | bigint | null | | srcid | string | null | | dstid | string | null | | duration | double | null | | feature0 | double | null | | feature1 | double | null | | feature2 | double | null | | feature3 | double | null | | label | boolean | null | + --------+---------+-------+ NOTE : bigint in HIVE equals to int in Nebula Graph. Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode HIVE 2.3.7, with MySQL 8.0.22 Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from HIVE with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started and the hive metastore database (MySQL is used in this example) is started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named hive and create a schema. -- Create a graph space named hive CREATE SPACE hive(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the hive graph space USE hive; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph . Step 2. Verify the HIVE SQL statements \u00b6 When spark-shell starts, run these statements one by one to make sure that Spark can read data from HIVE. scala > sql ( \"select userid from mooc.users\" ). show scala > sql ( \"select courseid, coursename from mooc.courses\" ). show scala > sql ( \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" ). show Here is an example of data read from the mooc.actions table. + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | actionid | srcid | dstid | duration | feature0 | feature1 | feature2 | feature3 | label | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | 0 | 0 | Environmental Dis ... | 0 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 1 | 0 | History of Ecology | 6 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 2 | 0 | Women in Islam | 41 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 3 | 0 | History of Ecology | 49 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 4 | 0 | Women in Islam | 51 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 5 | 0 | Legacies of the A ... | 55 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 6 | 0 | ITP Core 2 | 59 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 7 | 0 | The Research Pape ... | 62 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 8 | 0 | Neurobiology | 65 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 9 | 0 | Wikipedia | 113 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 12 . 77723482 | false | | 10 | 0 | Media History and ... | 226 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 149 . 4512115 | false | | 11 | 0 | WIKISOO | 974 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 3 . 344522776 | false | | 12 | 0 | Environmental Dis ... | 1000 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 13 | 0 | WIKISOO | 1172 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 1 . 136866766 | false | | 14 | 0 | Women in Islam | 1182 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 15 | 0 | History of Ecology | 1185 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 16 | 0 | Human Development ... | 1687 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 17 | 1 | Human Development ... | 7262 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 18 | 1 | History of Ecology | 7266 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 19 | 1 | Women in Islam | 7273 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 0 . 936170765 | false | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ only showing top 20 rows Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for HIVE. In this example, a new configuration file is named hive_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # If Spark and HIVE are deployed in the different clusters, # configure these parameters for HIVE. Otherwise, ignore them. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password user: user pswd: password # Specifies a graph space name space: hive connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the user tag { # Specifies a tag name defined in Nebula Graph name: user type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the users table in the mooc database exec: \"select userid from mooc.users\" # Specifies the column names from the users table to fields. # Their values are used as the source of the userId (nebula.fields) property defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must have the one-to-one correspondence relationship. fields: [userid] nebula.fields: [userId] # Specifies a column as the source of VIDs. # The value of vertex must be one column name in the exec sentence. # If the values are not of the int type, use vertex.policy to # set the mapping policy. \"hash\" is preferred. # Refer to the configuration of the course tag. vertex: userid # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the course tag { name: course type: { source: hive sink: client } exec: \"select courseid, coursename from mooc.courses\" fields: [courseid, coursename] nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex.field must be one column name in the exec sentence. vertex: coursename # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #vertex: { # field: coursename # policy: \"hash\" #} batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst # For more information about importing sst files, # see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the actions table in # the mooc database. exec: \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" # Specifies the column names from the actions table to fields. # Their values are used as the source of the properties of # the action edge type defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [actionid, duration, feature0, feature1, feature2, feature3, label] nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # source specifies a column as the source of the IDs of # the source vertex of an edge. # target specifies a column as the source of the IDs of # the target vertex of an edge. # The value of source.field and target.field must be # column names set in the exec sentence. source: srcid target: dstid # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #target: { # field: dstid # policy: \"hash\" #} # Specifies the maximum number of vertex data to be # written into Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from HIVE into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from HIVE"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#import_data_from_hive","text":"This article uses an example to show how to use Exchange to import data from HIVE into Nebula Graph.","title":"Import data from HIVE"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#dataset","text":"In this article, the Social Network: MOOC User Action Dataset provided by Stanford Network Analysis Platform (SNAP) and 97 unique course names obtained from the public network are used as the sample dataset. The dataset contains: Two vertex types ( user and course ), 7,144 vertices in total. One edge type ( action ), 411,749 edges in total. You can download the example dataset from the nebula-web-docker repository. In this example, the dataset is stored in a database named mooc in HIVE, and the information of all vertices and edges is stored in the users , courses , and actions tables. Here are the structures of all the tables. scala > sql ( \"describe mooc.users\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | userid | string | null | + --------+---------+-------+ scala > sql ( \"describe mooc.courses\" ). show + ----------+---------+-------+ | col_name | data_type | comment | + ----------+---------+-------+ | courseid | bigint | null | | coursename | string | null | + ----------+---------+-------+ scala > sql ( \"describe mooc.actions\" ). show + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | actionid | bigint | null | | srcid | string | null | | dstid | string | null | | duration | double | null | | feature0 | double | null | | feature1 | double | null | | feature2 | double | null | | feature3 | double | null | | label | boolean | null | + --------+---------+-------+ NOTE : bigint in HIVE equals to int in Nebula Graph.","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode HIVE 2.3.7, with MySQL 8.0.22 Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#prerequisites","text":"To import data from HIVE with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started and the hive metastore database (MySQL is used in this example) is started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_1_create_a_schema_in_nebula_graph","text":"Follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag user userId string Tag course courseId int, courseName string Edge Type action actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double In Nebula Graph, create a graph space named hive and create a schema. -- Create a graph space named hive CREATE SPACE hive(partition_num=10, replica_factor=1, vid_type=fixed_string(100)); -- Choose the hive graph space USE hive; -- Create the user tag CREATE TAG user(userId string); -- Create the course tag CREATE TAG course(courseId int, courseName string); -- Create the action edge type CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); For more information, see Quick Start of Nebula Graph .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_2_verify_the_hive_sql_statements","text":"When spark-shell starts, run these statements one by one to make sure that Spark can read data from HIVE. scala > sql ( \"select userid from mooc.users\" ). show scala > sql ( \"select courseid, coursename from mooc.courses\" ). show scala > sql ( \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" ). show Here is an example of data read from the mooc.actions table. + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | actionid | srcid | dstid | duration | feature0 | feature1 | feature2 | feature3 | label | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ | 0 | 0 | Environmental Dis ... | 0 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 1 | 0 | History of Ecology | 6 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 2 | 0 | Women in Islam | 41 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 3 | 0 | History of Ecology | 49 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 4 | 0 | Women in Islam | 51 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 5 | 0 | Legacies of the A ... | 55 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 6 | 0 | ITP Core 2 | 59 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 7 | 0 | The Research Pape ... | 62 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 8 | 0 | Neurobiology | 65 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 9 | 0 | Wikipedia | 113 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 12 . 77723482 | false | | 10 | 0 | Media History and ... | 226 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 149 . 4512115 | false | | 11 | 0 | WIKISOO | 974 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 3 . 344522776 | false | | 12 | 0 | Environmental Dis ... | 1000 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 13 | 0 | WIKISOO | 1172 . 0 |- 0 . 319991479 |- 0 . 435701433 | 1 . 108826104 | 1 . 136866766 | false | | 14 | 0 | Women in Islam | 1182 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 15 | 0 | History of Ecology | 1185 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 16 | 0 | Human Development ... | 1687 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 17 | 1 | Human Development ... | 7262 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 18 | 1 | History of Ecology | 7266 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 106783779 |- 0 . 06730924 | false | | 19 | 1 | Women in Islam | 7273 . 0 |- 0 . 319991479 |- 0 . 435701433 | 0 . 607804941 | 0 . 936170765 | false | + --------+-----+--------------------+--------+------------+------------+-----------+-----------+-----+ only showing top 20 rows","title":"Step 2. Verify the HIVE SQL statements"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for HIVE. In this example, a new configuration file is named hive_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Nebula Exchange 2.0 } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # If Spark and HIVE are deployed in the different clusters, # configure these parameters for HIVE. Otherwise, ignore them. #hive: { # waredir: \"hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/\" # connectionURL: \"jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8\" # connectionDriverName: \"com.mysql.jdbc.Driver\" # connectionUserName: \"user\" # connectionPassword: \"password\" #} # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password user: user pswd: password # Specifies a graph space name space: hive connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the user tag { # Specifies a tag name defined in Nebula Graph name: user type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the users table in the mooc database exec: \"select userid from mooc.users\" # Specifies the column names from the users table to fields. # Their values are used as the source of the userId (nebula.fields) property defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must have the one-to-one correspondence relationship. fields: [userid] nebula.fields: [userId] # Specifies a column as the source of VIDs. # The value of vertex must be one column name in the exec sentence. # If the values are not of the int type, use vertex.policy to # set the mapping policy. \"hash\" is preferred. # Refer to the configuration of the course tag. vertex: userid # Specifies the maximum number of vertex data to be written into # Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } # Sets for the course tag { name: course type: { source: hive sink: client } exec: \"select courseid, coursename from mooc.courses\" fields: [courseid, coursename] nebula.fields: [courseId, courseName] # Specifies a column as the source of VIDs. # The value of vertex.field must be one column name in the exec sentence. vertex: coursename # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #vertex: { # field: coursename # policy: \"hash\" #} batch: 256 partition: 32 } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the action edge type { # Specifies an edge type name defined in Nebula Graph name: action type: { # Specifies the data source. hive is used. source: hive # Specifies how to import vertex data into Nebula Graph: client or sst # For more information about importing sst files, # see Import SST files (doc to do). sink: client } # Specifies the SQL statement to read data from the actions table in # the mooc database. exec: \"select actionid, srcid, dstid, duration, feature0, feature1, feature2, feature3, label from mooc.actions\" # Specifies the column names from the actions table to fields. # Their values are used as the source of the properties of # the action edge type defined in Nebula Graph. # If more than one column name is specified, separate them with commas. # fields for the HIVE and nebula.fields for Nebula Graph must # have the one-to-one correspondence relationship. fields: [actionid, duration, feature0, feature1, feature2, feature3, label] nebula.fields: [actionId, duration, feature0, feature1, feature2, feature3, label] # source specifies a column as the source of the IDs of # the source vertex of an edge. # target specifies a column as the source of the IDs of # the target vertex of an edge. # The value of source.field and target.field must be # column names set in the exec sentence. source: srcid target: dstid # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. #target: { # field: dstid # policy: \"hash\" #} # Specifies the maximum number of vertex data to be # written into Nebula Graph in a single batch. batch: 256 # Specifies the partition number of Spark. partition: 32 } ] }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from HIVE into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/hive_application.conf -h","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"1\" OVER action; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-hive/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/","text":"Import data from JSON files \u00b6 This article uses an example to show how to use Exchange to import data from JSON files stored on HDFS into Nebula Graph. Dataset \u00b6 The JSON file (test.json) used in this example is like {\"source\":string, \"target\":string, \"likeness\":double} , representing a like relationship between source and target . 21,645 records in total. Here are some sample data: { \"source\" : 53802643 , \"target\" : 87847387 , \"likeness\" : 0.34 } { \"source\" : 29509860 , \"target\" : 57501950 , \"likeness\" : 0.40 } { \"source\" : 97319348 , \"target\" : 50240344 , \"likeness\" : 0.77 } { \"source\" : 94295709 , \"target\" : 8189720 , \"likeness\" : 0.82 } { \"source\" : 78707720 , \"target\" : 53874070 , \"likeness\" : 0.98 } { \"source\" : 23399562 , \"target\" : 20136097 , \"likeness\" : 0.47 } Environment \u00b6 The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose . Prerequisites \u00b6 To import data from JSON files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 Step 1. Create a schema in Nebula Graph \u00b6 Analyze the data in the JSON files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag source srcId string Tag target dstId string Edge Type like likeness double In Nebula Graph, create a graph space named json and create a schema. -- Create a graph space named json CREATE SPACE json(partition_num=10, replica_factor=1, vid_type=fixed_string(30)); -- Choose the json graph space USE json; -- Create the source tag CREATE TAG source (srcId string); -- Create the target tag CREATE TAG target (dstId string); -- Create the like edge type CREATE EDGE like (likeness double); For more information, see Quick Start of Nebula Graph . Step 2. Prepare JSON files \u00b6 Create separate JSON files for vertex and edge data. Store the JSON files in HDFS and get the HDFS path of the files. NOTE : In this example, only one JSON file is used to import vertex and edge data at the same time. Some vertex data representing source and target are duplicate. Therefore, during the import process, these vertices are written repeatedly. In Nebula Graph, data is overwritten when repeated insertion occurs, and the last write is read out. In practice, to increase the write speed, creating separate files for vertex and edge data is recommended. Step 3. Edit configuration file \u00b6 After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for JSON files. In this example, a new configuration file is named json_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: json connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the source tag { # Specifies a tag name defined in Nebula Graph name: source type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the srcId property # defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"source\"] nebula.fields: [\"srcId\"] # Specifies the values of a key in the JSON file as # the source of the VID in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: key_name_in_json # policy: \"hash\" # } vertex: source batch: 256 partition: 32 } # Sets for the target tag { name: target type: { source: json sink: client } path: \"hdfs://namenode_ip:port/path/to/test.json\" fields: [\"target\"] nebula.fields: [\"dstId\"] vertex: \"target\" batch: 256 partition: 32 isImplicit: true } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the like edge type { # Specifies an edge type name defined in Nebula Graph name: like type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the likeness property defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"likeness\"] nebula.fields: [\"likeness\"] # Specifies the values of two keys in the JSON file as the source # of the IDs of source and destination vertices of the like edges in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # source: { # field: key_name_in_json # policy: \"hash\" # } # target: { # field: key_name_in_json # policy: \"hash\" # } source: \"source\" target: \"target\" batch: 256 partition: 32 } # If more edge types are necessary, refer to the preceding configuration to add more. ] } Step 4. (Optional) Verify the configuration \u00b6 After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf -D Step 5. Import data into Nebula Graph \u00b6 When the configuration is ready, run this command to import data from JSON files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf Step 6. (Optional) Verify data in Nebula Graph \u00b6 You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"53802643\" OVER like; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data. Step 7. (Optional) Create and rebuild indexes in Nebula Graph \u00b6 After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#import_data_from_json_files","text":"This article uses an example to show how to use Exchange to import data from JSON files stored on HDFS into Nebula Graph.","title":"Import data from JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#dataset","text":"The JSON file (test.json) used in this example is like {\"source\":string, \"target\":string, \"likeness\":double} , representing a like relationship between source and target . 21,645 records in total. Here are some sample data: { \"source\" : 53802643 , \"target\" : 87847387 , \"likeness\" : 0.34 } { \"source\" : 29509860 , \"target\" : 57501950 , \"likeness\" : 0.40 } { \"source\" : 97319348 , \"target\" : 50240344 , \"likeness\" : 0.77 } { \"source\" : 94295709 , \"target\" : 8189720 , \"likeness\" : 0.82 } { \"source\" : 78707720 , \"target\" : 53874070 , \"likeness\" : 0.98 } { \"source\" : 23399562 , \"target\" : 20136097 , \"likeness\" : 0.47 }","title":"Dataset"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#environment","text":"The practice is done in macOS. Here is the environment information: Hardware specifications: CPU: 1.7 GHz Quad-Core Intel Core i7 Memory: 16 GB Spark 2.4.7, deployed in the Standalone mode Hadoop 2.9.2, deployed in the Pseudo-Distributed mode Nebula Graph v2-nightly, deployed with Docker Compose. For more information, see Deploy Nebula Graph with Docker Compose .","title":"Environment"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#prerequisites","text":"To import data from JSON files on HDFS with Exchange v2.x, do a check of these: Exchange v2.x is compiled. For more information, see Compile Exchange v2.x . Exchange 2.0.0 is used in this example. Spark is installed. Hadoop is installed and started. Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#procedure","text":"","title":"Procedure"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_1_create_a_schema_in_nebula_graph","text":"Analyze the data in the JSON files and follow these steps to create a schema in Nebula Graph: Confirm the essential elements of the schema. Elements Names Properties Tag source srcId string Tag target dstId string Edge Type like likeness double In Nebula Graph, create a graph space named json and create a schema. -- Create a graph space named json CREATE SPACE json(partition_num=10, replica_factor=1, vid_type=fixed_string(30)); -- Choose the json graph space USE json; -- Create the source tag CREATE TAG source (srcId string); -- Create the target tag CREATE TAG target (dstId string); -- Create the like edge type CREATE EDGE like (likeness double); For more information, see Quick Start of Nebula Graph .","title":"Step 1. Create a schema in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_2_prepare_json_files","text":"Create separate JSON files for vertex and edge data. Store the JSON files in HDFS and get the HDFS path of the files. NOTE : In this example, only one JSON file is used to import vertex and edge data at the same time. Some vertex data representing source and target are duplicate. Therefore, during the import process, these vertices are written repeatedly. In Nebula Graph, data is overwritten when repeated insertion occurs, and the last write is read out. In practice, to increase the write speed, creating separate files for vertex and edge data is recommended.","title":"Step 2. Prepare JSON files"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_3_edit_configuration_file","text":"After compiling of Exchange, copy the target/classes/application.conf file and edit the configuration for JSON files. In this example, a new configuration file is named json_ application.conf . In this file, the vertex and edge related configuration is introduced as comments and all the items that are not used in this example are commented out. For more information about the Spark and Nebula related parameters, see Spark related parameters and Nebula Graph related parameters . { # Spark related configuration spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores { max: 16 } } # Nebula Graph related configuration nebula: { address:{ # Specifies the IP addresses and ports of the Graph Service and the Meta Service of Nebula Graph. # If multiple servers are used, separate the addresses with commas. # Format: \"ip1:port\",\"ip2:port\",\"ip3:port\" graph:[\"127.0.0.1:9669\"] meta:[\"127.0.0.1:9559\"] } # Specifies an account that has the WriteData privilege in Nebula Graph and its password. user: user pswd: password # Specifies a graph space name space: json connection { timeout: 3000 retry: 3 } execution { retry: 3 } error: { max: 32 output: /tmp/errors } rate: { limit: 1024 timeout: 1000 } } # Process vertices tags: [ # Sets for the source tag { # Specifies a tag name defined in Nebula Graph name: source type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the srcId property # defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"source\"] nebula.fields: [\"srcId\"] # Specifies the values of a key in the JSON file as # the source of the VID in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # vertex: { # field: key_name_in_json # policy: \"hash\" # } vertex: source batch: 256 partition: 32 } # Sets for the target tag { name: target type: { source: json sink: client } path: \"hdfs://namenode_ip:port/path/to/test.json\" fields: [\"target\"] nebula.fields: [\"dstId\"] vertex: \"target\" batch: 256 partition: 32 isImplicit: true } # If more tags are necessary, refer to the preceding configuration to add more. ] # Process edges edges: [ # Sets for the like edge type { # Specifies an edge type name defined in Nebula Graph name: like type: { # Specifies the data source. json is used. source: json # Specifies how to import vertex data into Nebula Graph: client or sst. # For more information about importing sst files, see Import SST files (doc to do). sink: client } # Specifies the HDFS path of the JSON file. # Enclose the path with double quotes and start the path with hdfs://. path: \"hdfs://namenode_ip:port/path/to/test.json\" # Specifies the keys in the JSON file. # Their values are used as the source of the likeness property defined in Nebula Graph. # If more than one key is specified, separate them with commas. fields: [\"likeness\"] nebula.fields: [\"likeness\"] # Specifies the values of two keys in the JSON file as the source # of the IDs of source and destination vertices of the like edges in Nebula Graph. # For now, only string type VIDs are supported in Nebula Graph v2.x. # Do not use vertex.policy for mapping. # source: { # field: key_name_in_json # policy: \"hash\" # } # target: { # field: key_name_in_json # policy: \"hash\" # } source: \"source\" target: \"target\" batch: 256 partition: 32 } # If more edge types are necessary, refer to the preceding configuration to add more. ] }","title":"Step 3. Edit configuration file"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_4_optional_verify_the_configuration","text":"After the configuration, run the import command with the -D parameter to verify the configuration file. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf -D","title":"Step 4. (Optional) Verify the configuration"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_5_import_data_into_nebula_graph","text":"When the configuration is ready, run this command to import data from JSON files into Nebula Graph. For more information about the parameters, see Import command parameters . $SPARK_HOME /bin/spark-submit --master \"local\" --class com.vesoft.nebula.exchange.Exchange /path/to/nebula-exchange-2.0.0.jar -c /path/to/conf/json_application.conf","title":"Step 5. Import data into Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_6_optional_verify_data_in_nebula_graph","text":"You can use a Nebula Graph client, such as Nebula Graph Studio, to verify the imported data. For example, in Nebula Graph Studio, run this statement. GO FROM \"53802643\" OVER like; If the queried destination vertices return, the data are imported into Nebula Graph. You can run the SHOW STATS statement to count the data.","title":"Step 6. (Optional) Verify data in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-json/#step_7_optional_create_and_rebuild_indexes_in_nebula_graph","text":"After the data is imported, you can create and rebuild indexes in Nebula Graph. For more information, see nGQL User Guide .","title":"Step 7. (Optional) Create and rebuild indexes in Nebula Graph"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/","text":"Use Exchange \u00b6 This article introduces the generally-used procedure on how to use Exchange to import data from a specified source to Nebula Graph. Prerequisites \u00b6 To import data with Exchange, do a check of these: Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Exchange is compiled. For more information, see Compile Exchange . Spark is installed. Get the necessary information for schema creation in Nebula Graph, including tags and edge types. Procedure \u00b6 To import data from a source to Nebula Graph, follow these steps: Create a graph space and a schema in Nebula Graph. (Optional) Process the source data. For example, to import data from a Neo4j database, create indexes for the specified tags in Neo4j to export the data from Neo4j more quickly. Edit the configuration file for Spark, Nebula Graph, vertices, and edges. NOTE : After compiling of Exchange, refer to the example configuration files in the nebula-exchange/target/classes directory for the configuration for different sources. (Optional) Run the import command with the -D parameter to verify the configuration. For more information, see Import command parameters . Run the import command to import data into Nebula Graph. Verify the imported data in Nebula Graph. (Optional) Create and rebuild indexes in Nebula Graph. For more information, see the examples: Import data from HIVE Import data from CSV files Import data from JSON files","title":"Use Exchange"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#use_exchange","text":"This article introduces the generally-used procedure on how to use Exchange to import data from a specified source to Nebula Graph.","title":"Use Exchange"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#prerequisites","text":"To import data with Exchange, do a check of these: Nebula Graph is deployed and started. Get the information: IP addresses and ports of the Graph Service and the Meta Service. A Nebula Graph account with the privilege of writing data and its password. Exchange is compiled. For more information, see Compile Exchange . Spark is installed. Get the necessary information for schema creation in Nebula Graph, including tags and edge types.","title":"Prerequisites"},{"location":"nebula-exchange/use-exchange/ex-ug-import-steps/#procedure","text":"To import data from a source to Nebula Graph, follow these steps: Create a graph space and a schema in Nebula Graph. (Optional) Process the source data. For example, to import data from a Neo4j database, create indexes for the specified tags in Neo4j to export the data from Neo4j more quickly. Edit the configuration file for Spark, Nebula Graph, vertices, and edges. NOTE : After compiling of Exchange, refer to the example configuration files in the nebula-exchange/target/classes directory for the configuration for different sources. (Optional) Run the import command with the -D parameter to verify the configuration. For more information, see Import command parameters . Run the import command to import data into Nebula Graph. Verify the imported data in Nebula Graph. (Optional) Create and rebuild indexes in Nebula Graph. For more information, see the examples: Import data from HIVE Import data from CSV files Import data from JSON files","title":"Procedure"},{"location":"nebula-studio/st-ug-toc/","text":"Nebula Graph Studio User Guide \u00b6 About Nebula Graph Studio What is Nebula Graph Studio Limitations Check updates Deploy and connect Deploy Studio Connect to Nebula Graph Design a schema Create a schema Import data Query graph data --> Operation guide [Use Explore][DOC_TO_DO] Use Console Open in Explore View subgraphs [Export as CSV files][DOC_TO_DO] Best practices [DOC_TO_DO] Troubleshooting [DOC_TO_DO] Connection Error messages","title":"Nebula Graph Studio User Guide"},{"location":"nebula-studio/st-ug-toc/#nebula_graph_studio_user_guide","text":"About Nebula Graph Studio What is Nebula Graph Studio Limitations Check updates Deploy and connect Deploy Studio Connect to Nebula Graph Design a schema Create a schema Import data Query graph data --> Operation guide [Use Explore][DOC_TO_DO] Use Console Open in Explore View subgraphs [Export as CSV files][DOC_TO_DO] Best practices [DOC_TO_DO] Troubleshooting [DOC_TO_DO] Connection Error messages","title":"Nebula Graph Studio User Guide"},{"location":"nebula-studio/about-studio/st-ug-check-updates/","text":"Check updates \u00b6 Studio v2.x is in development. To get updated with its development, visit GitHub and read its Changelog . For Docker-based Studio v2.x, when you get access to Studio, on the upper-right corner of the page, click the version number and then New version , and you will be directed to the Changelog. When new version is released, under the nebula-web-docker directory, run these commands one by one to update the Docker image and start the services: $ git pull origin master $ cd v2 $ docker-compose pull && docker-compose up -d","title":"Check updates"},{"location":"nebula-studio/about-studio/st-ug-check-updates/#check_updates","text":"Studio v2.x is in development. To get updated with its development, visit GitHub and read its Changelog . For Docker-based Studio v2.x, when you get access to Studio, on the upper-right corner of the page, click the version number and then New version , and you will be directed to the Changelog. When new version is released, under the nebula-web-docker directory, run these commands one by one to update the Docker image and start the services: $ git pull origin master $ cd v2 $ docker-compose pull && docker-compose up -d","title":"Check updates"},{"location":"nebula-studio/about-studio/st-ug-limitations/","text":"Limitations \u00b6 This article introduces the limitations of Studio v2.x. Nebula Graph versions \u00b6 Only Nebula Graph v2.x is supported. If you are using Nebula Graph v1.x, please use Studio v1.x. For more information, see Studio v1.x User Guide . Architecture \u00b6 For now, Docker-based Studio v2.x supports x86_64 architecture only. Upload data \u00b6 On Docker-based Studio v2.x, only CSV files with comma-separated data and without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine. Data backup \u00b6 For now, you can export the queried results in the CSV format on the Console page. No other backup methods are available. nGQL statements \u00b6 On the Console page of Docker-based Studio v2.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line. Browser \u00b6 We recommend that you use the latest version of Chrome to get access to Studio v2.x.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#limitations","text":"This article introduces the limitations of Studio v2.x.","title":"Limitations"},{"location":"nebula-studio/about-studio/st-ug-limitations/#nebula_graph_versions","text":"Only Nebula Graph v2.x is supported. If you are using Nebula Graph v1.x, please use Studio v1.x. For more information, see Studio v1.x User Guide .","title":"Nebula Graph versions"},{"location":"nebula-studio/about-studio/st-ug-limitations/#architecture","text":"For now, Docker-based Studio v2.x supports x86_64 architecture only.","title":"Architecture"},{"location":"nebula-studio/about-studio/st-ug-limitations/#upload_data","text":"On Docker-based Studio v2.x, only CSV files with comma-separated data and without headers can be uploaded, but no limitations are applied to the size and store period for a single file. The maximum data volume depends on the storage capacity of your machine.","title":"Upload data"},{"location":"nebula-studio/about-studio/st-ug-limitations/#data_backup","text":"For now, you can export the queried results in the CSV format on the Console page. No other backup methods are available.","title":"Data backup"},{"location":"nebula-studio/about-studio/st-ug-limitations/#ngql_statements","text":"On the Console page of Docker-based Studio v2.x, all the nGQL syntaxes except these are supported: USE <space_name> : You cannot run such a statement on the Console page to choose a graph space. As an alternative, you can click a graph space name in the drop-down list of Current Graph Space . You cannot use line breaks (\\). As an alternative, you can use the Enter key to split a line.","title":"nGQL statements"},{"location":"nebula-studio/about-studio/st-ug-limitations/#browser","text":"We recommend that you use the latest version of Chrome to get access to Studio v2.x.","title":"Browser"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/","text":"What is Nebula Graph Studio \u00b6 Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, explore graph data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch. Release distributions \u00b6 For now, Studio v2.x has only Docker-based distribution. You can deploy Studio v2.x with Docker and connect it to Nebula Graph v2.x. For more information, see Deploy Studio . Studio on Cloud is not available now. Features \u00b6 Studio provides these features: Graphical Console enables you to run nGQL statements and read the results in a human-friendly way. The Explore function enables you to explore the graph data. It helps you dig the relationships among data and improves the efficiency of data analysis. Scenarios \u00b6 You can use Studio v2.x in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose or Nebula Graph Cloud Service to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.","title":"What is Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#what_is_nebula_graph_studio","text":"Nebula Graph Studio (Studio in short) is a browser-based visualization tool to manage Nebula Graph. It provides you with a graphical user interface to manipulate graph schemas, import data, explore graph data, and run nGQL statements to retrieve data. With Studio, you can quickly become a graph exploration expert from scratch.","title":"What is Nebula Graph Studio"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#release_distributions","text":"For now, Studio v2.x has only Docker-based distribution. You can deploy Studio v2.x with Docker and connect it to Nebula Graph v2.x. For more information, see Deploy Studio . Studio on Cloud is not available now.","title":"Release distributions"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#features","text":"Studio provides these features: Graphical Console enables you to run nGQL statements and read the results in a human-friendly way. The Explore function enables you to explore the graph data. It helps you dig the relationships among data and improves the efficiency of data analysis.","title":"Features"},{"location":"nebula-studio/about-studio/st-ug-what-is-graph-studio/#scenarios","text":"You can use Studio v2.x in one of these scenarios: You have a dataset, and you want to explore and analyze data in a visualized way. You can use Docker Compose or Nebula Graph Cloud Service to deploy Nebula Graph and then use Studio to explore or analyze data in a visualized way. You have deployed Nebula Graph and imported a dataset. You want to use a GUI to run nGQL statements or explore and analyze graph data in a visualized way. You are a beginner of nGQL (Nebula Graph Query Language) and you prefer to use a GUI rather than a command-line interface (CLI) to learn the language.","title":"Scenarios"},{"location":"nebula-studio/install-configure/st-ug-connect/","text":"Connect to Nebula Graph \u00b6 For Docker-based Studio v2.x, when it is started, you must configure it to connect to Nebula Graph v2.x. This article introduces how to connect Docker-based Studio v2.x to Nebula Graph v2.x. Prerequisites \u00b6 Before you connect Docker-based Studio v2.x to Nebula Graph v2.x, you must do a check of these: The Nebula Graph v2.x services and Studio v2.x are started. For more information, see Deploy Studio . You have the IP address and the port used by the Graph service of Nebula Graph v2.x. The default port is 9669 . NOTE : Run ifconfig or ipconfig on the machine to get the IP address. Procedure \u00b6 To connect Docker-based Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . NOTE : When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : You can use user and password as the username and its password. After the configuration, click the Connect button. If you can see the Console page, Docker-based Studio is successfully connected to Nebula Graph. One session continues up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again. Next to do \u00b6 When Studio v2.x is successfully connected to Nebula Graph v2.x, you can learn nGQL v2.x on the Console page or explore and analyze data on the Explore page.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/install-configure/st-ug-connect/#connect_to_nebula_graph","text":"For Docker-based Studio v2.x, when it is started, you must configure it to connect to Nebula Graph v2.x. This article introduces how to connect Docker-based Studio v2.x to Nebula Graph v2.x.","title":"Connect to Nebula Graph"},{"location":"nebula-studio/install-configure/st-ug-connect/#prerequisites","text":"Before you connect Docker-based Studio v2.x to Nebula Graph v2.x, you must do a check of these: The Nebula Graph v2.x services and Studio v2.x are started. For more information, see Deploy Studio . You have the IP address and the port used by the Graph service of Nebula Graph v2.x. The default port is 9669 . NOTE : Run ifconfig or ipconfig on the machine to get the IP address.","title":"Prerequisites"},{"location":"nebula-studio/install-configure/st-ug-connect/#procedure","text":"To connect Docker-based Studio to Nebula Graph, follow these steps: On the Config Server page of Studio, configure these fields: Host : Enter the IP address and the port of the Graph service of Nebula Graph. The valid format is IP:port . The default port is 9669 . NOTE : When Nebula Graph and Studio are deployed on the same machine, you must enter the IP address of the machine, but not 127.0.0.1 or localhost , in the Host field. Username and Password : You can use user and password as the username and its password. After the configuration, click the Connect button. If you can see the Console page, Docker-based Studio is successfully connected to Nebula Graph. One session continues up to 30 minutes. If you do not operate Studio within 30 minutes, the active session will time out and you must connect to Nebula Graph again.","title":"Procedure"},{"location":"nebula-studio/install-configure/st-ug-connect/#next_to_do","text":"When Studio v2.x is successfully connected to Nebula Graph v2.x, you can learn nGQL v2.x on the Console page or explore and analyze data on the Explore page.","title":"Next to do"},{"location":"nebula-studio/install-configure/st-ug-deploy/","text":"Deploy Studio \u00b6 This article introduces how to deploy Docker-based Studio v2.x. Prerequisites \u00b6 Before you deploy Docker-based Studio v2.x, you must do a check of these: The Nebula Graph v2.x services are deployed and started. For more information, see Nebula Graph Database Manual . NOTE : Different methods are available for you to deploy Nebula Graph. If this is your first time to use Nebula Graph, we recommend that you use Docker Compose to deploy Nebula Graph. For more information, see Deploy Nebula Graph with Docker Compose . On the machine where Studio v2.x will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation . Procedure \u00b6 To deploy and start Docker-based Studio v2.x, run these commands one by one: Download the configuration files for the deployment. git clone https://github.com/vesoft-inc/nebula-web-docker.git Change to the nebula-web-docker/v2 directory. cd nebula-web-docker/v2 Pull the Docker image of Studio v2.x. docker-compose pull Build and start Docker-based Studio v2.x. In this command, -d is to run the containers in the background. docker-compose up -d If these lines return, Docker-based Studio v2.x is deployed and started. Creating docker_importer_1 ... done Creating docker_client_1 ... done Creating docker_web_1 ... done Creating docker_nginx_1 ... done When Docker-based Studio v2.x is started, use http://ip address:7001 to get access to Studio v2.x. NOTE : Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully. Next to do \u00b6 On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Deploy Studio"},{"location":"nebula-studio/install-configure/st-ug-deploy/#deploy_studio","text":"This article introduces how to deploy Docker-based Studio v2.x.","title":"Deploy Studio"},{"location":"nebula-studio/install-configure/st-ug-deploy/#prerequisites","text":"Before you deploy Docker-based Studio v2.x, you must do a check of these: The Nebula Graph v2.x services are deployed and started. For more information, see Nebula Graph Database Manual . NOTE : Different methods are available for you to deploy Nebula Graph. If this is your first time to use Nebula Graph, we recommend that you use Docker Compose to deploy Nebula Graph. For more information, see Deploy Nebula Graph with Docker Compose . On the machine where Studio v2.x will run, Docker Compose is installed and started. For more information, see Docker Compose Documentation .","title":"Prerequisites"},{"location":"nebula-studio/install-configure/st-ug-deploy/#procedure","text":"To deploy and start Docker-based Studio v2.x, run these commands one by one: Download the configuration files for the deployment. git clone https://github.com/vesoft-inc/nebula-web-docker.git Change to the nebula-web-docker/v2 directory. cd nebula-web-docker/v2 Pull the Docker image of Studio v2.x. docker-compose pull Build and start Docker-based Studio v2.x. In this command, -d is to run the containers in the background. docker-compose up -d If these lines return, Docker-based Studio v2.x is deployed and started. Creating docker_importer_1 ... done Creating docker_client_1 ... done Creating docker_web_1 ... done Creating docker_nginx_1 ... done When Docker-based Studio v2.x is started, use http://ip address:7001 to get access to Studio v2.x. NOTE : Run ifconfig or ipconfig to get the IP address of the machine where Docker-based Studio is running. On the machine running Docker-based Studio, you can use http://localhost:7001 to get access to Studio. If you can see the Config Server page on the browser, Docker-based Studio is started successfully.","title":"Procedure"},{"location":"nebula-studio/install-configure/st-ug-deploy/#next_to_do","text":"On the Config Server page, connect Docker-based Studio to Nebula Graph. For more information, see Connect to Nebula Graph .","title":"Next to do"},{"location":"nebula-studio/manage-schema/st-ug-crud-edge-type/","text":"","title":"St ug crud edge type"},{"location":"nebula-studio/manage-schema/st-ug-crud-index/","text":"","title":"St ug crud index"},{"location":"nebula-studio/manage-schema/st-ug-crud-space/","text":"","title":"St ug crud space"},{"location":"nebula-studio/manage-schema/st-ug-crud-tag/","text":"","title":"St ug crud tag"},{"location":"nebula-studio/quick-start/st-ug-create-schema/","text":"Create a schema \u00b6 To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. NOTE : You can use nebula-console to create a schema. For more information, see Deploy Nebula Graph with Docker Compose and Get started with Nebula Graph . Prerequisites \u00b6 To create a graph schema on Studio v2.x, you must do a check of these: Studio is connected to Nebula Graph v2.x. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. NOTE : If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE . In this example, we recommend that you set vid_type=FIXED_STRING(100) in the CREATE SPACE statement. Create a schema with Schema \u00b6 To create a schema on the Schema page, follow these steps: Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types . Create a schema with Console \u00b6 To create a schema on the Console page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is used. In the input box, enter these statements one by one and click the button . // To create a tag named \"user\", with no property nebula> CREATE TAG user (); // To create a tag named \"course\", with one property nebula> CREATE TAG course (courseId int); // To create an edge type named \"action\", with seven properties nebula> CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types nebula> DESCRIBE TAG user; nebula> DESCRIBE TAG course; nebula> DESCRIBE EDGE action; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types. Next to do \u00b6 When a schema is created, you can import data .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema","text":"To batch import data into Nebula Graph, you must have a graph schema. You can create a schema on the Console page or on the Schema page of Studio. NOTE : You can use nebula-console to create a schema. For more information, see Deploy Nebula Graph with Docker Compose and Get started with Nebula Graph .","title":"Create a schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#prerequisites","text":"To create a graph schema on Studio v2.x, you must do a check of these: Studio is connected to Nebula Graph v2.x. Your account has the privilege of GOD, ADMIN, or DBA. The schema is designed. A graph space is created. NOTE : If no graph space exists and your account has the GOD privilege, you can create a graph space on the Console page. For more information, see CREATE SPACE . In this example, we recommend that you set vid_type=FIXED_STRING(100) in the CREATE SPACE statement.","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_schema","text":"To create a schema on the Schema page, follow these steps: Create tags. For more information, see Operate tags . Create edge types. For more information, see Operate edge types .","title":"Create a schema with Schema"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#create_a_schema_with_console","text":"To create a schema on the Console page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is used. In the input box, enter these statements one by one and click the button . // To create a tag named \"user\", with no property nebula> CREATE TAG user (); // To create a tag named \"course\", with one property nebula> CREATE TAG course (courseId int); // To create an edge type named \"action\", with seven properties nebula> CREATE EDGE action (actionId int, duration double, label bool, feature0 double, feature1 double, feature2 double, feature3 double); If the preceding statements are executed successfully, the schema is created. You can run the statements as follows to view the schema. // To list all the tags in the current graph space nebula> SHOW TAGS; // To list all the edge types in the current graph space nebula> SHOW EDGES; // To view the definition of the tags and edge types nebula> DESCRIBE TAG user; nebula> DESCRIBE TAG course; nebula> DESCRIBE EDGE action; If the schema is created successfully, in the result window, you can see the definition of the tags and edge types.","title":"Create a schema with Console"},{"location":"nebula-studio/quick-start/st-ug-create-schema/#next_to_do","text":"When a schema is created, you can import data .","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-explore/","text":"Query graph data \u00b6 When data is imported, you can use the Console page or the Explore page to query graph data. For example, if you want to query the properties of the course named History of Chinese Women Through Time , you can perform these optional operations: On the Console tab: Run FETCH PROP ON * \"History of Chinese Women Through Time\"; . The result window shows all the property information of this vertex. When the result returns, click the Open in Explore button and then you can view the vertex information in a visualized way. On the Explore tab: Click the Start with Vertices button. In the dialog box, enter History of Chinese Women Through Time and then click the Add button. On the board, you can see the vertex. Move your mouse pointer on the vertex to see the vertex details, as shown in the preceding figure.","title":"Query graph data"},{"location":"nebula-studio/quick-start/st-ug-explore/#query_graph_data","text":"When data is imported, you can use the Console page or the Explore page to query graph data. For example, if you want to query the properties of the course named History of Chinese Women Through Time , you can perform these optional operations: On the Console tab: Run FETCH PROP ON * \"History of Chinese Women Through Time\"; . The result window shows all the property information of this vertex. When the result returns, click the Open in Explore button and then you can view the vertex information in a visualized way. On the Explore tab: Click the Start with Vertices button. In the dialog box, enter History of Chinese Women Through Time and then click the Add button. On the board, you can see the vertex. Move your mouse pointer on the vertex to see the vertex details, as shown in the preceding figure.","title":"Query graph data"},{"location":"nebula-studio/quick-start/st-ug-import-data/","text":"Import data \u00b6 After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis. Prerequisites \u00b6 To batch import data, do a check of these: Studio v2.x is connected to Nebula Graph v2.x. A schema is created. CSV files for vertex and edge data separately are created. Your account has privilege of GOD, ADMIN, DBA, or USER. Procedure \u00b6 To batch import data, follow these steps: In the toolbar, click the Import tab. On the Select Space page, choose a graph space name. In this example, mooc_actions is used. And then click the Next button. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, user.csv , course.csv , and actions.csv are chosen. NOTE : You can choose multiple CSV files at the same time. On the Select Files page, do a check of the file size and click Preview or Delete in the Operations column to make sure that all source data is correct. And then click the Next button. On the Map Vertices page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, user.csv or course.csv is chosen. In the DataSource X tab, click the + Tag button. In the vertexId section, do these operations: a. In the CSV Index column, click Mapping . b. In the dialog box, choose a column from the CSV file. In this example, the only one cloumn of user.csv is chosen to generate VIDs representing users and the courseName column of course.csv is chosen to generate VIDs representing courses. In the TAG 1 section, do these operations: a. In the TAG drop-down list, choose a tag name. In this example, user is used for the user.csv file, and course is used for the course.csv file. b. In the property list, click Mapping to choose a data column from the CSV file as the value of a property. In this example, no data is chosen for user . For the course tag, choose Column 0 for the courseId property and set its type to int . (Optional) If necessary, repeat Step 5 through Step 8 for more tags. When the configuration is done, click the Next button. When Config validation was successful prompts, data mapping for the vertices is successful. On the Map Edges page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, the actions.csv file is chosen. In the Type drop-down list, choose an edge type name. In this example, action is chosen. In the property list, click Mapping to choose a column from the actions.csv file as values of a property for the edges. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the users and dstId must be set to the VIDs of the courses. rank is optional. When the configuration is done, click the Next button. On the Import page, click the Start Import button. On the log window, you can see the import progress. The consumed time depends on the data volume. During data import, you can click the Stop Import button to stop data import. When the log window shows information as follows, the data import is done. Next to do \u00b6 When the data are imported to Nebula Graph v2.x, you can query graph data .","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#import_data","text":"After CSV files of data and a schema are created, you can use the Import page to batch import vertex and edge data into Nebula Graph for graph exploration and data analysis.","title":"Import data"},{"location":"nebula-studio/quick-start/st-ug-import-data/#prerequisites","text":"To batch import data, do a check of these: Studio v2.x is connected to Nebula Graph v2.x. A schema is created. CSV files for vertex and edge data separately are created. Your account has privilege of GOD, ADMIN, DBA, or USER.","title":"Prerequisites"},{"location":"nebula-studio/quick-start/st-ug-import-data/#procedure","text":"To batch import data, follow these steps: In the toolbar, click the Import tab. On the Select Space page, choose a graph space name. In this example, mooc_actions is used. And then click the Next button. On the Upload Files page, click the Upload Files button and then choose CSV files. In this example, user.csv , course.csv , and actions.csv are chosen. NOTE : You can choose multiple CSV files at the same time. On the Select Files page, do a check of the file size and click Preview or Delete in the Operations column to make sure that all source data is correct. And then click the Next button. On the Map Vertices page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, user.csv or course.csv is chosen. In the DataSource X tab, click the + Tag button. In the vertexId section, do these operations: a. In the CSV Index column, click Mapping . b. In the dialog box, choose a column from the CSV file. In this example, the only one cloumn of user.csv is chosen to generate VIDs representing users and the courseName column of course.csv is chosen to generate VIDs representing courses. In the TAG 1 section, do these operations: a. In the TAG drop-down list, choose a tag name. In this example, user is used for the user.csv file, and course is used for the course.csv file. b. In the property list, click Mapping to choose a data column from the CSV file as the value of a property. In this example, no data is chosen for user . For the course tag, choose Column 0 for the courseId property and set its type to int . (Optional) If necessary, repeat Step 5 through Step 8 for more tags. When the configuration is done, click the Next button. When Config validation was successful prompts, data mapping for the vertices is successful. On the Map Edges page, click the + Bind Datasource button, and in the dialog box, choose a CSV file. In this example, the actions.csv file is chosen. In the Type drop-down list, choose an edge type name. In this example, action is chosen. In the property list, click Mapping to choose a column from the actions.csv file as values of a property for the edges. srcId and dstId are the VIDs of the source vertex and destination vertex of an edge. In this example, srcId must be set to the VIDs of the users and dstId must be set to the VIDs of the courses. rank is optional. When the configuration is done, click the Next button. On the Import page, click the Start Import button. On the log window, you can see the import progress. The consumed time depends on the data volume. During data import, you can click the Stop Import button to stop data import. When the log window shows information as follows, the data import is done.","title":"Procedure"},{"location":"nebula-studio/quick-start/st-ug-import-data/#next_to_do","text":"When the data are imported to Nebula Graph v2.x, you can query graph data .","title":"Next to do"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/","text":"Design a schema \u00b6 To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties In this article, the Social Network: MOOC User Action Dataset and 97 distinct course names are used to introduce how to design a schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag user No property. Represents users of the specified MOOC platform. The userId values are used as VIDs of user vertices. Tag course courseId ( int ). Represents the courses on the specified MOOC platform. The courseName values are used as the VIDs of the course vertices. Edge type action - actionId ( int ) - duration ( double ): Represents the duration of an action measured in seconds from the beginning. Its values are equal to the timestamp values in the data source. - label ( bool ): Represents whether a user drops out after an action. TRUE indicates a drop-out action, FALSE otherwise. - feature0 ( double ) - feature1 ( double ) - feature2 ( double ) - feature3 ( double ) Represents actions taken by users on the specified MOOC platform. An action links a user and a course and the direction is from a user to a course. It has four features. This figure shows the relationship ( action ) between a user and a course on the MOOC platform.","title":"Design a schema"},{"location":"nebula-studio/quick-start/st-ug-plan-schema/#design_a_schema","text":"To manipulate graph data in Nebula Graph with Studio, you must have a graph schema. This article introduces how to design a graph schema for Nebula Graph. A graph schema for Nebula Graph must have these essential elements: Tags (namely vertex types) and their properties. Edge types and their properties In this article, the Social Network: MOOC User Action Dataset and 97 distinct course names are used to introduce how to design a schema. This table gives all the essential elements of the schema. Element Name Property name (Data type) Description Tag user No property. Represents users of the specified MOOC platform. The userId values are used as VIDs of user vertices. Tag course courseId ( int ). Represents the courses on the specified MOOC platform. The courseName values are used as the VIDs of the course vertices. Edge type action - actionId ( int ) - duration ( double ): Represents the duration of an action measured in seconds from the beginning. Its values are equal to the timestamp values in the data source. - label ( bool ): Represents whether a user drops out after an action. TRUE indicates a drop-out action, FALSE otherwise. - feature0 ( double ) - feature1 ( double ) - feature2 ( double ) - feature3 ( double ) Represents actions taken by users on the specified MOOC platform. An action links a user and a course and the direction is from a user to a course. It has four features. This figure shows the relationship ( action ) between a user and a course on the MOOC platform.","title":"Design a schema"},{"location":"nebula-studio/quick-start/st-ug-prepare-csv/","text":"Prepare CSV files \u00b6 With Studio, you can batch import vertex and edge data into Nebula Graph. Currently, only CSV files without headers and comma-separated data are supported. Each file represents vertex or edge data of one type. To create applicable CSV files, process the source data as follows: Generate CSV files for vertex and edge data: user.csv : Contains the vertices representing users with no property. The userId column can be used as the vertex IDs. course.csv : Contains the vertices representing courses with the courseId properties. The courseName column can be used as the vertex IDs. actions.csv contains: The edges representing actions with the actionId , label , duration , feature0 , feature1 , feature2 , and feature3 properties. For the label column, 1 is replaced with TRUE and 0 is replaced with FALSE . The userId column representing the source vertices of the edges. The courseName column representing the destination vertices of the edges. This figure shows an example of a CSV file with the header. Delete all the headers from the CSV files.","title":"Prepare CSV files"},{"location":"nebula-studio/quick-start/st-ug-prepare-csv/#prepare_csv_files","text":"With Studio, you can batch import vertex and edge data into Nebula Graph. Currently, only CSV files without headers and comma-separated data are supported. Each file represents vertex or edge data of one type. To create applicable CSV files, process the source data as follows: Generate CSV files for vertex and edge data: user.csv : Contains the vertices representing users with no property. The userId column can be used as the vertex IDs. course.csv : Contains the vertices representing courses with the courseId properties. The courseName column can be used as the vertex IDs. actions.csv contains: The edges representing actions with the actionId , label , duration , feature0 , feature1 , feature2 , and feature3 properties. For the label column, 1 is replaced with TRUE and 0 is replaced with FALSE . The userId column representing the source vertices of the edges. The courseName column representing the destination vertices of the edges. This figure shows an example of a CSV file with the header. Delete all the headers from the CSV files.","title":"Prepare CSV files"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/","text":"Open in Explore \u00b6 With the Open in Explore function, you can run nGQL statements on the Console page to query vertex or edge data and then view the result on the Explore page in a visualized way. Prerequisites \u00b6 To use the Open in Explore function, you must do a check of these: The version of Studio is v2.0.0-alpha or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. Query and visualize edge data \u00b6 To query edge data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the source vertex and the destination vertex of an edge. Here is an nGQL statement example. nebula> MATCH (u:user {userId: 1}) - [:action] -> (c) RETURN u.userId AS UserID, c.courseName AS Course; NOTE : For more information about the MATCH syntax, see MATCH in nGQL User Guide . The query result gives the edges between User 1 and the courses that he/she takes on the MOOC platform, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Edge Type . b. In the Edge Type field, enter an edge type name. In this example, action is used. c. In the Src ID field, choose a column name from the result table representing the VIDs of the source vertices. In this example, UserID is chosen. d. In the Dst ID field, choose a column name from the result table representing the VIDs of the destination vertices. In this example, Course is chosen. e. (Optional) If the result table contains the ranking information of the edges, in the Rank field, choose a column name representing the rank of the edges. If no ranking information exists in the result, leave the Rank field blank. f. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the edge data. Query and visualize vertex data \u00b6 To query vertex data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the vertices. Here is an nGQL statement example. nebula> GO FROM \"1\" OVER action YIELD action._dst AS Course; The query result gives the courses that the specified user took, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Vertex . b. In the Vertex ID field, choose a column name from the result table representing the VIDs of the vertices. In this example, Course is chosen. c. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the queried result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data. When the data is inserted, you can view the visualized representation of the vertex data. Next to do \u00b6 On the Explore page, you can expand the board to explore and analyze graph data.","title":"Open in Explore"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#open_in_explore","text":"With the Open in Explore function, you can run nGQL statements on the Console page to query vertex or edge data and then view the result on the Explore page in a visualized way.","title":"Open in Explore"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#prerequisites","text":"To use the Open in Explore function, you must do a check of these: The version of Studio is v2.0.0-alpha or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database.","title":"Prerequisites"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#query_and_visualize_edge_data","text":"To query edge data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the source vertex and the destination vertex of an edge. Here is an nGQL statement example. nebula> MATCH (u:user {userId: 1}) - [:action] -> (c) RETURN u.userId AS UserID, c.courseName AS Course; NOTE : For more information about the MATCH syntax, see MATCH in nGQL User Guide . The query result gives the edges between User 1 and the courses that he/she takes on the MOOC platform, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Edge Type . b. In the Edge Type field, enter an edge type name. In this example, action is used. c. In the Src ID field, choose a column name from the result table representing the VIDs of the source vertices. In this example, UserID is chosen. d. In the Dst ID field, choose a column name from the result table representing the VIDs of the destination vertices. In this example, Course is chosen. e. (Optional) If the result table contains the ranking information of the edges, in the Rank field, choose a column name representing the rank of the edges. If no ranking information exists in the result, leave the Rank field blank. f. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the edge data.","title":"Query and visualize edge data"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#query_and_visualize_vertex_data","text":"To query vertex data on the Console page and then view the result on the Explore page, follow these steps: In the toolbar, click the Console tab. In the Current Graph Space field, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter an nGQL statement and click the button . NOTE : The query result must contain the VIDs of the vertices. Here is an nGQL statement example. nebula> GO FROM \"1\" OVER action YIELD action._dst AS Course; The query result gives the courses that the specified user took, as shown in this figure. Click the Open in Explore button. In the dialog box, configure as follows: a. Click Vertex . b. In the Vertex ID field, choose a column name from the result table representing the VIDs of the vertices. In this example, Course is chosen. c. When the configuration is done, click the Import button. If some data exists on the board of Explore , choose a method to insert data: Incremental Insertion : Click this button to add the queried result to the existing data on the board. Insert After Clear : Click this button to clear the existing data from the board and then add the data. When the data is inserted, you can view the visualized representation of the vertex data.","title":"Query and visualize vertex data"},{"location":"nebula-studio/use-console/st-ug-open-in-explore/#next_to_do","text":"On the Explore page, you can expand the board to explore and analyze graph data.","title":"Next to do"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/","text":"View subgraphs \u00b6 With the View Subgraphs function, you can run a FIND SHORTEST | ALL PATH or a GET SUBGRAPH statement on the Console page and then view the result on the Explore page. Studio version \u00b6 Studio of v2.0.0-alpha or later versions supports this function. To update the version, see Check updates . Prerequisites \u00b6 To use the View Subgraphs function, you must do a check of these: The version of Studio is v2.0.0-alpha or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. In the example of this article, the mooc_actions dataset is used. For more information, see Import data . Procedure \u00b6 To query the paths or subgraph on the Console page and then view them on the Explore page, follow these steps: In the navigation bar, click the Console tab. In the Current Graph Space dropdown list, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter a FIND SHORTEST PATH , FIND ALL PATH , or GET SUBGRAPH statement and click Run . Here is an nGQL statement example. // Run FIND ALL PATH nebula> FIND ALL PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run FIND SHORTEST PATH nebula> FIND SHORTEST PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run GET SUBGRAPH nebula> GET SUBGRAPH 1 STEPS FROM \"1\"; Take the FIND ALL PATH for example, the queried result gives all the paths from the specified user vertices to the course vertices, as shown in this figure. Click the View Subgraphs button. (Optional) If some data exists on the board of Explore , choose a method to insert data: - Incremental Insertion : Click this button to add the result to the existing data on the board. - Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the paths. Next to do \u00b6 On the Explore page, you can expand the graph to explore and analyze graph data.","title":"View subgraphs"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#view_subgraphs","text":"With the View Subgraphs function, you can run a FIND SHORTEST | ALL PATH or a GET SUBGRAPH statement on the Console page and then view the result on the Explore page.","title":"View subgraphs"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#studio_version","text":"Studio of v2.0.0-alpha or later versions supports this function. To update the version, see Check updates .","title":"Studio version"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#prerequisites","text":"To use the View Subgraphs function, you must do a check of these: The version of Studio is v2.0.0-alpha or later. Studio is connected to Nebula Graph v2.x. A dataset exists in the database. In the example of this article, the mooc_actions dataset is used. For more information, see Import data .","title":"Prerequisites"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#procedure","text":"To query the paths or subgraph on the Console page and then view them on the Explore page, follow these steps: In the navigation bar, click the Console tab. In the Current Graph Space dropdown list, choose a graph space name. In this example, mooc_actions is chosen. In the input box, enter a FIND SHORTEST PATH , FIND ALL PATH , or GET SUBGRAPH statement and click Run . Here is an nGQL statement example. // Run FIND ALL PATH nebula> FIND ALL PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run FIND SHORTEST PATH nebula> FIND SHORTEST PATH FROM \"1\",\"2\",\"4\",\"6\",\"42\" to \"History of Ecology\",\"Neurobiology\" OVER action; // Run GET SUBGRAPH nebula> GET SUBGRAPH 1 STEPS FROM \"1\"; Take the FIND ALL PATH for example, the queried result gives all the paths from the specified user vertices to the course vertices, as shown in this figure. Click the View Subgraphs button. (Optional) If some data exists on the board of Explore , choose a method to insert data: - Incremental Insertion : Click this button to add the result to the existing data on the board. - Insert After Clear : Click this button to clear the existing data from the board and then add the data to the board. When the data is inserted, you can view the visualized representation of the paths.","title":"Procedure"},{"location":"nebula-studio/use-console/st-ug-visualize-subgraph/#next_to_do","text":"On the Explore page, you can expand the graph to explore and analyze graph data.","title":"Next to do"}]}