{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Nebula Graph \u00b6 Nebula Graph is an open-source graph database capable of hosting super large-scale graphs with billions of vertices (nodes) and trillions of edges, with milliseconds of latency. It delivers enterprise-grade high performance to simplify the most complex data sets imaginable into meaningful and useful information. Below is the architecture of Nebula Graph : Compared with other graph database solutions, Nebula Graph has the following advantages: Symmetrically distributed Storage and computing separation Horizontal scalability Strong data consistency by RAFT protocol SQL-like query language Role-based access control for higher level security Quick start \u00b6 Read the Getting started guide to quickly get going with Nebula Graph . Please note that you must install Nebula Graph by installing source code , rpm/deb packages or docker compose , before you can actually start using it. If you prefer a video tutorial, visit our YouTube channel . In case you encounter any problem, be sure to ask us on our official forum . Documentation \u00b6 English \u7b80\u4f53\u4e2d\u6587 Roadmap \u00b6 See our Roadmap for what's coming soon in Nebula Graph . Visualization Tool: Nebula Graph Studio \u00b6 Visit Nebula Graph Studio for visual exploration of graph data on a web UI. Supported Clients \u00b6 Go Python Java Licensing \u00b6 Nebula Graph is under Apache 2.0 license. So you can freely download, modify, and deploy the source code to meet your needs. You can also freely deploy Nebula Graph as a back-end service to support your SaaS deployment. In order to prevent cloud providers from monetizing the project without contributing back, we added Commons Clause 1.0 to the project. As mentioned, we are fully committed to the open source community. We would love to hear your thoughts on the licensing model and are willing to make it more suitable for the community. Contributing \u00b6 Contributions are warmly welcomed and greatly appreciated. Here are a few ways you can contribute: Start by some good first issues Submit Pull Requests to us. See how-to-contribute . Getting help & Contact \u00b6 In case you encounter any problems playing around Nebula Graph , please reach out for help: Official Forum Twitter: @NebulaGraph Facebook page LinkedIn page If you like Nebula Graph , please leave us a star.","title":"What is Nebula Graph"},{"location":"#what_is_nebula_graph","text":"Nebula Graph is an open-source graph database capable of hosting super large-scale graphs with billions of vertices (nodes) and trillions of edges, with milliseconds of latency. It delivers enterprise-grade high performance to simplify the most complex data sets imaginable into meaningful and useful information. Below is the architecture of Nebula Graph : Compared with other graph database solutions, Nebula Graph has the following advantages: Symmetrically distributed Storage and computing separation Horizontal scalability Strong data consistency by RAFT protocol SQL-like query language Role-based access control for higher level security","title":"What is Nebula Graph"},{"location":"#quick_start","text":"Read the Getting started guide to quickly get going with Nebula Graph . Please note that you must install Nebula Graph by installing source code , rpm/deb packages or docker compose , before you can actually start using it. If you prefer a video tutorial, visit our YouTube channel . In case you encounter any problem, be sure to ask us on our official forum .","title":"Quick start"},{"location":"#documentation","text":"English \u7b80\u4f53\u4e2d\u6587","title":"Documentation"},{"location":"#roadmap","text":"See our Roadmap for what's coming soon in Nebula Graph .","title":"Roadmap"},{"location":"#visualization_tool_nebula_graph_studio","text":"Visit Nebula Graph Studio for visual exploration of graph data on a web UI.","title":"Visualization Tool: Nebula Graph Studio"},{"location":"#supported_clients","text":"Go Python Java","title":"Supported Clients"},{"location":"#licensing","text":"Nebula Graph is under Apache 2.0 license. So you can freely download, modify, and deploy the source code to meet your needs. You can also freely deploy Nebula Graph as a back-end service to support your SaaS deployment. In order to prevent cloud providers from monetizing the project without contributing back, we added Commons Clause 1.0 to the project. As mentioned, we are fully committed to the open source community. We would love to hear your thoughts on the licensing model and are willing to make it more suitable for the community.","title":"Licensing"},{"location":"#contributing","text":"Contributions are warmly welcomed and greatly appreciated. Here are a few ways you can contribute: Start by some good first issues Submit Pull Requests to us. See how-to-contribute .","title":"Contributing"},{"location":"#getting_help_contact","text":"In case you encounter any problems playing around Nebula Graph , please reach out for help: Official Forum Twitter: @NebulaGraph Facebook page LinkedIn page If you like Nebula Graph , please leave us a star.","title":"Getting help &amp; Contact"},{"location":"doc-tools/","text":"Generate documentation in PDF \u00b6 This document teaches you how to make documentation in PDF. Step One: Merge \u00b6 Use the provided script merge-all.py to merge all the markdown doc files into one. Step Two: Generate TOC \u00b6 Use pandoc to generated TOC (table of content) for the file you just merged. Firstly, you should make sure that pandoc has been installed on your machine. Open your terminal and run the command: pandoc -v If pandoc is not installed, please install it first. Windows choco install pandoc macOS brew install pandoc If you have any questions on pandoc installation, please check its document here . To generate TOC, you should first change directory to the merged file and type the following command: pandoc -s --toc merged.md -o merged.md Note : The default number of section levels is 3 in the table of contents (which means that level-1, 2, and 3 headings will be listed in the contents), use --toc-depth=NUMBER to specify that number. Step Three: Generate PDF \u00b6 You can convert the merged markdown file into PDF and print it out for easy-reading. Use the following command to generate PDF: pandoc merged.md -o merged.pdf Note: Make sure MiKTeX is installed. Now you've got your PDF documentation and have fun with Nebula Graph .","title":"Generate documentation in PDF"},{"location":"doc-tools/#generate_documentation_in_pdf","text":"This document teaches you how to make documentation in PDF.","title":"Generate documentation in PDF"},{"location":"doc-tools/#step_one_merge","text":"Use the provided script merge-all.py to merge all the markdown doc files into one.","title":"Step One: Merge"},{"location":"doc-tools/#step_two_generate_toc","text":"Use pandoc to generated TOC (table of content) for the file you just merged. Firstly, you should make sure that pandoc has been installed on your machine. Open your terminal and run the command: pandoc -v If pandoc is not installed, please install it first. Windows choco install pandoc macOS brew install pandoc If you have any questions on pandoc installation, please check its document here . To generate TOC, you should first change directory to the merged file and type the following command: pandoc -s --toc merged.md -o merged.md Note : The default number of section levels is 3 in the table of contents (which means that level-1, 2, and 3 headings will be listed in the contents), use --toc-depth=NUMBER to specify that number.","title":"Step Two: Generate TOC"},{"location":"doc-tools/#step_three_generate_pdf","text":"You can convert the merged markdown file into PDF and print it out for easy-reading. Use the following command to generate PDF: pandoc merged.md -o merged.pdf Note: Make sure MiKTeX is installed. Now you've got your PDF documentation and have fun with Nebula Graph .","title":"Step Three: Generate PDF"},{"location":"manual-EN/","text":"Welcome to the Official Nebula Graph Documentation \u00b6 Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency. Preface \u00b6 About This Manual Manual Change Log Overview (for Beginners) \u00b6 Introduction Concepts Data Model Query Language Overview Quick Start and Useful Links Get Started FAQ Build Source Code Import .csv File Nebula Graph Clients Design and Architecture Design Overview Storage Architecture Query Engine Query Language (for All Users) \u00b6 Data Types Data Types Type Conversion Functions and Operators Bitwise Operators Built-In Functions Comparison Functions And Operators Group By Function Limit Syntax Logical Operators Order By Function Set Operations String Comparison Functions and Operators uuid Function Language Structure Literal Values Boolean Literals Numeric Literals String Literals Comment Syntax Identifier Case Sensitivity Keywords and Reserved Words Pipe Syntax Property Reference Schema Object Names Statement Composition User-Defined Variables Statement Syntax Data Definition Statements Alter Tag/Edge Syntax Create Space Syntax Create Tag/Edge Syntax Drop Edge Syntax Drop Space Syntax Drop Tag Syntax Index TTL (time-to-live) Data Query and Manipulation Statements Delete Edge Syntax Delete Vertex Syntax Fetch Syntax Go Syntax Insert Edge Syntax Insert Vertex Syntax Lookup Syntax Return Syntax Update Vertex/Edge Syntax Upsert Syntax Where Syntax Yield Syntax Utility Statements Show Statements Show Charset Syntax Show Collation Syntax Show Configs Syntax Show Create Spaces Syntax Show Create Tag/Edge Syntax Show Hosts Syntax Show Indexes Syntax Show Parts Syntax Show Roles Syntax Show Snapshots Syntax Show Spaces Syntax Show Tag/Edge Syntax Show Users Syntax Describe Syntax Use Syntax Graph Algorithms Find Path Syntax Build Develop and Administration (for Developers and DBA) \u00b6 Build Build Source Code Build By Docker Install rpm Installation Start and Stop Services Configuration System Requirement Config Persistency and Priority CONFIG Syntax Metad Configuration Graphd Configuration Storaged Configuration Console Configuration Account Management Statement Alter User Syntax Built-in Roles Change Password Create User Drop User Grant Role Revoke Batch Data Management Data Import Import .csv File Spark Writer Data Export Dump Tool Storage Balance Cluster Snapshot Long Time-Consuming Task Management Monitoring and Statistics Connect Prometheus Metrics Meta Metrics Storage Metrics Graph Metrics Development and API Key Value API Nebula Graph Clients Contributions (for Contributors) \u00b6 Contribute to Documentation Cpp Coding Style Developer Documentation Style Guide How to Contribute Appendix \u00b6 Gremlin V.S. nGQL Cypher V.S. nGQL Upgrading Nebula Graph Misc \u00b6 Video \u00b6 YouTube bilibili","title":"Welcome to the Official Nebula Graph Documentation"},{"location":"manual-EN/#welcome_to_the_official_nebula_graph_documentation","text":"Nebula Graph is a distributed, scalable, and lightning-fast graph database. It is the optimal solution in the world capable of hosting graphs with dozens of billions of vertices (nodes) and trillions of edges with millisecond latency.","title":"Welcome to the Official Nebula Graph Documentation"},{"location":"manual-EN/#preface","text":"About This Manual Manual Change Log","title":"Preface"},{"location":"manual-EN/#overview_for_beginners","text":"Introduction Concepts Data Model Query Language Overview Quick Start and Useful Links Get Started FAQ Build Source Code Import .csv File Nebula Graph Clients Design and Architecture Design Overview Storage Architecture Query Engine","title":"Overview (for Beginners)"},{"location":"manual-EN/#query_language_for_all_users","text":"Data Types Data Types Type Conversion Functions and Operators Bitwise Operators Built-In Functions Comparison Functions And Operators Group By Function Limit Syntax Logical Operators Order By Function Set Operations String Comparison Functions and Operators uuid Function Language Structure Literal Values Boolean Literals Numeric Literals String Literals Comment Syntax Identifier Case Sensitivity Keywords and Reserved Words Pipe Syntax Property Reference Schema Object Names Statement Composition User-Defined Variables Statement Syntax Data Definition Statements Alter Tag/Edge Syntax Create Space Syntax Create Tag/Edge Syntax Drop Edge Syntax Drop Space Syntax Drop Tag Syntax Index TTL (time-to-live) Data Query and Manipulation Statements Delete Edge Syntax Delete Vertex Syntax Fetch Syntax Go Syntax Insert Edge Syntax Insert Vertex Syntax Lookup Syntax Return Syntax Update Vertex/Edge Syntax Upsert Syntax Where Syntax Yield Syntax Utility Statements Show Statements Show Charset Syntax Show Collation Syntax Show Configs Syntax Show Create Spaces Syntax Show Create Tag/Edge Syntax Show Hosts Syntax Show Indexes Syntax Show Parts Syntax Show Roles Syntax Show Snapshots Syntax Show Spaces Syntax Show Tag/Edge Syntax Show Users Syntax Describe Syntax Use Syntax Graph Algorithms Find Path Syntax","title":"Query Language (for All Users)"},{"location":"manual-EN/#build_develop_and_administration_for_developers_and_dba","text":"Build Build Source Code Build By Docker Install rpm Installation Start and Stop Services Configuration System Requirement Config Persistency and Priority CONFIG Syntax Metad Configuration Graphd Configuration Storaged Configuration Console Configuration Account Management Statement Alter User Syntax Built-in Roles Change Password Create User Drop User Grant Role Revoke Batch Data Management Data Import Import .csv File Spark Writer Data Export Dump Tool Storage Balance Cluster Snapshot Long Time-Consuming Task Management Monitoring and Statistics Connect Prometheus Metrics Meta Metrics Storage Metrics Graph Metrics Development and API Key Value API Nebula Graph Clients","title":"Build Develop and Administration (for Developers and DBA)"},{"location":"manual-EN/#contributions_for_contributors","text":"Contribute to Documentation Cpp Coding Style Developer Documentation Style Guide How to Contribute","title":"Contributions (for Contributors)"},{"location":"manual-EN/#appendix","text":"Gremlin V.S. nGQL Cypher V.S. nGQL Upgrading Nebula Graph","title":"Appendix"},{"location":"manual-EN/#misc","text":"","title":"Misc"},{"location":"manual-EN/#video","text":"YouTube bilibili","title":"Video"},{"location":"manual-EN/0.about-this-manual/","text":"About This Manual \u00b6 This is the Nebula Graph User Manual. It documents Nebula Graph 1.0. For information about which versions have been released, see Release Notes . Who Shall Read This Manual \u00b6 This manual is written for algorithms engineers , data scientists , software developers , database administrators , and all the people who are interested in the Graph Database areas. If you have questions about using Nebula Graph , join the Nebula Graph Community Slack or our Official Forum . If you have suggestions concerning additions or corrections to the manual itself, please do not hesitate to open an issue on GitHub . Syntax Conventions \u00b6 Nebula Graph is under constant development, and this User Manual is updated frequently as well. This manual uses certain typographical conventions: Fixed width A fixed-width font is used for ngql statements , code examples , system output , and file names . Bold Bold typeface indicates commands , user types , or interface . UPPERCASE REVERSED KEYWORDS and NON REVERSED KEYWORDS in query-language and code examples are almost always shown in upper case. File Formats \u00b6 The manual source files are written in Markdown format. The HTML version is produced by mkdocs .","title":"Introduction"},{"location":"manual-EN/0.about-this-manual/#about_this_manual","text":"This is the Nebula Graph User Manual. It documents Nebula Graph 1.0. For information about which versions have been released, see Release Notes .","title":"About This Manual"},{"location":"manual-EN/0.about-this-manual/#who_shall_read_this_manual","text":"This manual is written for algorithms engineers , data scientists , software developers , database administrators , and all the people who are interested in the Graph Database areas. If you have questions about using Nebula Graph , join the Nebula Graph Community Slack or our Official Forum . If you have suggestions concerning additions or corrections to the manual itself, please do not hesitate to open an issue on GitHub .","title":"Who Shall Read This Manual"},{"location":"manual-EN/0.about-this-manual/#syntax_conventions","text":"Nebula Graph is under constant development, and this User Manual is updated frequently as well. This manual uses certain typographical conventions: Fixed width A fixed-width font is used for ngql statements , code examples , system output , and file names . Bold Bold typeface indicates commands , user types , or interface . UPPERCASE REVERSED KEYWORDS and NON REVERSED KEYWORDS in query-language and code examples are almost always shown in upper case.","title":"Syntax Conventions"},{"location":"manual-EN/0.about-this-manual/#file_formats","text":"The manual source files are written in Markdown format. The HTML version is produced by mkdocs .","title":"File Formats"},{"location":"manual-EN/CHANGELOG/","text":"Manual Changes \u00b6 0.1.9 - 1.0 0.1.8 - 1.0 RC4 0.1.7 - 1.0 RC3 0.1.6 - 1.0 RC2 0.1.5 - 1.0 RC1 0.1.4 - 1.0 beta release 0.1.3 - Add files in manual-CN 0.1.2 - Add files in manual-EN 0.1.1 - Initial release","title":"Manual Changes"},{"location":"manual-EN/CHANGELOG/#manual_changes","text":"0.1.9 - 1.0 0.1.8 - 1.0 RC4 0.1.7 - 1.0 RC3 0.1.6 - 1.0 RC2 0.1.5 - 1.0 RC1 0.1.4 - 1.0 beta release 0.1.3 - Add files in manual-CN 0.1.2 - Add files in manual-EN 0.1.1 - Initial release","title":"Manual Changes"},{"location":"manual-EN/1.overview/","text":"Reader \u00b6 This chapter is for Nebula Graph beginners.","title":"Reader"},{"location":"manual-EN/1.overview/#reader","text":"This chapter is for Nebula Graph beginners.","title":"Reader"},{"location":"manual-EN/1.overview/0.introduction/","text":"Overview of Nebula Graph \u00b6 What is Nebula Graph \u00b6 Nebula Graph is an open source (Apache 2.0 licensed), distributed, scalable, lightning-fast graph database. It is the only solution in the world capable to host graphs with dozens of billions of vertices (nodes) and trillions of edges, while still provides millisecond latency. Nebula Graph's goal is to provide reading, writing, and computing with high concurrency, low latency for super large scale graphs. Nebula Graph is an open source project and we are looking forward to working with the community to popularize and promote the graph database. Main Features of Nebula Graph \u00b6 This section describes some of the important characteristics of Nebula Graph . High performance Nebula Graph provides low latency read and write, while still maintaining high throughput. SQL-like Nebula Graph 's SQL-like query language is easy to understand and powerful enough to meet complex business needs. Secure With role-based access control, Nebula Graph only allows authenticated access. Scalable With shared-nothing distributed architecture, Nebula Graph offers linear scalability. Extensible Nebula Graph supports multiple storage engine types. The query language can be extended to support new algorithms. Transactional With distributed ACID transaction support, Nebula Graph ensures data integrity. Highly available Nebula Graph guarantees high availability even in case of failures.","title":"Overview of Nebula Graph"},{"location":"manual-EN/1.overview/0.introduction/#overview_of_nebula_graph","text":"","title":"Overview of Nebula Graph"},{"location":"manual-EN/1.overview/0.introduction/#what_is_nebula_graph","text":"Nebula Graph is an open source (Apache 2.0 licensed), distributed, scalable, lightning-fast graph database. It is the only solution in the world capable to host graphs with dozens of billions of vertices (nodes) and trillions of edges, while still provides millisecond latency. Nebula Graph's goal is to provide reading, writing, and computing with high concurrency, low latency for super large scale graphs. Nebula Graph is an open source project and we are looking forward to working with the community to popularize and promote the graph database.","title":"What is Nebula Graph"},{"location":"manual-EN/1.overview/0.introduction/#main_features_of_nebula_graph","text":"This section describes some of the important characteristics of Nebula Graph . High performance Nebula Graph provides low latency read and write, while still maintaining high throughput. SQL-like Nebula Graph 's SQL-like query language is easy to understand and powerful enough to meet complex business needs. Secure With role-based access control, Nebula Graph only allows authenticated access. Scalable With shared-nothing distributed architecture, Nebula Graph offers linear scalability. Extensible Nebula Graph supports multiple storage engine types. The query language can be extended to support new algorithms. Transactional With distributed ACID transaction support, Nebula Graph ensures data integrity. Highly available Nebula Graph guarantees high availability even in case of failures.","title":"Main Features of Nebula Graph"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/","text":"Graph Data Modeling \u00b6 This guide is designed to walk you through the graph data modeling of Nebula Graph . Basic concepts of designing a graph data model will be introduced. Graph Space \u00b6 Graph Space is a physically isolated space for different graph. It is similar to database in MySQL. Directed Property Graph \u00b6 The data model handled by the Nebula Graph is a directed property graph , whose edges are directional and there could be properties on both edges and vertices. It can be represented as: G = < V, E, P V , P E >, where V is a set of nodes aka vertices, E is a set of directional edges, P V represents properties on vertices, and P E is the properties on edges. We will use the example graph below to introduce the basic concepts of property graph: In the preceding picture, we have a data set about the players and teams information of NBA. We can see the eleven vertices are classified to two kinds, i.e. player and name while the fifteen edges are classified to serve and like . To better understand the elements of a graph data model, let us walk through each concept of the example graph. Vertices \u00b6 Vertices are typically used to represent entities in the real world. In the preceding example, the graph contains eleven vertices. Tags \u00b6 In Nebula Graph , vertex properties are clustered by tags . In the example above, the vertices have tags player and team . Edge \u00b6 Edges are used to connect vertices. Each edge usually represents a relationship or a behavior between two vertices. In the preceding example, edges are serve and like . Edge Type \u00b6 Each edge is an instance of an edge type. Our example uses serve and like as edge types. Take edge serve for example, in the preceding picture, vertex 101 (represents a player ) is the source vertex and vertex 215 (represents a team ) is the target vertex. We see that vertex 101 has an outgoing edge while vertex 215 has an incoming edge. Properties \u00b6 Properties are named-value pairs within vertices and edges. In our example graph, we have used the properties id , name and age on player , id and name on team , and likeness on like edge. Schema \u00b6 In Nebula Graph , schema refers to the definition of properties (name, type, etc.). Like MySQL , Nebula Graph is a strong typed database. The name and data type of the properties should be determined before the data is written.","title":"Data Model"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#graph_data_modeling","text":"This guide is designed to walk you through the graph data modeling of Nebula Graph . Basic concepts of designing a graph data model will be introduced.","title":"Graph Data Modeling"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#graph_space","text":"Graph Space is a physically isolated space for different graph. It is similar to database in MySQL.","title":"Graph Space"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#directed_property_graph","text":"The data model handled by the Nebula Graph is a directed property graph , whose edges are directional and there could be properties on both edges and vertices. It can be represented as: G = < V, E, P V , P E >, where V is a set of nodes aka vertices, E is a set of directional edges, P V represents properties on vertices, and P E is the properties on edges. We will use the example graph below to introduce the basic concepts of property graph: In the preceding picture, we have a data set about the players and teams information of NBA. We can see the eleven vertices are classified to two kinds, i.e. player and name while the fifteen edges are classified to serve and like . To better understand the elements of a graph data model, let us walk through each concept of the example graph.","title":"Directed Property Graph"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#vertices","text":"Vertices are typically used to represent entities in the real world. In the preceding example, the graph contains eleven vertices.","title":"Vertices"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#tags","text":"In Nebula Graph , vertex properties are clustered by tags . In the example above, the vertices have tags player and team .","title":"Tags"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#edge","text":"Edges are used to connect vertices. Each edge usually represents a relationship or a behavior between two vertices. In the preceding example, edges are serve and like .","title":"Edge"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#edge_type","text":"Each edge is an instance of an edge type. Our example uses serve and like as edge types. Take edge serve for example, in the preceding picture, vertex 101 (represents a player ) is the source vertex and vertex 215 (represents a team ) is the target vertex. We see that vertex 101 has an outgoing edge while vertex 215 has an incoming edge.","title":"Edge Type"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#properties","text":"Properties are named-value pairs within vertices and edges. In our example graph, we have used the properties id , name and age on player , id and name on team , and likeness on like edge.","title":"Properties"},{"location":"manual-EN/1.overview/1.concepts/1.data-model/#schema","text":"In Nebula Graph , schema refers to the definition of properties (name, type, etc.). Like MySQL , Nebula Graph is a strong typed database. The name and data type of the properties should be determined before the data is written.","title":"Schema"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/","text":"Nebula Graph Query Language (nGQL) \u00b6 About nGQL \u00b6 nGQL is a declarative, textual query language like SQL, but for graphs. Unlike SQL, nGQL is all about expressing graph patterns. nGQL is a work in progress. We will add more features and further simplify the existing ones. There might be inconsistency between the syntax specs and implementation for the time being. Goals \u00b6 Easy to learn Easy to understand To focus on the online queries, also to provide the foundation for the offline computation Features \u00b6 Syntax is close to SQL, but not exactly the same (Easy to learn) Expandable Case insensitive Support basic graph traverse Support pattern match Support aggregation Support graph mutation Support distributed transaction (future release) Statement composition, but NO statement embedding (Easy to read) Terminology \u00b6 Graph Space : A physically isolated space for different graph Tag : A label associated with a list of properties Each tag has a name (human readable string), and internally each tag will be assigned a 32-bit integer Each tag associates with a list of properties, each property has a name and a type There could be dependencies between tags. The dependency is a constrain, for instance, if tag S depends on tag T, then tag S cannot exist unless tag T exists Vertex : A Node in the graph Each vertex has a unique 64-bit (signed integer) ID ( VID ) Each vertex can associate with multiple tags Edge : A Link between two vertices Each edge can be uniquely identified by a tuple Edge type (ET) is a human readable string, internally it will be assigned a 32-bit integer. The edge type decides the property list (schema) on the edge Edge rank is an immutable user-assigned 64-bit signed integer. It affects the edge order between two vertices. The edge with a higher rank value comes first. When not specified, the default rank value is zero. Each edge can only be of one type Path : A non-forked connection with multiple vertices and edges between them The length of a path is the number of the edges on the path, which is one less than the number of vertices A path can be represented by a list of vertices, edge types, and rank. An edge is a special path with length==1 <vid, <edge_type, rank>, vid, ...> Language Specification at a Glance \u00b6 For most readers, You can skip this section if you are not familiar with BNF. General \u00b6 The entire set of statements can be categorized into three classes: query , mutation , and administration Every statement can yield a data set as the result. Each data set contains a schema (column name and type) and multiple data rows Composition \u00b6 Statements could be composed in two ways: Statements could be piped together using operator \" | \", much like the pipe in the shell scripts. The result yielded from the previous statement could be redirected to the next statement as input More than one statements can be batched together, separated by \" ; \". The result of the last statement (or a RETURN statement is executed) will be returned as the result of the batch Data Types \u00b6 Simple type: vid , double , int , bool , string , timestamp vid : 64-bit signed integer, representing a vertex ID List of simple types, such as integer[] , double[] , string[] Map : A list of KV pairs. The key must be a string , the value must be the same type for the given map Object (future release??): A list of KV pairs. The key mush be a string , the value can be any simple type Tuple List : This is only used for return values . It's composed by both meta data and data (multiple rows). The meta data includes the column names and their types. Type Conversion \u00b6 A simple typed value can be implicitly converted into a list A list can be implicitly converted into a one-column tuple list \"<type>_list\" can be used as the column name Common BNF \u00b6 ::= vid | integer | double | float | bool | string | path | timestamp | year | month | date | datetime ::= <type> ::= | ::= vid (, vid )* | \"{\" vid (, vid )* \"}\" <label> ::= [:alpha] ([:alnum:] | \"_\")* ::= (\"_\")* <label> ::= <label> ::= (, )* ::= :<type> ::= \":\" ::= ::= <tuple> (, <tuple>)* | \"{\" <tuple> (, <tuple>)* \"}\" <tuple> ::= \"(\" VALUE (, VALUE )* \")\" <var> ::= \"$\" <label> Statements \u00b6 Choose a Graph Space \u00b6 Nebula supports multiple graph spaces. Data in different graph spaces are physically isolated. Before executing a query, a graph space needs to be selected using the following statement USE Return a Data Set \u00b6 Simply return a single value or a data set RETURN ::= vid | | | <var> Create a Tag \u00b6 The following statement defines a new tag CREATE TAG ( ) ::= <label> ::= + ::= ,<type> ::= <label> Create an Edge Type \u00b6 The following statement defines a new edge type CREATE EDGE ( ) := <label> Insert Vertices \u00b6 The following statement inserts one or more vertices INSERT VERTEX [ NO OVERWRITE ] VALUES ::= ( ) (, ( ))* ::= :( ) (, :( ))* ::= vid ::= (, )* ::= VALUE (, VALUE )* Insert Edges \u00b6 The following statement inserts one or more edges INSERT EDGE [ NO OVERWRITE ] [( )] VALUES ( )+ edge_value ::= -> [@ <weight>] : Update a Vertex \u00b6 The following statement updates a vertex UPDATE VERTEX SET \\<update_decl> [ WHERE <conditions>] [ YIELD ] ::= | ::= = <expression> {, = <expression>}+ ::= ( ) = ( ) | ( ) = <var> Update an Edge \u00b6 The following statement updates an edge UPDATE EDGE -> [@<weight>] OF SET [ WHERE <conditions>] [ YIELD ] Traverse the Graph \u00b6 Navigate from given vertices to their neighbors according to the given conditions. It returns either a list of vertex IDs, or a list of tuples GO [ STEPS ] FROM [ OVER [ REVERSELY ] ] [ WHERE ] [ YIELD ] ::= [data_set] [[ AS ] <label>] ::= vid | | | <var> ::= [ AS <label>] ::= {, }* ::= <label> ::= <filter> { AND | OR <filter>}* ::= \\ \\ **>**\\ | \\ **>= | < | <= | == | != <expression> | <expression> IN <value_list> ::= {, }* ::= <expression> [ AS** <label>] WHERE clause only applies to the results that are going to be returned. It will not be applied to the intermediate results (See the detail description of the STEP[S] clause) When STEP[S] clause is skipped, it implies one step When going out for one step from the given vertex, all neighbors will be checked against the WHERE clause, only results satisfied the WHERE clause will be returned When going out for more than one step, WHERE clause will only be applied to the final results. It will not be applied to the intermediate results. Here is an example GO 2 STEPS FROM me OVER friend WHERE birthday > \"1988/1/1\" Obviously, you will probably guess the meaning of the query is to get all my fof (friend of friend) whose birthday is after 1988/1/1. You are absolutely right. We will not apply the filter to my friends (in the first step). Search \u00b6 Following statements looks for vertices or edges that match certain conditions FIND VERTEX WHERE [ YIELD ] FIND EDGE WHERE [ YIELD ] Property Reference \u00b6 It's common to refer a property in the statement, such as in WHERE clause and YIELD clause. In nGQL, the reference to a property is defined as ::= <object> \".\" <object> ::= | | <var> ::= <label> ::= '[' \"]\" <var> always starts with \"$\". There are two special variables: $- and $$. $- refers to the input stream, while $$ refers to the destination objects All property names start with a letter. There are a few system property names starting with \"_\". All properties names starting with \"_\" are reserved. Built-in Properties \u00b6 _id : Vertex id _type : Edge type _src : Source ID of the edge _dst : Destination ID of the edge _rank : Edge rank number","title":"nGQL Overview"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#nebula_graph_query_language_ngql","text":"","title":"Nebula Graph Query Language (nGQL)"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#about_ngql","text":"nGQL is a declarative, textual query language like SQL, but for graphs. Unlike SQL, nGQL is all about expressing graph patterns. nGQL is a work in progress. We will add more features and further simplify the existing ones. There might be inconsistency between the syntax specs and implementation for the time being.","title":"About nGQL"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#goals","text":"Easy to learn Easy to understand To focus on the online queries, also to provide the foundation for the offline computation","title":"Goals"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#features","text":"Syntax is close to SQL, but not exactly the same (Easy to learn) Expandable Case insensitive Support basic graph traverse Support pattern match Support aggregation Support graph mutation Support distributed transaction (future release) Statement composition, but NO statement embedding (Easy to read)","title":"Features"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#terminology","text":"Graph Space : A physically isolated space for different graph Tag : A label associated with a list of properties Each tag has a name (human readable string), and internally each tag will be assigned a 32-bit integer Each tag associates with a list of properties, each property has a name and a type There could be dependencies between tags. The dependency is a constrain, for instance, if tag S depends on tag T, then tag S cannot exist unless tag T exists Vertex : A Node in the graph Each vertex has a unique 64-bit (signed integer) ID ( VID ) Each vertex can associate with multiple tags Edge : A Link between two vertices Each edge can be uniquely identified by a tuple Edge type (ET) is a human readable string, internally it will be assigned a 32-bit integer. The edge type decides the property list (schema) on the edge Edge rank is an immutable user-assigned 64-bit signed integer. It affects the edge order between two vertices. The edge with a higher rank value comes first. When not specified, the default rank value is zero. Each edge can only be of one type Path : A non-forked connection with multiple vertices and edges between them The length of a path is the number of the edges on the path, which is one less than the number of vertices A path can be represented by a list of vertices, edge types, and rank. An edge is a special path with length==1 <vid, <edge_type, rank>, vid, ...>","title":"Terminology"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#language_specification_at_a_glance","text":"For most readers, You can skip this section if you are not familiar with BNF.","title":"Language Specification at a Glance"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#general","text":"The entire set of statements can be categorized into three classes: query , mutation , and administration Every statement can yield a data set as the result. Each data set contains a schema (column name and type) and multiple data rows","title":"General"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#composition","text":"Statements could be composed in two ways: Statements could be piped together using operator \" | \", much like the pipe in the shell scripts. The result yielded from the previous statement could be redirected to the next statement as input More than one statements can be batched together, separated by \" ; \". The result of the last statement (or a RETURN statement is executed) will be returned as the result of the batch","title":"Composition"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#data_types","text":"Simple type: vid , double , int , bool , string , timestamp vid : 64-bit signed integer, representing a vertex ID List of simple types, such as integer[] , double[] , string[] Map : A list of KV pairs. The key must be a string , the value must be the same type for the given map Object (future release??): A list of KV pairs. The key mush be a string , the value can be any simple type Tuple List : This is only used for return values . It's composed by both meta data and data (multiple rows). The meta data includes the column names and their types.","title":"Data Types"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#type_conversion","text":"A simple typed value can be implicitly converted into a list A list can be implicitly converted into a one-column tuple list \"<type>_list\" can be used as the column name","title":"Type Conversion"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#common_bnf","text":"::= vid | integer | double | float | bool | string | path | timestamp | year | month | date | datetime ::= <type> ::= | ::= vid (, vid )* | \"{\" vid (, vid )* \"}\" <label> ::= [:alpha] ([:alnum:] | \"_\")* ::= (\"_\")* <label> ::= <label> ::= (, )* ::= :<type> ::= \":\" ::= ::= <tuple> (, <tuple>)* | \"{\" <tuple> (, <tuple>)* \"}\" <tuple> ::= \"(\" VALUE (, VALUE )* \")\" <var> ::= \"$\" <label>","title":"Common BNF"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#statements","text":"","title":"Statements"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#choose_a_graph_space","text":"Nebula supports multiple graph spaces. Data in different graph spaces are physically isolated. Before executing a query, a graph space needs to be selected using the following statement USE","title":"Choose a Graph Space"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#return_a_data_set","text":"Simply return a single value or a data set RETURN ::= vid | | | <var>","title":"Return a Data Set"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#create_a_tag","text":"The following statement defines a new tag CREATE TAG ( ) ::= <label> ::= + ::= ,<type> ::= <label>","title":"Create a Tag"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#create_an_edge_type","text":"The following statement defines a new edge type CREATE EDGE ( ) := <label>","title":"Create an Edge Type"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#insert_vertices","text":"The following statement inserts one or more vertices INSERT VERTEX [ NO OVERWRITE ] VALUES ::= ( ) (, ( ))* ::= :( ) (, :( ))* ::= vid ::= (, )* ::= VALUE (, VALUE )*","title":"Insert Vertices"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#insert_edges","text":"The following statement inserts one or more edges INSERT EDGE [ NO OVERWRITE ] [( )] VALUES ( )+ edge_value ::= -> [@ <weight>] :","title":"Insert Edges"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#update_a_vertex","text":"The following statement updates a vertex UPDATE VERTEX SET \\<update_decl> [ WHERE <conditions>] [ YIELD ] ::= | ::= = <expression> {, = <expression>}+ ::= ( ) = ( ) | ( ) = <var>","title":"Update a Vertex"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#update_an_edge","text":"The following statement updates an edge UPDATE EDGE -> [@<weight>] OF SET [ WHERE <conditions>] [ YIELD ]","title":"Update an Edge"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#traverse_the_graph","text":"Navigate from given vertices to their neighbors according to the given conditions. It returns either a list of vertex IDs, or a list of tuples GO [ STEPS ] FROM [ OVER [ REVERSELY ] ] [ WHERE ] [ YIELD ] ::= [data_set] [[ AS ] <label>] ::= vid | | | <var> ::= [ AS <label>] ::= {, }* ::= <label> ::= <filter> { AND | OR <filter>}* ::= \\ \\ **>**\\ | \\ **>= | < | <= | == | != <expression> | <expression> IN <value_list> ::= {, }* ::= <expression> [ AS** <label>] WHERE clause only applies to the results that are going to be returned. It will not be applied to the intermediate results (See the detail description of the STEP[S] clause) When STEP[S] clause is skipped, it implies one step When going out for one step from the given vertex, all neighbors will be checked against the WHERE clause, only results satisfied the WHERE clause will be returned When going out for more than one step, WHERE clause will only be applied to the final results. It will not be applied to the intermediate results. Here is an example GO 2 STEPS FROM me OVER friend WHERE birthday > \"1988/1/1\" Obviously, you will probably guess the meaning of the query is to get all my fof (friend of friend) whose birthday is after 1988/1/1. You are absolutely right. We will not apply the filter to my friends (in the first step).","title":"Traverse the Graph"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#search","text":"Following statements looks for vertices or edges that match certain conditions FIND VERTEX WHERE [ YIELD ] FIND EDGE WHERE [ YIELD ]","title":"Search"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#property_reference","text":"It's common to refer a property in the statement, such as in WHERE clause and YIELD clause. In nGQL, the reference to a property is defined as ::= <object> \".\" <object> ::= | | <var> ::= <label> ::= '[' \"]\" <var> always starts with \"$\". There are two special variables: $- and $$. $- refers to the input stream, while $$ refers to the destination objects All property names start with a letter. There are a few system property names starting with \"_\". All properties names starting with \"_\" are reserved.","title":"Property Reference"},{"location":"manual-EN/1.overview/1.concepts/2.nGQL-overview/#built-in_properties","text":"_id : Vertex id _type : Edge type _src : Source ID of the edge _dst : Destination ID of the edge _rank : Edge rank number","title":"Built-in Properties"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/","text":"Quick Start \u00b6 This guide walks you through the process of using Nebula Graph : Install Data Schema CRUD Batch inserting Data Import Tools Installing Nebula Graph \u00b6 It is recommended to install Nebula Graph by docker compose . We have recorded a quick video tutorial on YouTube for your reference. In addition to Docker, you can also install Nebula Graph by installing source code or rpm/deb packages . In case you encounter any problem during the installation, be sure to ask us on our official forum . We have developers on call to answer your questions. Data Schema \u00b6 In this guide, all operations are based on the NBA dataset below: In the above figure, there are two tags ( player , team ) and two edge types ( serve and follow ). Creating and Using a Graph Space \u00b6 A graph space in Nebula Graph is similar to an individual database that you create in traditional databases such as MySQL. First, you need to create a space and use it before can do any other operations. You can create and use a graph space by the following steps: Enter the following statement to create a graph space: nebula> CREATE SPACE nba(partition_num=10, replica_factor=1); Note : partition_num specifies the number of partitions in one replica. It is usually 5 times the number of hard disks in the cluster. replica_factor specifies the number of replicas in the cluster. It is usually 3 in production, 1 in test. Enter the following statement to use the graph space: nebula> USE nba; Now you can check the space you just created by the following statement: nebula> SHOW SPACES; The following information is returned: ======== | Name | ======== | nba | -------- Defining the Schema for Your Data \u00b6 In Nebula Graph , we classify different vertices with similar properties into one group which is named a tag. The CREATE TAG statement defines a tag with a tag name followed by properties and the property types enclosed in parentheses. The CREATE EDGE statement defines an edge type with a type name, followed by properties and the property types enclosed in parentheses. You can create tags and edge types by the following steps: Enter the following statement to create the player tag: nebula> CREATE TAG player(name string, age int); Enter the following statement to create the team tag: nebula> CREATE TAG team(name string); Enter the following statement to create the follow edge type: nebula> CREATE EDGE follow(degree int); Enter the following statement to create the serve edge type: nebula> CREATE EDGE serve(start_year int, end_year int); Now you can check the tags and edge types you just created. To show the tags you just created, enter the following statement: nebula> SHOW TAGS; The following information is returned: ============ | Name | ============ | player | ------------ | team | ------------ To show the edge types you just created, enter the following statement: nebula> SHOW EDGES; The following information is returned: ========== | Name | ========== | serve | ---------- | follow | ---------- To show the properties of the player tag, enter the following statement: nebula> DESCRIBE TAG player; The following information is returned: =================== | Field | Type | =================== | name | string | ------------------- | age | int | ------------------- To show the properties of the follow edge type, enter the following statement: nebula> DESCRIBE EDGE follow; The following information is returned: ===================== | Field | Type | ===================== | degree | int | --------------------- CRUD \u00b6 Inserting Data \u00b6 You can insert vertices and edges based on relations in the illustration figure . Inserting Vertices \u00b6 The INSERT VERTEX statement inserts a vertex by specifying the vertex tag, properties, vertex ID and property values. You can insert some vertices by the following statements: nebula> INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES 101:(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES 102:(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES 200:(\"Warriors\"); nebula> INSERT VERTEX team(name) VALUES 201:(\"Nuggets\"); nebula> INSERT VERTEX player(name, age) VALUES 121:(\"Useless\", 60); Note : In the above vertices inserted, the number after the keyword VALUES is the vertex ID (abbreviated for VID ). The VID must be unique in the space. The last vertex inserted will be deleted in the deleting data section. If you want to insert multiple vertices for the same tag by a single INSERT VERTEX operation, you can enter the following statement: nebula> INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42), \\ 101:(\"Tony Parker\", 36), 102:(\"LaMarcus Aldridge\", 33); Inserting Edges \u00b6 The INSERT EDGE statement inserts an edge by specifying the edge type name, properties, source vertex ID and target vertex ID, and property values. You can insert some edges by the following statements: nebula> INSERT EDGE follow(degree) VALUES 100 -> 101:(95); nebula> INSERT EDGE follow(degree) VALUES 100 -> 102:(90); nebula> INSERT EDGE follow(degree) VALUES 102 -> 101:(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES 100 -> 200:(1997, 2016); nebula> INSERT EDGE serve(start_year, end_year) VALUES 101 -> 201:(1999, 2018); Note : If you want to insert multiple edges for the same edge type by a single INSERT EDGES operation, you can enter the following statement: INSERT EDGE follow(degree) VALUES 100 -> 101:(95),100 -> 102:(90),102 -> 101:(75); Fetching Data \u00b6 After you insert some data in Nebula Graph , you can retrieve any data from your graph space. The FETCH PROP ON statement retrieve data from your graph space. If you want to fetch vertex data, you must specify the vertex tag and vertex ID; if you want to fetch edge data, you must specify the edge type name, source vertex ID and target vertex ID. To fetch the data of the player whose VID is 100 , enter the following statement: nebula> FETCH PROP ON player 100; The following information is returned: ======================================= | VertexID | player.name | player.age | ======================================= | 100 | Tim Duncan | 42 | --------------------------------------- To fetch the data of the serve edge between VID 100 and VID 200 , enter the following statement: nebula> FETCH PROP ON serve 100 -> 200; The following information is returned: ============================================================================= | serve._src | serve._dst | serve._rank | serve.start_year | serve.end_year | ============================================================================= | 100 | 200 | 0 | 1997 | 2016 | ----------------------------------------------------------------------------- Updating Data \u00b6 You can update the vertices and edges you just inserted. Updating Vertices \u00b6 The UPDATE VERTEX statement updates data for your vertex by selecting the vertex that you want to update and then setting the property value with an equal sign to assign it a new value. The following example shows you how to change the name value of VID 100 from Tim Duncan to Tim . Enter the following statement to update the name value: nebula> UPDATE VERTEX 100 SET player.name = \"Tim\"; To check whether the name value is updated, enter the following statement: nebula> FETCH PROP ON player 100; The following information is displayed: ======================================= | VertexID | player.name | player.age | ======================================= | 100 | Tim | 42 | --------------------------------------- Updating Edges \u00b6 The UPDATE EDGE statement updates data for your edge by specifying the source vertex ID and the target vertex ID of the edge and then setting the property value with an equal sign to assign it a new value. The following example shows you how to change the value of the degree property in the follow edge between VID 100 and VID 101 . Now we change the degree property from 95 to 96 . Enter the following statement to update the degree value: nebula> UPDATE EDGE 100 -> 101 OF follow SET degree = 96; To check whether the degree value is updated, enter the following statement: nebula> FETCH PROP ON follow 100 -> 101; The following information is returned: ============================================================ | follow._src | follow._dst | follow._rank | follow.degree | ============================================================ | 100 | 101 | 0 | 96 | ------------------------------------------------------------ Deleting Data \u00b6 If you have some data that you do not need, you can delete it from your graph space. Deleting Vertices \u00b6 You can delete any vertex from your graph space. The DELETE VERTEX statement deletes a vertex by specifying the vertex ID. To delete a vertex whose VID is 121 , enter the following statement: nebula> DELETE VERTEX 121; To check whether the vertex is deleted, enter the following statement; nebula> FETCH PROP ON player 121; The following information is returned: Execution succeeded (Time spent: 1571/1910 us) Note : The above information with an empty return result indicates the query operation is successful but no data is queried from your graph space because the data is deleted. Deleting Edges \u00b6 You can delete any edge from your graph space. The DELETE EDGE statement deletes an edge by specifying the edge type name and the source vertex ID and target vertex ID. To delete a follow edge between VID 100 and VID 200 , enter the following statement: nebula> DELETE EDGE follow 100 -> 200; Note : If you delete a vertex, all the out-going and in-coming edges of this vertex are deleted. Sample Queries \u00b6 This section gives more query examples for your reference. Example 1 . Find the vertices that VID 100 follows. Enter the following statement: nebula> GO FROM 100 OVER follow; The following information is returned: =============== | follow._dst | =============== | 101 | --------------- | 102 | --------------- Example 2 . Find the vertex that VID 100 follows, whose age is greater than 35 . Return his name and age, and set the column names to Teammate and Age respectively. Enter the following statement: nebula> GO FROM 100 OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; The following information is returned: ===================== | Teammate | Age | ===================== | Tony Parker | 36 | --------------------- Note : YIELD specifies what values or results you want to return from the query. $$ represents the target vertex. \\ represents a line break. Example 3 . Find the team which is served by the player who is followed by 100 . There are two ways to get the same result. First, we can use a pipe to retrieve the team. Then we use a temporary variable to retrieve the same team. Enter the following statement with a pipe : GO FROM 100 OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name \\ AS Team, $^.player.name AS Player; The following information is returned. =============================== | Team | Player | =============================== | Nuggets | Tony Parker | ------------------------------- Enter the following statement with a temporary variable : $var=GO FROM 100 OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name \\ AS Team, $^.player.name AS Player; The following information is returned. =============================== | Team | Player | =============================== | Nuggets | Tony Parker | ------------------------------- Note : $^ represents the source vertex. | denotes a pipe. The output of the previous query acts as an input to the next query. $- refers to the input stream. The second approach adopts a user-defined variable $var . The scope of this variable is within the compound statement. Batch Inserting \u00b6 To insert multiple data, you can put all the DDL (Data Definition Language) statements in a .ngql file as follows. CREATE SPACE nba(partition_num=10, replica_factor=1); USE nba; CREATE TAG player(name string, age int); CREATE TAG team(name string); CREATE EDGE follow(degree int); CREATE EDGE serve(start_year int, end_year int); If you install Nebula Graph by compiling the source code, you can batch write to console by the following command: $ cat schema.ngql | ./bin/nebula -u user -p password If you are using Nebula Graph by docker-compose, you can batch write to console by the following command: $ cat nba.ngql | sudo docker run --rm -i --network = host \\ vesoft/nebula-console:nightly --addr = 127 .0.0.1 --port = 3699 Note : You must change the IP address and the port number to yours. You can download the nba.ngql file here . Likewise, you can put hundreds or thousands DML (Data Manipulation Language) statements in a data.ngql file to insert data. Data Import Tools \u00b6 If you have millions of records to insert, it is recommended to use the csv importer . Again, if you come across any problem following the steps in this guide, please head over to our official forum and our on-call developers are more than happy to answer your questions! Finish the steps in this guide and feel Nebula Graph is good? Please star us on GitHub and make our day!","title":"Get Started"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#quick_start","text":"This guide walks you through the process of using Nebula Graph : Install Data Schema CRUD Batch inserting Data Import Tools","title":"Quick Start"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#installing_nebula_graph","text":"It is recommended to install Nebula Graph by docker compose . We have recorded a quick video tutorial on YouTube for your reference. In addition to Docker, you can also install Nebula Graph by installing source code or rpm/deb packages . In case you encounter any problem during the installation, be sure to ask us on our official forum . We have developers on call to answer your questions.","title":"Installing Nebula Graph"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#data_schema","text":"In this guide, all operations are based on the NBA dataset below: In the above figure, there are two tags ( player , team ) and two edge types ( serve and follow ).","title":"Data Schema"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#creating_and_using_a_graph_space","text":"A graph space in Nebula Graph is similar to an individual database that you create in traditional databases such as MySQL. First, you need to create a space and use it before can do any other operations. You can create and use a graph space by the following steps: Enter the following statement to create a graph space: nebula> CREATE SPACE nba(partition_num=10, replica_factor=1); Note : partition_num specifies the number of partitions in one replica. It is usually 5 times the number of hard disks in the cluster. replica_factor specifies the number of replicas in the cluster. It is usually 3 in production, 1 in test. Enter the following statement to use the graph space: nebula> USE nba; Now you can check the space you just created by the following statement: nebula> SHOW SPACES; The following information is returned: ======== | Name | ======== | nba | --------","title":"Creating and Using a Graph Space"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#defining_the_schema_for_your_data","text":"In Nebula Graph , we classify different vertices with similar properties into one group which is named a tag. The CREATE TAG statement defines a tag with a tag name followed by properties and the property types enclosed in parentheses. The CREATE EDGE statement defines an edge type with a type name, followed by properties and the property types enclosed in parentheses. You can create tags and edge types by the following steps: Enter the following statement to create the player tag: nebula> CREATE TAG player(name string, age int); Enter the following statement to create the team tag: nebula> CREATE TAG team(name string); Enter the following statement to create the follow edge type: nebula> CREATE EDGE follow(degree int); Enter the following statement to create the serve edge type: nebula> CREATE EDGE serve(start_year int, end_year int); Now you can check the tags and edge types you just created. To show the tags you just created, enter the following statement: nebula> SHOW TAGS; The following information is returned: ============ | Name | ============ | player | ------------ | team | ------------ To show the edge types you just created, enter the following statement: nebula> SHOW EDGES; The following information is returned: ========== | Name | ========== | serve | ---------- | follow | ---------- To show the properties of the player tag, enter the following statement: nebula> DESCRIBE TAG player; The following information is returned: =================== | Field | Type | =================== | name | string | ------------------- | age | int | ------------------- To show the properties of the follow edge type, enter the following statement: nebula> DESCRIBE EDGE follow; The following information is returned: ===================== | Field | Type | ===================== | degree | int | ---------------------","title":"Defining the Schema for Your Data"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#crud","text":"","title":"CRUD"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#inserting_data","text":"You can insert vertices and edges based on relations in the illustration figure .","title":"Inserting Data"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#inserting_vertices","text":"The INSERT VERTEX statement inserts a vertex by specifying the vertex tag, properties, vertex ID and property values. You can insert some vertices by the following statements: nebula> INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42); nebula> INSERT VERTEX player(name, age) VALUES 101:(\"Tony Parker\", 36); nebula> INSERT VERTEX player(name, age) VALUES 102:(\"LaMarcus Aldridge\", 33); nebula> INSERT VERTEX team(name) VALUES 200:(\"Warriors\"); nebula> INSERT VERTEX team(name) VALUES 201:(\"Nuggets\"); nebula> INSERT VERTEX player(name, age) VALUES 121:(\"Useless\", 60); Note : In the above vertices inserted, the number after the keyword VALUES is the vertex ID (abbreviated for VID ). The VID must be unique in the space. The last vertex inserted will be deleted in the deleting data section. If you want to insert multiple vertices for the same tag by a single INSERT VERTEX operation, you can enter the following statement: nebula> INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42), \\ 101:(\"Tony Parker\", 36), 102:(\"LaMarcus Aldridge\", 33);","title":"Inserting Vertices"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#inserting_edges","text":"The INSERT EDGE statement inserts an edge by specifying the edge type name, properties, source vertex ID and target vertex ID, and property values. You can insert some edges by the following statements: nebula> INSERT EDGE follow(degree) VALUES 100 -> 101:(95); nebula> INSERT EDGE follow(degree) VALUES 100 -> 102:(90); nebula> INSERT EDGE follow(degree) VALUES 102 -> 101:(75); nebula> INSERT EDGE serve(start_year, end_year) VALUES 100 -> 200:(1997, 2016); nebula> INSERT EDGE serve(start_year, end_year) VALUES 101 -> 201:(1999, 2018); Note : If you want to insert multiple edges for the same edge type by a single INSERT EDGES operation, you can enter the following statement: INSERT EDGE follow(degree) VALUES 100 -> 101:(95),100 -> 102:(90),102 -> 101:(75);","title":"Inserting Edges"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#fetching_data","text":"After you insert some data in Nebula Graph , you can retrieve any data from your graph space. The FETCH PROP ON statement retrieve data from your graph space. If you want to fetch vertex data, you must specify the vertex tag and vertex ID; if you want to fetch edge data, you must specify the edge type name, source vertex ID and target vertex ID. To fetch the data of the player whose VID is 100 , enter the following statement: nebula> FETCH PROP ON player 100; The following information is returned: ======================================= | VertexID | player.name | player.age | ======================================= | 100 | Tim Duncan | 42 | --------------------------------------- To fetch the data of the serve edge between VID 100 and VID 200 , enter the following statement: nebula> FETCH PROP ON serve 100 -> 200; The following information is returned: ============================================================================= | serve._src | serve._dst | serve._rank | serve.start_year | serve.end_year | ============================================================================= | 100 | 200 | 0 | 1997 | 2016 | -----------------------------------------------------------------------------","title":"Fetching Data"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#updating_data","text":"You can update the vertices and edges you just inserted.","title":"Updating Data"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#updating_vertices","text":"The UPDATE VERTEX statement updates data for your vertex by selecting the vertex that you want to update and then setting the property value with an equal sign to assign it a new value. The following example shows you how to change the name value of VID 100 from Tim Duncan to Tim . Enter the following statement to update the name value: nebula> UPDATE VERTEX 100 SET player.name = \"Tim\"; To check whether the name value is updated, enter the following statement: nebula> FETCH PROP ON player 100; The following information is displayed: ======================================= | VertexID | player.name | player.age | ======================================= | 100 | Tim | 42 | ---------------------------------------","title":"Updating Vertices"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#updating_edges","text":"The UPDATE EDGE statement updates data for your edge by specifying the source vertex ID and the target vertex ID of the edge and then setting the property value with an equal sign to assign it a new value. The following example shows you how to change the value of the degree property in the follow edge between VID 100 and VID 101 . Now we change the degree property from 95 to 96 . Enter the following statement to update the degree value: nebula> UPDATE EDGE 100 -> 101 OF follow SET degree = 96; To check whether the degree value is updated, enter the following statement: nebula> FETCH PROP ON follow 100 -> 101; The following information is returned: ============================================================ | follow._src | follow._dst | follow._rank | follow.degree | ============================================================ | 100 | 101 | 0 | 96 | ------------------------------------------------------------","title":"Updating Edges"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#deleting_data","text":"If you have some data that you do not need, you can delete it from your graph space.","title":"Deleting Data"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#deleting_vertices","text":"You can delete any vertex from your graph space. The DELETE VERTEX statement deletes a vertex by specifying the vertex ID. To delete a vertex whose VID is 121 , enter the following statement: nebula> DELETE VERTEX 121; To check whether the vertex is deleted, enter the following statement; nebula> FETCH PROP ON player 121; The following information is returned: Execution succeeded (Time spent: 1571/1910 us) Note : The above information with an empty return result indicates the query operation is successful but no data is queried from your graph space because the data is deleted.","title":"Deleting Vertices"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#deleting_edges","text":"You can delete any edge from your graph space. The DELETE EDGE statement deletes an edge by specifying the edge type name and the source vertex ID and target vertex ID. To delete a follow edge between VID 100 and VID 200 , enter the following statement: nebula> DELETE EDGE follow 100 -> 200; Note : If you delete a vertex, all the out-going and in-coming edges of this vertex are deleted.","title":"Deleting Edges"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#sample_queries","text":"This section gives more query examples for your reference. Example 1 . Find the vertices that VID 100 follows. Enter the following statement: nebula> GO FROM 100 OVER follow; The following information is returned: =============== | follow._dst | =============== | 101 | --------------- | 102 | --------------- Example 2 . Find the vertex that VID 100 follows, whose age is greater than 35 . Return his name and age, and set the column names to Teammate and Age respectively. Enter the following statement: nebula> GO FROM 100 OVER follow WHERE $$.player.age >= 35 \\ YIELD $$.player.name AS Teammate, $$.player.age AS Age; The following information is returned: ===================== | Teammate | Age | ===================== | Tony Parker | 36 | --------------------- Note : YIELD specifies what values or results you want to return from the query. $$ represents the target vertex. \\ represents a line break. Example 3 . Find the team which is served by the player who is followed by 100 . There are two ways to get the same result. First, we can use a pipe to retrieve the team. Then we use a temporary variable to retrieve the same team. Enter the following statement with a pipe : GO FROM 100 OVER follow YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve YIELD $$.team.name \\ AS Team, $^.player.name AS Player; The following information is returned. =============================== | Team | Player | =============================== | Nuggets | Tony Parker | ------------------------------- Enter the following statement with a temporary variable : $var=GO FROM 100 OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve YIELD $$.team.name \\ AS Team, $^.player.name AS Player; The following information is returned. =============================== | Team | Player | =============================== | Nuggets | Tony Parker | ------------------------------- Note : $^ represents the source vertex. | denotes a pipe. The output of the previous query acts as an input to the next query. $- refers to the input stream. The second approach adopts a user-defined variable $var . The scope of this variable is within the compound statement.","title":"Sample Queries"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#batch_inserting","text":"To insert multiple data, you can put all the DDL (Data Definition Language) statements in a .ngql file as follows. CREATE SPACE nba(partition_num=10, replica_factor=1); USE nba; CREATE TAG player(name string, age int); CREATE TAG team(name string); CREATE EDGE follow(degree int); CREATE EDGE serve(start_year int, end_year int); If you install Nebula Graph by compiling the source code, you can batch write to console by the following command: $ cat schema.ngql | ./bin/nebula -u user -p password If you are using Nebula Graph by docker-compose, you can batch write to console by the following command: $ cat nba.ngql | sudo docker run --rm -i --network = host \\ vesoft/nebula-console:nightly --addr = 127 .0.0.1 --port = 3699 Note : You must change the IP address and the port number to yours. You can download the nba.ngql file here . Likewise, you can put hundreds or thousands DML (Data Manipulation Language) statements in a data.ngql file to insert data.","title":"Batch Inserting"},{"location":"manual-EN/1.overview/2.quick-start/1.get-started/#data_import_tools","text":"If you have millions of records to insert, it is recommended to use the csv importer . Again, if you come across any problem following the steps in this guide, please head over to our official forum and our on-call developers are more than happy to answer your questions! Finish the steps in this guide and feel Nebula Graph is good? Please star us on GitHub and make our day!","title":"Data Import Tools"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/","text":"Frequently Asked Questions \u00b6 Common questions about Nebula Graph and more. If you do not find the information you need in this documentation, please try searching the Users tab in the Nebula Graph official forum . Frequently Asked Questions Trouble Shooting Server Parameter Configuration How to Check Configs Unbalanced Partitions Log and Changing Log Levels Using Multiple Hard Disks Process Crash Errors Thrown When Executing Command in Docker Adding Two Clusters on a Single Host Connection Refused Could not create logging file:... Too many open files How to Check Nebula Graph Version Modifying the Configuration File Does not Take Effect Modify RocksDB block cache Nebula fails on CentOS 6.5 General Information Explanations on the Time Return in Queries Trouble Shooting \u00b6 Trouble Shooting session lists the common operation errors in Nebula Graph . Server Parameter Configuration \u00b6 In Nebula console, run nebula> SHOW CONFIGS; For configuration details, please see here . [\u2191] Back to top How to Check Configs \u00b6 Configuration files are stored under /usr/local/nebula/etc/ by default. [\u2191] Back to top Unbalanced Partitions \u00b6 See Storage Balance . Log and Changing Log Levels \u00b6 Logs are stored under /usr/local/nebula/logs/ by default. See graphd Logs and storaged Logs . Using Multiple Hard Disks \u00b6 Modify /usr/local/nebula/etc/nebula-storage.conf . For example --data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/ When multiple hard disks are used, multiple directories can be separated by commas, and each directory corresponds to a RocksDB instance for better concurrency. See here for details. Process Crash \u00b6 Check disk space df -h . Check memory usage free -h . [\u2191] Back to top Errors Thrown When Executing Command in Docker \u00b6 This is likely caused by the inconsistency between the docker IP and the default listening address (172.17.0.2). Thus we need to change the the latter. First run ifconfig in container to check your container IP, here we assume your IP is 172.17.0.3. In directory /usr/local/nebula/etc , check the config locations of all the IP addresses with the command grep \"172.17.0.2\" . -r . Change all the IPs you find in step 2 to your container IP 172.17.0.3. Restart all the services. [\u2191] Back to top Adding Two Clusters on a Single Host \u00b6 When the same host is used for single host or cluster test, the storaged service cannot start normally. The listening port of the storaged service is red in the console. Check the logs (/usr/local/nebula/nebula-storaged.ERROR) of the storaged service. If you find the \"wrong cluster\" error message, the possible cause is that the cluster id generated by Nebula Graph during the single host test and the cluster test are inconsistent. You need to delete the cluster.id file under the installation directory (/usr/local/nebula) and the data directory and restart the service. [\u2191] Back to top Connection Refused \u00b6 E1121 04:49:34.563858 256 GraphClient.cpp:54] Thrift rpc call failed: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused Check service status by $ /usr/local/nebula/scripts/nebula.service status all [\u2191] Back to top Could not create logging file:... Too many open files \u00b6 Check your disk space df -h Check log directory /usr/local/nebula/logs/ reset your max open files by ulimit -n 65536 [\u2191] Back to top How to Check Nebula Graph Version \u00b6 Use the command curl http://ip:port/status to obtain the git_info_sha, the commitID of the binary package. Modifying the Configuration File Does not Take Effect \u00b6 Nebula Graph uses the following two methods obtaining configurations: From the configuration files (You need to modify the files then restart the services); From the Meta. Set via CLI and persists in Meta service. Please refer to the Configs Syntax for details. Modifying the configuration file does not take effect because Nebula Graph gets configuration in the second method (from meta) by default. If you want to use the first way, please add the --local_config=true option in flag files metad.conf , storaged.conf , graphd.conf (flag files directory is /home/user/nebula/build/install/etc ) respectively. [\u2191] Back to top Modify RocksDB block cache \u00b6 Modify the storage layer's configuration file storaged.conf (the default directory is /usr/local/nebula/etc/ , yours maybe different) and restart the service. For example: # Change rocksdb_block_cache to 1024 MB --rocksdb_block_cache = 1024 # Stop storaged and restart /usr/local/nebula/scripts/nebula.service stop storaged /usr/local/nebula/scripts/nebula.service start storaged Details see here . [\u2191] Back to top Nebula fails on CentOS 6.5 \u00b6 Nebula Graph fails on CentOS 6.5, the error message is as follows: # storage log Heartbeat failed, status:RPC failure in MetaClient: N6apache6thrift9transport19TTransportExceptionE: AsyncSocketException: connect failed, type = Socket not open, errno = 111 ( Connection refused ) : Connection refused # meta log Log line format: [ IWEF ] mmdd hh:mm:ss.uuuuuu threadid file:line ] msg E0415 22 :32:38.944437 15532 AsyncServerSocket.cpp:762 ] failed to set SO_REUSEPORT on async server socket Protocol not available E0415 22 :32:38.945001 15510 ThriftServer.cpp:440 ] Got an exception while setting up the server: 92failed to bind to async server socket: [ :: ] :0: Protocol not available E0415 22 :32:38.945057 15510 RaftexService.cpp:90 ] Setup the Raftex Service failed, error: 92failed to bind to async server socket: [ :: ] :0: Protocol not available E0415 22 :32:38.949586 15463 NebulaStore.cpp:47 ] Start the raft service failed E0415 22 :32:38.949597 15463 MetaDaemon.cpp:88 ] Nebula store init failed E0415 22 :32:38.949796 15463 MetaDaemon.cpp:215 ] Init kv failed! Nebula service status is as follows: [ root@redhat6 scripts ] # ./nebula.service status all [ WARN ] The maximum files allowed to open might be too few: 1024 [ INFO ] nebula-metad: Exited [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 15547 , Listening on 44500 Reason for error: CentOS 6.5 system kernel version is 2.6.32, which is less than 3.9. However, SO_REUSEPORT only supports Linux 3.9 and above. Upgrading the system to CentOS 7.5 can solve the problem by itself. [\u2191] Back to top General Information \u00b6 General Information lists the conceptual questions about Nebula Graph . Explanations on the Time Return in Queries \u00b6 nebula> GO FROM 101 OVER follow =============== | follow._dst | =============== | 100 | --------------- | 102 | --------------- | 125 | --------------- Got 3 rows (Time spent: 7431/10406 us) Taking the above query as an example, the number 7431 in Time spent is the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the console, fetch the data from the storage and perform a series of calculation ; the number 10406 is the time spent from the client's perspective, that is, the time it takes for the console from sending a request and receiving a response to displaying the result on the screen. [\u2191] Back to top","title":"FAQ"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#frequently_asked_questions","text":"Common questions about Nebula Graph and more. If you do not find the information you need in this documentation, please try searching the Users tab in the Nebula Graph official forum . Frequently Asked Questions Trouble Shooting Server Parameter Configuration How to Check Configs Unbalanced Partitions Log and Changing Log Levels Using Multiple Hard Disks Process Crash Errors Thrown When Executing Command in Docker Adding Two Clusters on a Single Host Connection Refused Could not create logging file:... Too many open files How to Check Nebula Graph Version Modifying the Configuration File Does not Take Effect Modify RocksDB block cache Nebula fails on CentOS 6.5 General Information Explanations on the Time Return in Queries","title":"Frequently Asked Questions"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#trouble_shooting","text":"Trouble Shooting session lists the common operation errors in Nebula Graph .","title":"Trouble Shooting"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#server_parameter_configuration","text":"In Nebula console, run nebula> SHOW CONFIGS; For configuration details, please see here . [\u2191] Back to top","title":"Server Parameter Configuration"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#how_to_check_configs","text":"Configuration files are stored under /usr/local/nebula/etc/ by default. [\u2191] Back to top","title":"How to Check Configs"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#unbalanced_partitions","text":"See Storage Balance .","title":"Unbalanced Partitions"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#log_and_changing_log_levels","text":"Logs are stored under /usr/local/nebula/logs/ by default. See graphd Logs and storaged Logs .","title":"Log and Changing Log Levels"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#using_multiple_hard_disks","text":"Modify /usr/local/nebula/etc/nebula-storage.conf . For example --data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/ When multiple hard disks are used, multiple directories can be separated by commas, and each directory corresponds to a RocksDB instance for better concurrency. See here for details.","title":"Using Multiple Hard Disks"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#process_crash","text":"Check disk space df -h . Check memory usage free -h . [\u2191] Back to top","title":"Process Crash"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#errors_thrown_when_executing_command_in_docker","text":"This is likely caused by the inconsistency between the docker IP and the default listening address (172.17.0.2). Thus we need to change the the latter. First run ifconfig in container to check your container IP, here we assume your IP is 172.17.0.3. In directory /usr/local/nebula/etc , check the config locations of all the IP addresses with the command grep \"172.17.0.2\" . -r . Change all the IPs you find in step 2 to your container IP 172.17.0.3. Restart all the services. [\u2191] Back to top","title":"Errors Thrown When Executing Command in Docker"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#adding_two_clusters_on_a_single_host","text":"When the same host is used for single host or cluster test, the storaged service cannot start normally. The listening port of the storaged service is red in the console. Check the logs (/usr/local/nebula/nebula-storaged.ERROR) of the storaged service. If you find the \"wrong cluster\" error message, the possible cause is that the cluster id generated by Nebula Graph during the single host test and the cluster test are inconsistent. You need to delete the cluster.id file under the installation directory (/usr/local/nebula) and the data directory and restart the service. [\u2191] Back to top","title":"Adding Two Clusters on a Single Host"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#connection_refused","text":"E1121 04:49:34.563858 256 GraphClient.cpp:54] Thrift rpc call failed: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused Check service status by $ /usr/local/nebula/scripts/nebula.service status all [\u2191] Back to top","title":"Connection Refused"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#could_not_create_logging_file_too_many_open_files","text":"Check your disk space df -h Check log directory /usr/local/nebula/logs/ reset your max open files by ulimit -n 65536 [\u2191] Back to top","title":"Could not create logging file:... Too many open files"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#how_to_check_nebula_graph_version","text":"Use the command curl http://ip:port/status to obtain the git_info_sha, the commitID of the binary package.","title":"How to Check Nebula Graph Version"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#modifying_the_configuration_file_does_not_take_effect","text":"Nebula Graph uses the following two methods obtaining configurations: From the configuration files (You need to modify the files then restart the services); From the Meta. Set via CLI and persists in Meta service. Please refer to the Configs Syntax for details. Modifying the configuration file does not take effect because Nebula Graph gets configuration in the second method (from meta) by default. If you want to use the first way, please add the --local_config=true option in flag files metad.conf , storaged.conf , graphd.conf (flag files directory is /home/user/nebula/build/install/etc ) respectively. [\u2191] Back to top","title":"Modifying the Configuration File Does not Take Effect"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#modify_rocksdb_block_cache","text":"Modify the storage layer's configuration file storaged.conf (the default directory is /usr/local/nebula/etc/ , yours maybe different) and restart the service. For example: # Change rocksdb_block_cache to 1024 MB --rocksdb_block_cache = 1024 # Stop storaged and restart /usr/local/nebula/scripts/nebula.service stop storaged /usr/local/nebula/scripts/nebula.service start storaged Details see here . [\u2191] Back to top","title":"Modify RocksDB block cache"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#nebula_fails_on_centos_65","text":"Nebula Graph fails on CentOS 6.5, the error message is as follows: # storage log Heartbeat failed, status:RPC failure in MetaClient: N6apache6thrift9transport19TTransportExceptionE: AsyncSocketException: connect failed, type = Socket not open, errno = 111 ( Connection refused ) : Connection refused # meta log Log line format: [ IWEF ] mmdd hh:mm:ss.uuuuuu threadid file:line ] msg E0415 22 :32:38.944437 15532 AsyncServerSocket.cpp:762 ] failed to set SO_REUSEPORT on async server socket Protocol not available E0415 22 :32:38.945001 15510 ThriftServer.cpp:440 ] Got an exception while setting up the server: 92failed to bind to async server socket: [ :: ] :0: Protocol not available E0415 22 :32:38.945057 15510 RaftexService.cpp:90 ] Setup the Raftex Service failed, error: 92failed to bind to async server socket: [ :: ] :0: Protocol not available E0415 22 :32:38.949586 15463 NebulaStore.cpp:47 ] Start the raft service failed E0415 22 :32:38.949597 15463 MetaDaemon.cpp:88 ] Nebula store init failed E0415 22 :32:38.949796 15463 MetaDaemon.cpp:215 ] Init kv failed! Nebula service status is as follows: [ root@redhat6 scripts ] # ./nebula.service status all [ WARN ] The maximum files allowed to open might be too few: 1024 [ INFO ] nebula-metad: Exited [ INFO ] nebula-graphd: Exited [ INFO ] nebula-storaged: Running as 15547 , Listening on 44500 Reason for error: CentOS 6.5 system kernel version is 2.6.32, which is less than 3.9. However, SO_REUSEPORT only supports Linux 3.9 and above. Upgrading the system to CentOS 7.5 can solve the problem by itself. [\u2191] Back to top","title":"Nebula fails on CentOS 6.5"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#general_information","text":"General Information lists the conceptual questions about Nebula Graph .","title":"General Information"},{"location":"manual-EN/1.overview/2.quick-start/2.FAQ/#explanations_on_the_time_return_in_queries","text":"nebula> GO FROM 101 OVER follow =============== | follow._dst | =============== | 100 | --------------- | 102 | --------------- | 125 | --------------- Got 3 rows (Time spent: 7431/10406 us) Taking the above query as an example, the number 7431 in Time spent is the time spent by the database itself, that is, the time it takes for the query engine to receive a query from the console, fetch the data from the storage and perform a series of calculation ; the number 10406 is the time spent from the client's perspective, that is, the time it takes for the console from sending a request and receiving a response to displaying the result on the screen. [\u2191] Back to top","title":"Explanations on the Time Return in Queries"},{"location":"manual-EN/1.overview/2.quick-start/3.supported-clients/","text":"Supported Clients by Nebula Graph \u00b6 Currently, Nebula Graph supports the following clients: Go Client Python Client Java Client","title":"Nebula Graph Clients"},{"location":"manual-EN/1.overview/2.quick-start/3.supported-clients/#supported_clients_by_nebula_graph","text":"Currently, Nebula Graph supports the following clients: Go Client Python Client Java Client","title":"Supported Clients by Nebula Graph"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/","text":"Example of Importing CSV Files \u00b6 The following example shows you how to import CSV data to Nebula Graph with Nebula Importer . In this example, Nebula Graph is installed with Docker and Docker Compose . We will walk you through the example by the following steps: Example of Importing CSV Files Starting Nebula Graph Services Creating the Schema for Vertices and Edges Preparing Your Configuration File Preparing the CSV Data Importing the CSV Data Importing the CSV Data With Go-importer Importing the CSV Data With Docker Starting Nebula Graph Services \u00b6 You can start your Nebula Graph services by the following steps: On a command line interface, go to the nebula-docker-compose directory. Execute the following command to start Nebula Graph services: $ sudo docker-compose up -d Execute the following command to pull the Nebula Graph image: $ sudo docker pull vesoft/nebula-console:nightly Execute the following command to connect to your Nebula Graph server: $ sudo docker run --rm -ti --network = host vesoft/nebula-console:nightly --addr = 127 .0.0.1 --port = 3699 Note : You must ensure your IP address and port number are configured correctly. Creating the Schema for Vertices and Edges \u00b6 Before you can input your schema, you must create a space and use it. In this example we create a nba space and use it. We create two tags and two edge types with the following commands: nebula> CREATE TAG player (name string, age int); nebula> CREATE TAG team (name string); nebula> CREATE EDGE serve (start_year int, end_year int); nebula> CREATE EDGE follow (degree, int); Preparing Your Configuration File \u00b6 You must configure the .yaml configuration file, which regulates how data is organized in the CSV files. In this example, we create a config.yaml file. In this example, we configure the config.yaml configuration file as follows: version: v1rc1 description: example clientSettings: concurrency: 2 # number of graph clients channelBufferSize: 50 space: nba connection: user: user password: password address: 127.0.0.1:3699 logPath: ./err/test.log files: - path: /home/nebula/serve.csv failDataPath: ./err/serve.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: edge edge: name: serve withRanking: false props: - name: start_year type: int - name: end_year type: int - path: /home/nebula/follow.csv failDataPath: ./err/follow.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: edge edge: name: follow withRanking: false props: - name: degree type: int - path: /home/nebula/player.csv failDataPath: ./err/player.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: vertex vertex: tags: - name: player props: - name: name type: string - name: age type: int - path: /home/nebula/team.csv failDataPath: ./err/team.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: vertex vertex: tags: - name: team props: - name: name type: string Note : In the above configuration file, you must change the IP address and the port number to yours. You must change the directory of the CSV files to yours, otherwise, Nebula Importer cannot find the CSV files. Preparing the CSV Data \u00b6 In this example, we prepare four CSV data files: player.csv , team.csv , serve.csv , and follow.csv . The data in the serve.csv file is as follows: 100,200,1997,2016 101,201,1999,2018 102,203,2006,2015 102,204,2015,2019 103,204,2017,2019 104,200,2007,2009 The data in the follow.csv file is as follows: 100,101,95 100,102,90 101,100,95 102,101,75 102,100,75 103,102,70 104,101,50 104,105,60 105,104,83 The data in the player.csv file is as follows: 100,Tim Duncan,42 101,Tony Parker,36 102,LaMarcus Aldridge,33 103,Rudy Gay,32 104,Marco Belinelli,32 105,Danny Green,31 106,Kyle Anderson,25 107,Aron Baynes,32 108,Boris Diaw,36 The data in the team.csv file is as follows: 200,Warriors 201,Nuggets 202,Rockets 203,Trail 204,Spurs 205,Thunders 206,Jazz 207,Clippers 208,Kings Note : In the serve and follow CSV files, the first column is the source vertex ID, the second column is the destination vertex ID, and the other columns are consistent with the config.yaml file. In the player and team CSV files, the first column is the vertex ID and the other columns are consistent with the config.yaml file. Importing the CSV Data \u00b6 After all the previous four steps are complete, you can import the CSV data with Docker or Go . Importing the CSV Data With Go-importer \u00b6 Before you import CSV data with Go-importer , you must ensure Go is installed and the environment variable for Go is configured. You can import the CSV data by the following steps: Change your current directory to the directory where the import.go file is located by the following command: $ cd /home/nebula/nebula-importer/cmd Execute the following command to import the CSV data: $ go run importer.go --config /home/nebula/config.yaml Note : You must change the directory for the import.go file and the directory for the config.yaml file to yours, otherwise, the importing operation might fail. Importing the CSV Data With Docker \u00b6 Before you import the CSV data with Docker , you must ensure that Docker is up and running. You can import the CSV data with Docker by the following command: $ sudo docker run --rm -ti --network = host \\ -v /home/nebula/config.yaml:/home/nebula/config.yaml \\ -v /home/nebula/:/home/nebula/ vesoft/nebula-importer \\ --config /home/nebula/config.yaml Note : You must change the directory for the config.yaml file to yours, otherwise the importing operation might fail.","title":"Example of Importing CSV Files"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#example_of_importing_csv_files","text":"The following example shows you how to import CSV data to Nebula Graph with Nebula Importer . In this example, Nebula Graph is installed with Docker and Docker Compose . We will walk you through the example by the following steps: Example of Importing CSV Files Starting Nebula Graph Services Creating the Schema for Vertices and Edges Preparing Your Configuration File Preparing the CSV Data Importing the CSV Data Importing the CSV Data With Go-importer Importing the CSV Data With Docker","title":"Example of Importing CSV Files"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#starting_nebula_graph_services","text":"You can start your Nebula Graph services by the following steps: On a command line interface, go to the nebula-docker-compose directory. Execute the following command to start Nebula Graph services: $ sudo docker-compose up -d Execute the following command to pull the Nebula Graph image: $ sudo docker pull vesoft/nebula-console:nightly Execute the following command to connect to your Nebula Graph server: $ sudo docker run --rm -ti --network = host vesoft/nebula-console:nightly --addr = 127 .0.0.1 --port = 3699 Note : You must ensure your IP address and port number are configured correctly.","title":"Starting Nebula Graph Services"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#creating_the_schema_for_vertices_and_edges","text":"Before you can input your schema, you must create a space and use it. In this example we create a nba space and use it. We create two tags and two edge types with the following commands: nebula> CREATE TAG player (name string, age int); nebula> CREATE TAG team (name string); nebula> CREATE EDGE serve (start_year int, end_year int); nebula> CREATE EDGE follow (degree, int);","title":"Creating the Schema for Vertices and Edges"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#preparing_your_configuration_file","text":"You must configure the .yaml configuration file, which regulates how data is organized in the CSV files. In this example, we create a config.yaml file. In this example, we configure the config.yaml configuration file as follows: version: v1rc1 description: example clientSettings: concurrency: 2 # number of graph clients channelBufferSize: 50 space: nba connection: user: user password: password address: 127.0.0.1:3699 logPath: ./err/test.log files: - path: /home/nebula/serve.csv failDataPath: ./err/serve.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: edge edge: name: serve withRanking: false props: - name: start_year type: int - name: end_year type: int - path: /home/nebula/follow.csv failDataPath: ./err/follow.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: edge edge: name: follow withRanking: false props: - name: degree type: int - path: /home/nebula/player.csv failDataPath: ./err/player.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: vertex vertex: tags: - name: player props: - name: name type: string - name: age type: int - path: /home/nebula/team.csv failDataPath: ./err/team.csv batchSize: 10 type: csv csv: withHeader: false withLabel: false schema: type: vertex vertex: tags: - name: team props: - name: name type: string Note : In the above configuration file, you must change the IP address and the port number to yours. You must change the directory of the CSV files to yours, otherwise, Nebula Importer cannot find the CSV files.","title":"Preparing Your Configuration File"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#preparing_the_csv_data","text":"In this example, we prepare four CSV data files: player.csv , team.csv , serve.csv , and follow.csv . The data in the serve.csv file is as follows: 100,200,1997,2016 101,201,1999,2018 102,203,2006,2015 102,204,2015,2019 103,204,2017,2019 104,200,2007,2009 The data in the follow.csv file is as follows: 100,101,95 100,102,90 101,100,95 102,101,75 102,100,75 103,102,70 104,101,50 104,105,60 105,104,83 The data in the player.csv file is as follows: 100,Tim Duncan,42 101,Tony Parker,36 102,LaMarcus Aldridge,33 103,Rudy Gay,32 104,Marco Belinelli,32 105,Danny Green,31 106,Kyle Anderson,25 107,Aron Baynes,32 108,Boris Diaw,36 The data in the team.csv file is as follows: 200,Warriors 201,Nuggets 202,Rockets 203,Trail 204,Spurs 205,Thunders 206,Jazz 207,Clippers 208,Kings Note : In the serve and follow CSV files, the first column is the source vertex ID, the second column is the destination vertex ID, and the other columns are consistent with the config.yaml file. In the player and team CSV files, the first column is the vertex ID and the other columns are consistent with the config.yaml file.","title":"Preparing the CSV Data"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#importing_the_csv_data","text":"After all the previous four steps are complete, you can import the CSV data with Docker or Go .","title":"Importing the CSV Data"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#importing_the_csv_data_with_go-importer","text":"Before you import CSV data with Go-importer , you must ensure Go is installed and the environment variable for Go is configured. You can import the CSV data by the following steps: Change your current directory to the directory where the import.go file is located by the following command: $ cd /home/nebula/nebula-importer/cmd Execute the following command to import the CSV data: $ go run importer.go --config /home/nebula/config.yaml Note : You must change the directory for the import.go file and the directory for the config.yaml file to yours, otherwise, the importing operation might fail.","title":"Importing the CSV Data With Go-importer"},{"location":"manual-EN/1.overview/2.quick-start/4.import-csv-file/#importing_the_csv_data_with_docker","text":"Before you import the CSV data with Docker , you must ensure that Docker is up and running. You can import the CSV data with Docker by the following command: $ sudo docker run --rm -ti --network = host \\ -v /home/nebula/config.yaml:/home/nebula/config.yaml \\ -v /home/nebula/:/home/nebula/ vesoft/nebula-importer \\ --config /home/nebula/config.yaml Note : You must change the directory for the config.yaml file to yours, otherwise the importing operation might fail.","title":"Importing the CSV Data With Docker"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/","text":"Design and Architecture of Nebula Graph \u00b6 This document is to walk you through on how Nebula Graph is designed and why, and it will be separated into two parts: Industry Overview and Nebula Graph Architecture. Industry Overview \u00b6 OLAP & OLTP in Graph \u00b6 The axis in the above picture shows the different requirements for query latency. Like a traditional database, graph database can be divided into two parts: OLAP and OLTP. OLAP cares more about offline analysis while OLTP prefers online processing. Graph computing framework in OLAP is used to analyse data based on graph structure. And it's similar to OLAP in traditional database. But it has features which are not available in traditional database, one is iterative algorithm based on graph. A typical example is the PageRank algorithm from Google, which obtains the relevance of web pages through constant iterative computing. Another example is the commonly-used LPA algorithm. Along the axis to right, there comes the graph streaming field, which is the combination of basic computing and streaming computing. A relational network is not a static structure, rather, it constantly changes in the business: be it graph structure or graph properties. Computing in this filed is often triggered by events and its latency is in second. Right beside the graph streaming is the online response system, whose requirement for latency is extremely high, which should be in millisecond. The rightmost field is graph database and its use cases are completely different from the one on the left. The Nebula Graph we're working on can find its usage here in OLTP. Native Vs Multi-Model \u00b6 Graph databases can be classified into two kinds: native graph database and multi-model graph database. Using a graph first design, native graph databases are specifically optimized in storage and processing, thus they tend to perform queries faster, scale bigger and run more efficiently, calling for much less hardware at the same time. As for multi-model products, their storage comes from an outside source, such as a relational, columnar, or other NoSQL database. These databases use other algorithms to store data about vertices and edges and can lead to latent results as their storage layer is not optimized for graphs. Data Stored in Graph Database \u00b6 In graph database, data is stored as graph. Modelling data as graph is natural, and has the nice benefit of staying legible even by non-technical people. The data model handled by Nebula Graph is directed property graph , whose edges are directional and there could be properties on both edges and vertices. It can be represented as: G = < V, E, P V , P E > Here V is a set of nodes, aka vertices, E is a set of directional edges, P V represents properties on vertices, and P E is the properties on edges. Nebula Graph Architecture \u00b6 Designed based on the above features, Nebula Graph is an open source, distributed, lightning-fast graph database, it is composed of four components: storage service, meta service, query engine and client. The dashed line in the above picture divided computing and storage as two independent parts, the upper is the computing service, each machine or virtual machine is stateless and never talks to other so it's easy to scale in or out; the lower is the storage service, it's stateful since data is stored there. Storage service can turn graph semantics into key-values and pass them to the KV-store below it. Between the two is the Raft protocol. The right side is the meta service, similar to the NameNode in HDFS, it stores all metadata like schema and controls scaling. Design Thinking: Storage Service \u00b6 Nebula Graph adopted the shared-nothing distributed architecture in storage so nodes do not share memory or storage, which means there are no central nodes in the whole system. Benefits of such design are: Easy to scale The overall system continues operating despite individual crash Another design is the separation of computing and storage , and the benefits are as follows: Scalability. Separating storage from computing makes storage service flexible, thus it's easy to scale out or in. Availability. Recovery from vertex failure can be performed quickly. The binary of storage service is nebula-storaged , which provides a key-value store. Multiple storage engines like RocksDB and HBase are supported, with RocksDB set as the default engine. To build a resilient distributed system, Raft is implemented as the consensus algorithm. Raft achieves data consensus via an elected leader. Based on that, nebula-storaged makes the following optimizations: Parallel Raft Partitions of the same ID from multiple machines form a raft group. And the parallel operations are implemented with multiple sets of Raft groups. Write Path & batch In Raft protocol, the master replicates log entries to all the followers and commits the entries in order. To improve the write throughput, Nebula Graph not only employs the parallel raft, but also implements the dynamic batch replication. Load-balance Migrating the partitions on an overworked server to other relatively idle servers to increases availability and capacity of the system. Design-Thinking: Meta Service \u00b6 The binary of the meta service is nebula-metad . Here is the list of its main functionalities: User management In Nebula Graph different roles are assigned diverse privileges. We provide the following native roles: Global Admin, Graph Space Admin, User and Guest. - Cluster configuration management Meta service manages the servers and partitions in the cluster, e.g. records location of the partitions, receives heartbeat from servers, etc. It balances the partitions and manages the communication traffic in case of server failure. - Graph space management Nebula Graph supports multiple graph spaces. Data in different graph spaces are physically isolated. Meta service stores the metadata of all spaces in the cluster and tracks changes that take place in these spaces, like adding, dropping space, modifying graph space configuration (Raft copies). - Schema management Nebula Graph is a strong typed database. Types of tag and edge properties are recorded by meta service. Supported data types are: int, double, timestamp, list, etc. Multi-version management, supporting adding, modifying and deleting schema, and recording its version. TTL (time-to-live) management, supporting automatic data deletion and space reclamation. The meta service is stateful, and just like the storage service, it persists data to a key-value store. Design-Thinking: Query Engine \u00b6 Nebula Graph 's query language nGQL is a SQL-like descriptive language rather than an imperative one. It's compossible but not embeddable, it uses Shell pipe as an alternative, aka output in the former query acts as the input in the latter one. Key features of nGQL are as follows: Main algorithms are built in the query engine Duplicate queries can be avoided by supporting user-defined function (UDF) Programmable The binary of the query engine is nebula-graphd . Each nebula-graphd instance is stateless and never talks to other nebula-graphd. nebula-graphd only talks to the storage service and the meta service. That makes it trivial to expand or shrink the query engine cluster. The query engine accepts the message from the client and generates the execution plan after the lexical parsing (Lexer), semantic analysis (Parser) and the query optimization. Then the execution plan will be passed to the execution engine. The query execution engine takes the query plans and interacts with meta server and the storage engine to retrieve the schema and data. The main optimizations of the query engine are: Asynchronous and parallel execution I/O operations and network transmission are time-consuming. Thus asynchronous and parallel operations are widely adopted in the query engine to reduce the latency and to improve the overall throughput. Also, a separate resource pool is set for each query to avoid the long-tail effect of those time-consuming queries. Pushing down computation In a distributed system, transferring a large amount of data on the network really extends the overall latency. In Nebula Graph , the query engine will make decisions to push some filter and aggregation down to the storage service. The purpose is to reduce the amount of data passing back from the storage. Design-Thinking: API and SDK \u00b6 Nebula Graph provides SDKs in C++, Java, and Golang. Nebula Graph uses fbthrift as the RPC framework to communicate among servers. Nebula Graph 's web console is in progress and will be released soon.","title":"Design and Architecture"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#design_and_architecture_of_nebula_graph","text":"This document is to walk you through on how Nebula Graph is designed and why, and it will be separated into two parts: Industry Overview and Nebula Graph Architecture.","title":"Design and Architecture of Nebula Graph"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#industry_overview","text":"","title":"Industry Overview"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#olap_oltp_in_graph","text":"The axis in the above picture shows the different requirements for query latency. Like a traditional database, graph database can be divided into two parts: OLAP and OLTP. OLAP cares more about offline analysis while OLTP prefers online processing. Graph computing framework in OLAP is used to analyse data based on graph structure. And it's similar to OLAP in traditional database. But it has features which are not available in traditional database, one is iterative algorithm based on graph. A typical example is the PageRank algorithm from Google, which obtains the relevance of web pages through constant iterative computing. Another example is the commonly-used LPA algorithm. Along the axis to right, there comes the graph streaming field, which is the combination of basic computing and streaming computing. A relational network is not a static structure, rather, it constantly changes in the business: be it graph structure or graph properties. Computing in this filed is often triggered by events and its latency is in second. Right beside the graph streaming is the online response system, whose requirement for latency is extremely high, which should be in millisecond. The rightmost field is graph database and its use cases are completely different from the one on the left. The Nebula Graph we're working on can find its usage here in OLTP.","title":"OLAP &amp; OLTP in Graph"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#native_vs_multi-model","text":"Graph databases can be classified into two kinds: native graph database and multi-model graph database. Using a graph first design, native graph databases are specifically optimized in storage and processing, thus they tend to perform queries faster, scale bigger and run more efficiently, calling for much less hardware at the same time. As for multi-model products, their storage comes from an outside source, such as a relational, columnar, or other NoSQL database. These databases use other algorithms to store data about vertices and edges and can lead to latent results as their storage layer is not optimized for graphs.","title":"Native Vs Multi-Model"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#data_stored_in_graph_database","text":"In graph database, data is stored as graph. Modelling data as graph is natural, and has the nice benefit of staying legible even by non-technical people. The data model handled by Nebula Graph is directed property graph , whose edges are directional and there could be properties on both edges and vertices. It can be represented as: G = < V, E, P V , P E > Here V is a set of nodes, aka vertices, E is a set of directional edges, P V represents properties on vertices, and P E is the properties on edges.","title":"Data Stored in Graph Database"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#nebula_graph_architecture","text":"Designed based on the above features, Nebula Graph is an open source, distributed, lightning-fast graph database, it is composed of four components: storage service, meta service, query engine and client. The dashed line in the above picture divided computing and storage as two independent parts, the upper is the computing service, each machine or virtual machine is stateless and never talks to other so it's easy to scale in or out; the lower is the storage service, it's stateful since data is stored there. Storage service can turn graph semantics into key-values and pass them to the KV-store below it. Between the two is the Raft protocol. The right side is the meta service, similar to the NameNode in HDFS, it stores all metadata like schema and controls scaling.","title":"Nebula Graph Architecture"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#design_thinking_storage_service","text":"Nebula Graph adopted the shared-nothing distributed architecture in storage so nodes do not share memory or storage, which means there are no central nodes in the whole system. Benefits of such design are: Easy to scale The overall system continues operating despite individual crash Another design is the separation of computing and storage , and the benefits are as follows: Scalability. Separating storage from computing makes storage service flexible, thus it's easy to scale out or in. Availability. Recovery from vertex failure can be performed quickly. The binary of storage service is nebula-storaged , which provides a key-value store. Multiple storage engines like RocksDB and HBase are supported, with RocksDB set as the default engine. To build a resilient distributed system, Raft is implemented as the consensus algorithm. Raft achieves data consensus via an elected leader. Based on that, nebula-storaged makes the following optimizations: Parallel Raft Partitions of the same ID from multiple machines form a raft group. And the parallel operations are implemented with multiple sets of Raft groups. Write Path & batch In Raft protocol, the master replicates log entries to all the followers and commits the entries in order. To improve the write throughput, Nebula Graph not only employs the parallel raft, but also implements the dynamic batch replication. Load-balance Migrating the partitions on an overworked server to other relatively idle servers to increases availability and capacity of the system.","title":"Design Thinking: Storage Service"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#design-thinking_meta_service","text":"The binary of the meta service is nebula-metad . Here is the list of its main functionalities: User management In Nebula Graph different roles are assigned diverse privileges. We provide the following native roles: Global Admin, Graph Space Admin, User and Guest. - Cluster configuration management Meta service manages the servers and partitions in the cluster, e.g. records location of the partitions, receives heartbeat from servers, etc. It balances the partitions and manages the communication traffic in case of server failure. - Graph space management Nebula Graph supports multiple graph spaces. Data in different graph spaces are physically isolated. Meta service stores the metadata of all spaces in the cluster and tracks changes that take place in these spaces, like adding, dropping space, modifying graph space configuration (Raft copies). - Schema management Nebula Graph is a strong typed database. Types of tag and edge properties are recorded by meta service. Supported data types are: int, double, timestamp, list, etc. Multi-version management, supporting adding, modifying and deleting schema, and recording its version. TTL (time-to-live) management, supporting automatic data deletion and space reclamation. The meta service is stateful, and just like the storage service, it persists data to a key-value store.","title":"Design-Thinking: Meta Service"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#design-thinking_query_engine","text":"Nebula Graph 's query language nGQL is a SQL-like descriptive language rather than an imperative one. It's compossible but not embeddable, it uses Shell pipe as an alternative, aka output in the former query acts as the input in the latter one. Key features of nGQL are as follows: Main algorithms are built in the query engine Duplicate queries can be avoided by supporting user-defined function (UDF) Programmable The binary of the query engine is nebula-graphd . Each nebula-graphd instance is stateless and never talks to other nebula-graphd. nebula-graphd only talks to the storage service and the meta service. That makes it trivial to expand or shrink the query engine cluster. The query engine accepts the message from the client and generates the execution plan after the lexical parsing (Lexer), semantic analysis (Parser) and the query optimization. Then the execution plan will be passed to the execution engine. The query execution engine takes the query plans and interacts with meta server and the storage engine to retrieve the schema and data. The main optimizations of the query engine are: Asynchronous and parallel execution I/O operations and network transmission are time-consuming. Thus asynchronous and parallel operations are widely adopted in the query engine to reduce the latency and to improve the overall throughput. Also, a separate resource pool is set for each query to avoid the long-tail effect of those time-consuming queries. Pushing down computation In a distributed system, transferring a large amount of data on the network really extends the overall latency. In Nebula Graph , the query engine will make decisions to push some filter and aggregation down to the storage service. The purpose is to reduce the amount of data passing back from the storage.","title":"Design-Thinking: Query Engine"},{"location":"manual-EN/1.overview/3.design-and-architecture/1.design-and-architecture/#design-thinking_api_and_sdk","text":"Nebula Graph provides SDKs in C++, Java, and Golang. Nebula Graph uses fbthrift as the RPC framework to communicate among servers. Nebula Graph 's web console is in progress and will be released soon.","title":"Design-Thinking: API and SDK"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/","text":"Storage Design \u00b6 Abstract \u00b6 This document gives an introduction to the storage design of the graph database Nebula Graph . The Storage Service of Nebula Graph is composed of two parts. One is Meta Service that stores the meta data, the other is Storage Service that stores the data. The two services are in two independent processes. The data directory and deployment are separated but their architectures are almost the same. Architecture \u00b6 Fig. 1 The Architecture of Storage Service As shown in Fig. 1, there are three layers in Storage Service. The bottom layer is the local storage engine, providing get , put , scan and delete operations on local data. The related interfaces are in KVStore/KVEngine.h and users can develop their own local store plugins based on their needs. Currently, Nebula Graph provides store engine based on RocksDB. Above the local storage engine is the consensus layer that implements multi group raft. Each partition corresponding to a Raft group, is for the data sharding. Currently, Nebula Graph uses hash to shard data. When creating a space, users need to specify the partition number. Once set, partition number cannot be changed. Generally, the partition number must meet the need to scale-out in the future. Above the consensus layer is the storage interface that defines a set of APIs that are related to graph. These API requests are translated into a set of kv operations to the corresponding partition. It is this layer that makes the storage service a real graph storage, otherwise it's just a kv storage. Nebula Graph doesn't use kv-store as an independent service as a graph query involves a lot of calculation that involves schema, which is not existed in the kv layer. Such architecture makes computation operation pushing down more easily. Schema & Partition \u00b6 As a graph database, Nebula Graph stores the vertices, edges and their properties. How to efficiently filtering or projecting is critical for a graph exploration. Nebula Graph uses tags to indicate a vertex type. One vertex can have multiple types (and therefore multiple tags), and each tag defines its own properties. In the kv store, we use vertex_ID + Tag_ID together as a key, and the corresponding value are the encoded property. The format is shown in Fig. 2: Fig. 2 Vertex Key Format Type : one byte, to indicate the key type. e.g. data, index, system, etc. Part ID : three bytes, used to indicate the (sharding) partition id. It's designed for the data migration/balance operation by prefix-scanning all the data in a partition. Vertex ID : eight bytes, used to indicate vertex ID. Two vertices with an identity vertexID are considered as the same one. Tag ID : four bytes, used to indicate its tag's (encoded) ID. Timestamp : eight bytes, not visible to users. Reserved for Multiversion concurrency control (MVCC) . Each edge in Nebula Graph is modeled and stored as two independent key-values. One, namely the out-edge , is stored in the same partition as the source vertex . The other one, namely in-edge , is stored in the same partition as the destination vertex . So generally, out-key and in-key are in different partitions. Between two vertices, edges with the same type are acceptable, and different types are legal as well. For example, by defining an edge type ' money-transfer-to ', user A can transfer money to user B at two timestamps. Thus a field, namely rank , is added to (the key part of the timestamp to) distinguish which transfer records is referring. Edge key format is shown in Fig. 3: Fig. 3 Edge Key Format Type : one byte, used to indicate key type. E.g., data, index, system, etc. Part ID : three bytes. The same as in Fig. 2. Vertex ID : eight bytes, used to indicate source vertex ID of an out-edge (Fig. 4), and destination vertex ID of an in-edge (Fig. 5). See below. Edge Type : four bytes, used to indicate (encoded) edge type id. A positive number means that this key is an out-edge , and a negative number indicates that this is an in-edge . Rank : eight bytes, used in multiple edges with the same type. E.g., It can store transaction time , transaction amount , or edge weight . Timestamp : eight bytes. The same as in Fig. 2. If Edge Type is positive, the corresponding edge key format is shown in Fig. 4; otherwise, the corresponding edge key format is shown in Fig. 5. Fig. 4 Out-key format Fig. 5 In-key format Besides the key part above, the value part is the encoded properties (of a vertex or an edge). As a strong typed database, Nebula Graph gets the schema information from the Meta Service before encoding/decoding. And multi-version schema are also considered when altering schema. Nebula Graph shards data through modulo operation on vertex ID . All the out-keys , in-keys and tag id are placed in the same partition. This improves query efficiency as a local/non-remote file access. Breadth-First-Search (BFS) expansion starting from a given vertex is a very common ad-hoc graph exploration. And during BFS, the performance of filtering out edge/vertex properties are time-consuming. Nebula Graph guarantees the operation efficiency by putting properties of a vertex and its edges locating near each other. It is worth noting that most graph databases vendors run their benchmarks with Graph 500 or Twitter data set, which are of no eloquence because the properties are not taken into consideration in this kind of graph exploration. While most production cases are not that simple. KVStore \u00b6 Nebula Graph writes its own kv store to meet the performance needs: High performance , a pure high performance key value store. Provided as a library , as a strong typed database, the performance of storage layer is key to Nebula Graph . Strong data consistency , since Nebula Graph is a distribution system. Written in C++ , as most of our developers are C++ programers. For users who are not sensitive to performance or unwilling to migrate data from other storage systems, such as HBase or MySQL, Nebula Graph also provides a plugin over the kv store to replace its default RocksDB. Currently, HBase plugin has been released yet. As RocksDB is the local storage engine, Nebula Graph can manage multiple hard disks to take full use of the parallel IO access. What a user needs to do is to configure multiple data directories. Nebula Graph manages the distributed kv store in with meta service. All the partition distribution and cluster machine status can be found in the meta service. Users can input commands in the console to add or remove machines to generate and execute a balance plan in meta service. Nebula Graph writes its own (Write-Ahead-Log, WAL) module to replace the default one in RocksDB. Since the WAL is used for (distributed system's) Raft consensus. Each partition has a WAL, so after a (crash and) reboot, the partition can catch up its own data, and there is no need to split WAL between several partitions. Besides, Nebula Graph defines a special category, namely Command Operation Log , to conduct some command operations. These logs are very short, with no real data, and are only used to inform all replicas to execute certain command operations with raft protocol. What's more, since the logs are serialized in the Raft protocol, Nebula Graph also provides another class, namely Atomic Operation Log , to conduct the atomic operation between the replicas of a partitions. E.g., the compare-and-set (CAS) or read-modify-write operations are atomic in Nebula Graph per partition. A Nebula Graph cluster can have multiple individual graph spaces. Each space has its own partition number and replica copies. Different spaces are isolated physically from each other in the same cluster. Besides, the spaces can also have very different storage engines and sharding strategies. E.g., One space can use HBase as its storage backend with alphabet ranging sharding, and the other space uses the default RocksDB with hashing sharding. And these two spaces are running in the same Nebula Graph cluster. Raft Implementation \u00b6 This part gives some details on how the raft protocol is implemented in Nebula Graph . Multi Raft Group \u00b6 According to Raft requirement, the log ID must be in a sequential order. Therefore, almost all the raft implementations will use Multi Raft Group to increase the concurrency. Therefore, the number of partition will determine how many operations can be executed simultaneously. But you can not simply add too much partitions in the system, which can have some side affects. Each raft group stores many state information and (as mentioned earlier) it has a WAL file. Thus, the more partitions, the more footprint costs. Also, if the work load is low, the batch operation can not gain from the parallel. E.g., consider a system with ten thousand partitions. For every second, there are about ten thousands write-in requests. You can calculate that in average, for every partition, there is only one write-in request. So from the client side, it's a 100k batch write. But from the partition side, it's a single write. There are two key challenges to implement the Multi Raft Group. First one is how to share the transport layer . Because each Raft Group sends messages to its corresponding peers, if the transport layer cannot be shared, the connection costs will be very high. Second one is how to design the multi-threading model . Raft Groups share the same thread pool to prevent starting too many threads and a high context switch cost. Batch \u00b6 For each Partition, it is necessary to do batch multiple operations together to improve throughput when writing WAL serially. In general, there is nothing special about batch, but Nebula Graph designs some special types of WAL based on each part serialization, which brings some challenges. For example, Nebula Graph uses WAL to implement lock-free CAS operations. And every CAS operation will be executed until the previous WAL has been committed. So for a batch, if there are some logs contain CAS operation, we need to divide this batch into several smaller (sub)groups. And make sure these (sub)groups are executed in sequential order. Learner \u00b6 When a new machine is added to a cluster, it has to catch up data for quite a long time. And there may be accidents during this process. If this one directly joins the raft group as a follower role, it will dramatically reduce the availability of the entire cluster. Nebula Graph introduces the learner role, and it is implemented by the command WAL mentioned above. When a leader is writing WAL and meets an add learner command , it will add the new coming-in learner to its peers list and mark it as a learner. The logs will send to all the peers, both the followers and the learner. But the learner can not vote for the leader's election. Transfer Leadership \u00b6 Transfer leadership is extremely important during a data balance operation. When migrating a partition from one machine to another, Nebula Graph will first check if it is a leader. If so, another follower should be elected as a leader before the migration. Otherwise, the cluster service is affected since the leader is on migration. After the migration is done, a BALANCE LEADER command is invoked, so that the work load on each machine can be balanced. When transferring leadership, it is worth noting the timing when a leader abandons the leadership and when all the followers start a leader election. When a transfer leadership command is committed, from the leader's view, it loses the leadership. From other followers' view, when receiving this command, it starts a new leader election. These two operations must be executed in the same process with a normal raft leader election. Otherwise, some corner cases can occur and they are very hard to test. Membership Change \u00b6 To avoid the brain-split, when Raft Group members changed, an intermediate state is required. In such state, the majority of the old group and new group always have an overlap. This majority overlap will prevent neither group from making decisions unilaterally. This is the joint consensus as mentioned in the famous Raft thesis. To make it even simpler, Diego Ongaro suggests to add or remove only one peer at a time to ensure the overlap between the majority in his doctoral thesis. Nebula Graph 's implementation also uses this approach, except that the implementation to add or remove member is different. For details, please refer to addPeer/removePeer in Raft Part source code. Snapshot \u00b6 Take snapshot is a common command during daily DBA operations. But snapshot operation will introduce extra challenges when considering together with the raft protocol. It's very error-prone. E.g., what if the leader loses its leadership in an election when sending a snapshot command. What should we do. In this situation, the follower may only receive half log of the snapshot command, should we cleanup and rollback? Because multiple partitions share a single storage, how to clean up the data is a cumbersome work. In addition, the snapshot process will start a heavy write to disks. To avoid slow down the frontend reads and writes, we do not want snapshot process to share the same IO threadPool with the normal Raft logs. Besides, snapshot also requires large footprint, which is critical for online service performance. Storage Service \u00b6 The Interfaces of Storage Service layer are: Insert vertex/edge : insert a vertex or edge and its properties. getNeighbors : get the in-edge or out-edge from a set of vertices. And return the edges and properties. Condition filtering are also considered. getProps : get the properties of a vertex or an edge. Graph semantics interfaces are translated into kv operations in this layer as well. In order to improve the performance, concurrent operations are also implemented in this layer. Meta Service \u00b6 Nebula Graph wrap up a set of meta-related interfaces from the kv store interface (as mentioned earlier). Meta service can support CRUD operation on schema, cluster administration and user privileges. Meta service can be deployed on a single host, but it is recommended to deploy on multiple hosts with at least three or five replicas to get a better availability and fault tolerance.","title":"Storage Design"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#storage_design","text":"","title":"Storage Design"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#abstract","text":"This document gives an introduction to the storage design of the graph database Nebula Graph . The Storage Service of Nebula Graph is composed of two parts. One is Meta Service that stores the meta data, the other is Storage Service that stores the data. The two services are in two independent processes. The data directory and deployment are separated but their architectures are almost the same.","title":"Abstract"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#architecture","text":"Fig. 1 The Architecture of Storage Service As shown in Fig. 1, there are three layers in Storage Service. The bottom layer is the local storage engine, providing get , put , scan and delete operations on local data. The related interfaces are in KVStore/KVEngine.h and users can develop their own local store plugins based on their needs. Currently, Nebula Graph provides store engine based on RocksDB. Above the local storage engine is the consensus layer that implements multi group raft. Each partition corresponding to a Raft group, is for the data sharding. Currently, Nebula Graph uses hash to shard data. When creating a space, users need to specify the partition number. Once set, partition number cannot be changed. Generally, the partition number must meet the need to scale-out in the future. Above the consensus layer is the storage interface that defines a set of APIs that are related to graph. These API requests are translated into a set of kv operations to the corresponding partition. It is this layer that makes the storage service a real graph storage, otherwise it's just a kv storage. Nebula Graph doesn't use kv-store as an independent service as a graph query involves a lot of calculation that involves schema, which is not existed in the kv layer. Such architecture makes computation operation pushing down more easily.","title":"Architecture"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#schema_partition","text":"As a graph database, Nebula Graph stores the vertices, edges and their properties. How to efficiently filtering or projecting is critical for a graph exploration. Nebula Graph uses tags to indicate a vertex type. One vertex can have multiple types (and therefore multiple tags), and each tag defines its own properties. In the kv store, we use vertex_ID + Tag_ID together as a key, and the corresponding value are the encoded property. The format is shown in Fig. 2: Fig. 2 Vertex Key Format Type : one byte, to indicate the key type. e.g. data, index, system, etc. Part ID : three bytes, used to indicate the (sharding) partition id. It's designed for the data migration/balance operation by prefix-scanning all the data in a partition. Vertex ID : eight bytes, used to indicate vertex ID. Two vertices with an identity vertexID are considered as the same one. Tag ID : four bytes, used to indicate its tag's (encoded) ID. Timestamp : eight bytes, not visible to users. Reserved for Multiversion concurrency control (MVCC) . Each edge in Nebula Graph is modeled and stored as two independent key-values. One, namely the out-edge , is stored in the same partition as the source vertex . The other one, namely in-edge , is stored in the same partition as the destination vertex . So generally, out-key and in-key are in different partitions. Between two vertices, edges with the same type are acceptable, and different types are legal as well. For example, by defining an edge type ' money-transfer-to ', user A can transfer money to user B at two timestamps. Thus a field, namely rank , is added to (the key part of the timestamp to) distinguish which transfer records is referring. Edge key format is shown in Fig. 3: Fig. 3 Edge Key Format Type : one byte, used to indicate key type. E.g., data, index, system, etc. Part ID : three bytes. The same as in Fig. 2. Vertex ID : eight bytes, used to indicate source vertex ID of an out-edge (Fig. 4), and destination vertex ID of an in-edge (Fig. 5). See below. Edge Type : four bytes, used to indicate (encoded) edge type id. A positive number means that this key is an out-edge , and a negative number indicates that this is an in-edge . Rank : eight bytes, used in multiple edges with the same type. E.g., It can store transaction time , transaction amount , or edge weight . Timestamp : eight bytes. The same as in Fig. 2. If Edge Type is positive, the corresponding edge key format is shown in Fig. 4; otherwise, the corresponding edge key format is shown in Fig. 5. Fig. 4 Out-key format Fig. 5 In-key format Besides the key part above, the value part is the encoded properties (of a vertex or an edge). As a strong typed database, Nebula Graph gets the schema information from the Meta Service before encoding/decoding. And multi-version schema are also considered when altering schema. Nebula Graph shards data through modulo operation on vertex ID . All the out-keys , in-keys and tag id are placed in the same partition. This improves query efficiency as a local/non-remote file access. Breadth-First-Search (BFS) expansion starting from a given vertex is a very common ad-hoc graph exploration. And during BFS, the performance of filtering out edge/vertex properties are time-consuming. Nebula Graph guarantees the operation efficiency by putting properties of a vertex and its edges locating near each other. It is worth noting that most graph databases vendors run their benchmarks with Graph 500 or Twitter data set, which are of no eloquence because the properties are not taken into consideration in this kind of graph exploration. While most production cases are not that simple.","title":"Schema &amp; Partition"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#kvstore","text":"Nebula Graph writes its own kv store to meet the performance needs: High performance , a pure high performance key value store. Provided as a library , as a strong typed database, the performance of storage layer is key to Nebula Graph . Strong data consistency , since Nebula Graph is a distribution system. Written in C++ , as most of our developers are C++ programers. For users who are not sensitive to performance or unwilling to migrate data from other storage systems, such as HBase or MySQL, Nebula Graph also provides a plugin over the kv store to replace its default RocksDB. Currently, HBase plugin has been released yet. As RocksDB is the local storage engine, Nebula Graph can manage multiple hard disks to take full use of the parallel IO access. What a user needs to do is to configure multiple data directories. Nebula Graph manages the distributed kv store in with meta service. All the partition distribution and cluster machine status can be found in the meta service. Users can input commands in the console to add or remove machines to generate and execute a balance plan in meta service. Nebula Graph writes its own (Write-Ahead-Log, WAL) module to replace the default one in RocksDB. Since the WAL is used for (distributed system's) Raft consensus. Each partition has a WAL, so after a (crash and) reboot, the partition can catch up its own data, and there is no need to split WAL between several partitions. Besides, Nebula Graph defines a special category, namely Command Operation Log , to conduct some command operations. These logs are very short, with no real data, and are only used to inform all replicas to execute certain command operations with raft protocol. What's more, since the logs are serialized in the Raft protocol, Nebula Graph also provides another class, namely Atomic Operation Log , to conduct the atomic operation between the replicas of a partitions. E.g., the compare-and-set (CAS) or read-modify-write operations are atomic in Nebula Graph per partition. A Nebula Graph cluster can have multiple individual graph spaces. Each space has its own partition number and replica copies. Different spaces are isolated physically from each other in the same cluster. Besides, the spaces can also have very different storage engines and sharding strategies. E.g., One space can use HBase as its storage backend with alphabet ranging sharding, and the other space uses the default RocksDB with hashing sharding. And these two spaces are running in the same Nebula Graph cluster.","title":"KVStore"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#raft_implementation","text":"This part gives some details on how the raft protocol is implemented in Nebula Graph .","title":"Raft Implementation"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#multi_raft_group","text":"According to Raft requirement, the log ID must be in a sequential order. Therefore, almost all the raft implementations will use Multi Raft Group to increase the concurrency. Therefore, the number of partition will determine how many operations can be executed simultaneously. But you can not simply add too much partitions in the system, which can have some side affects. Each raft group stores many state information and (as mentioned earlier) it has a WAL file. Thus, the more partitions, the more footprint costs. Also, if the work load is low, the batch operation can not gain from the parallel. E.g., consider a system with ten thousand partitions. For every second, there are about ten thousands write-in requests. You can calculate that in average, for every partition, there is only one write-in request. So from the client side, it's a 100k batch write. But from the partition side, it's a single write. There are two key challenges to implement the Multi Raft Group. First one is how to share the transport layer . Because each Raft Group sends messages to its corresponding peers, if the transport layer cannot be shared, the connection costs will be very high. Second one is how to design the multi-threading model . Raft Groups share the same thread pool to prevent starting too many threads and a high context switch cost.","title":"Multi Raft Group"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#batch","text":"For each Partition, it is necessary to do batch multiple operations together to improve throughput when writing WAL serially. In general, there is nothing special about batch, but Nebula Graph designs some special types of WAL based on each part serialization, which brings some challenges. For example, Nebula Graph uses WAL to implement lock-free CAS operations. And every CAS operation will be executed until the previous WAL has been committed. So for a batch, if there are some logs contain CAS operation, we need to divide this batch into several smaller (sub)groups. And make sure these (sub)groups are executed in sequential order.","title":"Batch"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#learner","text":"When a new machine is added to a cluster, it has to catch up data for quite a long time. And there may be accidents during this process. If this one directly joins the raft group as a follower role, it will dramatically reduce the availability of the entire cluster. Nebula Graph introduces the learner role, and it is implemented by the command WAL mentioned above. When a leader is writing WAL and meets an add learner command , it will add the new coming-in learner to its peers list and mark it as a learner. The logs will send to all the peers, both the followers and the learner. But the learner can not vote for the leader's election.","title":"Learner"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#transfer_leadership","text":"Transfer leadership is extremely important during a data balance operation. When migrating a partition from one machine to another, Nebula Graph will first check if it is a leader. If so, another follower should be elected as a leader before the migration. Otherwise, the cluster service is affected since the leader is on migration. After the migration is done, a BALANCE LEADER command is invoked, so that the work load on each machine can be balanced. When transferring leadership, it is worth noting the timing when a leader abandons the leadership and when all the followers start a leader election. When a transfer leadership command is committed, from the leader's view, it loses the leadership. From other followers' view, when receiving this command, it starts a new leader election. These two operations must be executed in the same process with a normal raft leader election. Otherwise, some corner cases can occur and they are very hard to test.","title":"Transfer Leadership"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#membership_change","text":"To avoid the brain-split, when Raft Group members changed, an intermediate state is required. In such state, the majority of the old group and new group always have an overlap. This majority overlap will prevent neither group from making decisions unilaterally. This is the joint consensus as mentioned in the famous Raft thesis. To make it even simpler, Diego Ongaro suggests to add or remove only one peer at a time to ensure the overlap between the majority in his doctoral thesis. Nebula Graph 's implementation also uses this approach, except that the implementation to add or remove member is different. For details, please refer to addPeer/removePeer in Raft Part source code.","title":"Membership Change"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#snapshot","text":"Take snapshot is a common command during daily DBA operations. But snapshot operation will introduce extra challenges when considering together with the raft protocol. It's very error-prone. E.g., what if the leader loses its leadership in an election when sending a snapshot command. What should we do. In this situation, the follower may only receive half log of the snapshot command, should we cleanup and rollback? Because multiple partitions share a single storage, how to clean up the data is a cumbersome work. In addition, the snapshot process will start a heavy write to disks. To avoid slow down the frontend reads and writes, we do not want snapshot process to share the same IO threadPool with the normal Raft logs. Besides, snapshot also requires large footprint, which is critical for online service performance.","title":"Snapshot"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#storage_service","text":"The Interfaces of Storage Service layer are: Insert vertex/edge : insert a vertex or edge and its properties. getNeighbors : get the in-edge or out-edge from a set of vertices. And return the edges and properties. Condition filtering are also considered. getProps : get the properties of a vertex or an edge. Graph semantics interfaces are translated into kv operations in this layer as well. In order to improve the performance, concurrent operations are also implemented in this layer.","title":"Storage Service"},{"location":"manual-EN/1.overview/3.design-and-architecture/2.storage-design/#meta_service","text":"Nebula Graph wrap up a set of meta-related interfaces from the kv store interface (as mentioned earlier). Meta service can support CRUD operation on schema, cluster administration and user privileges. Meta service can be deployed on a single host, but it is recommended to deploy on multiple hosts with at least three or five replicas to get a better availability and fault tolerance.","title":"Meta Service"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/","text":"Query Engine Overview \u00b6 The query engine is used to process the Nebula Graph Query Language (nGQL) statements. This article gives an architectural overview of the Nebula Graph query engine. Above is the overview chart of the query engine. If you are familiar with the SQL execution engine, this should be no stranger to you. In fact, the Nebula Graph query engine is very similar to the modern SQL execution engine except the query language parser and the real actions in the execution plan. Session Manager \u00b6 Nebula Graph employs the Role Based Access Control. So when the client first connects to the Query Engine, it needs to authenticate itself. When it succeeds, the query engine creates a new session and returns the session ID to the client. All sessions are managed by the Session Manager. The session will remember the current graph space and the access rights to the space. The session will keep some session-wide configurations and be used as an temporary storage to store information across multiple requests in the same session as well. The session will be dropped when the client connection is closed, or being idle for a period of time. The length of the idle time is configurable. When the client sends a request to the query engine, it needs to attach the current session ID, otherwise, the query engine will reject the request. When the query engine accesses the storage engines, it will attach the session object to every request, so that the storage engine does not have to manage sessions. Parser \u00b6 The first thing that the query engine will do when receiving a request is to parse the statements in the request. This is done by the parser. The majority of the parser code is generated by the famous flex/bison tool set. The lexicon and syntax files for the nGQL can be found in the src/parser folder in the source code tree. The nGQL is designed in a way that is close enough to SQL. The idea is to smoothen the learning curve as much as possible. Graph databases currently do not have a unified international query language standard. As soon as the ISO's GQL committee releases their first draft, we will make nGQL compatible with the proposed GQL. The output of the parser is an Abstract Syntax Tree (AST), which will be passed on to the next module: Execution Planner. Execution Planner \u00b6 The Execution Planner will convert the AST from the parser into a list of actions (execution plan). An action is the smallest unit that can be executed. A typical action could be fetching all neighbors for a given vertex, getting properties for an edge, or filtering vertices or edges based on the given condition. When converting AST to the execution plan, all IDs will be extracted, so that the execution plan can be reused. The extracted IDs will be placed in the context for the current request. The context will be used to store the variables and intermediate results as well. Optimization \u00b6 The newly generated execution plan will then be passed to the Optimization Framework. There are multiple optimizers registered in the framework. The execution plan will be passed through all optimizers sequentially. Each optimizer has the opportunity to modify (optimize) the plan. At the end, the final plan can look dramatically different from the original plan, but the execution result should be exactly same as the original plan. Execution \u00b6 The last step in the query engine is to execute the optimized execution plan. It's done by the Execution Framework. Each executor will process one execution plan at a time. Actions in the plan will be executed one by one. The executor will do limited local optimization as well, such as deciding whether to run in parallel. Depending on the action, the executor will communicate with the Meta Service or the Storage Engine via their clients.","title":"Query Engine"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#query_engine_overview","text":"The query engine is used to process the Nebula Graph Query Language (nGQL) statements. This article gives an architectural overview of the Nebula Graph query engine. Above is the overview chart of the query engine. If you are familiar with the SQL execution engine, this should be no stranger to you. In fact, the Nebula Graph query engine is very similar to the modern SQL execution engine except the query language parser and the real actions in the execution plan.","title":"Query Engine Overview"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#session_manager","text":"Nebula Graph employs the Role Based Access Control. So when the client first connects to the Query Engine, it needs to authenticate itself. When it succeeds, the query engine creates a new session and returns the session ID to the client. All sessions are managed by the Session Manager. The session will remember the current graph space and the access rights to the space. The session will keep some session-wide configurations and be used as an temporary storage to store information across multiple requests in the same session as well. The session will be dropped when the client connection is closed, or being idle for a period of time. The length of the idle time is configurable. When the client sends a request to the query engine, it needs to attach the current session ID, otherwise, the query engine will reject the request. When the query engine accesses the storage engines, it will attach the session object to every request, so that the storage engine does not have to manage sessions.","title":"Session Manager"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#parser","text":"The first thing that the query engine will do when receiving a request is to parse the statements in the request. This is done by the parser. The majority of the parser code is generated by the famous flex/bison tool set. The lexicon and syntax files for the nGQL can be found in the src/parser folder in the source code tree. The nGQL is designed in a way that is close enough to SQL. The idea is to smoothen the learning curve as much as possible. Graph databases currently do not have a unified international query language standard. As soon as the ISO's GQL committee releases their first draft, we will make nGQL compatible with the proposed GQL. The output of the parser is an Abstract Syntax Tree (AST), which will be passed on to the next module: Execution Planner.","title":"Parser"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#execution_planner","text":"The Execution Planner will convert the AST from the parser into a list of actions (execution plan). An action is the smallest unit that can be executed. A typical action could be fetching all neighbors for a given vertex, getting properties for an edge, or filtering vertices or edges based on the given condition. When converting AST to the execution plan, all IDs will be extracted, so that the execution plan can be reused. The extracted IDs will be placed in the context for the current request. The context will be used to store the variables and intermediate results as well.","title":"Execution Planner"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#optimization","text":"The newly generated execution plan will then be passed to the Optimization Framework. There are multiple optimizers registered in the framework. The execution plan will be passed through all optimizers sequentially. Each optimizer has the opportunity to modify (optimize) the plan. At the end, the final plan can look dramatically different from the original plan, but the execution result should be exactly same as the original plan.","title":"Optimization"},{"location":"manual-EN/1.overview/3.design-and-architecture/3.query-engine/#execution","text":"The last step in the query engine is to execute the optimized execution plan. It's done by the Execution Framework. Each executor will process one execution plan at a time. Actions in the plan will be executed one by one. The executor will do limited local optimization as well, such as deciding whether to run in parallel. Depending on the action, the executor will communicate with the Meta Service or the Storage Engine via their clients.","title":"Execution"},{"location":"manual-EN/2.query-language/0.README/","text":"Reader \u00b6 This chapter is for those who want to use Nebula Graph query language. Example Data \u00b6 The example data used in Nebula Graph query statements can be downloaded here . After downloading the example data, you can import it to your Nebula Graph database with Nebula Graph Studio . Placeholder Identifiers and Values \u00b6 The query language of Nebula Graph is nGQL. Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, refer to the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"Introduction"},{"location":"manual-EN/2.query-language/0.README/#reader","text":"This chapter is for those who want to use Nebula Graph query language.","title":"Reader"},{"location":"manual-EN/2.query-language/0.README/#example_data","text":"The example data used in Nebula Graph query statements can be downloaded here . After downloading the example data, you can import it to your Nebula Graph database with Nebula Graph Studio .","title":"Example Data"},{"location":"manual-EN/2.query-language/0.README/#placeholder_identifiers_and_values","text":"The query language of Nebula Graph is nGQL. Refer to the following standards in nGQL: ISO/IEC 10646 ISO/IEC 39075 ISO/IEC NP 39075 (Draft) In template code, any token that is not a keyword, a literal value, or punctuation is a placeholder identifier or a placeholder value. For details of the symbols in nGQL, refer to the following table: Token Meaning < > name of a syntactic element ::= formula that defines an element [ ] optional elements { } explicitly specified elements | complete alternative elements ... may be repeated any number of times","title":"Placeholder Identifiers and Values"},{"location":"manual-EN/2.query-language/1.data-types/data-types/","text":"Data Types \u00b6 The built-in data types supported by Nebula Graph are as follows: Numeric Types \u00b6 Integer \u00b6 An integer is declared with keyword int , which is 64-bit signed , the range is [-9223372036854775808, 9223372036854775807], and there is no overflow in int64-based calculation. Integer constants support multiple formats: Decimal, for example 123456 Hexadecimal, for example 0xdeadbeaf Octal, for example 01234567 Boolean \u00b6 A boolean data type is declared with the bool keyword and can only take the values true or false . String \u00b6 The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. Line breaks are not allowed in a string. For example \"Shaquile O'Neal\" , '\"This is a double-quoted literal string\"' . Embedding escape sequences are supported within strings, for example: 1. \"\\n\\t\\r\\b\\f\" 1. \"\\110ello world\" Timestamp \u00b6 The supported range of timestamp type is '1970-01-01 00:00:01' UTC to '2262-04-11 23:47:16' UTC Timestamp is measured in units of seconds Supported data inserting methods call function now() Time string, for example: \"2019-10-01 10:00:00\" Input the timestamp directly, namely the number of seconds from 1970-01-01 00:00:00 Nebula Graph converts TIMESTAMP values from the current time zone to UTC for storage, and back from UTC to the current time zone for retrieval The underlying storage data type is: int64 Examples Create a tag named school nebula> CREATE TAG school(name string , create_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01 08:00:00\" nebula> INSERT VERTEX school(name, create_time) VALUES hash(\"new\"):(\"new\", \"1985-10-01 08:00:00\") Insert a vertex named \"dut\" with the foundation date now nebula> INSERT VERTEX school(name, create_time) VALUES hash(\"dut\"):(\"dut\", now())","title":"Data Types"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#data_types","text":"The built-in data types supported by Nebula Graph are as follows:","title":"Data Types"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#numeric_types","text":"","title":"Numeric Types"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#integer","text":"An integer is declared with keyword int , which is 64-bit signed , the range is [-9223372036854775808, 9223372036854775807], and there is no overflow in int64-based calculation. Integer constants support multiple formats: Decimal, for example 123456 Hexadecimal, for example 0xdeadbeaf Octal, for example 01234567","title":"Integer"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#boolean","text":"A boolean data type is declared with the bool keyword and can only take the values true or false .","title":"Boolean"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#string","text":"The string type is used to store a sequence of characters (text). The literal constant is a sequence of characters of any length surrounded by double or single quotes. Line breaks are not allowed in a string. For example \"Shaquile O'Neal\" , '\"This is a double-quoted literal string\"' . Embedding escape sequences are supported within strings, for example: 1. \"\\n\\t\\r\\b\\f\" 1. \"\\110ello world\"","title":"String"},{"location":"manual-EN/2.query-language/1.data-types/data-types/#timestamp","text":"The supported range of timestamp type is '1970-01-01 00:00:01' UTC to '2262-04-11 23:47:16' UTC Timestamp is measured in units of seconds Supported data inserting methods call function now() Time string, for example: \"2019-10-01 10:00:00\" Input the timestamp directly, namely the number of seconds from 1970-01-01 00:00:00 Nebula Graph converts TIMESTAMP values from the current time zone to UTC for storage, and back from UTC to the current time zone for retrieval The underlying storage data type is: int64 Examples Create a tag named school nebula> CREATE TAG school(name string , create_time timestamp); Insert a vertex named \"stanford\" with the foundation date \"1885-10-01 08:00:00\" nebula> INSERT VERTEX school(name, create_time) VALUES hash(\"new\"):(\"new\", \"1985-10-01 08:00:00\") Insert a vertex named \"dut\" with the foundation date now nebula> INSERT VERTEX school(name, create_time) VALUES hash(\"dut\"):(\"dut\", now())","title":"Timestamp"},{"location":"manual-EN/2.query-language/1.data-types/type-conversion/","text":"Type Conversion \u00b6 Converting an expression of a given type to another type is known as type-conversion. In nGQL, type conversion is divided into implicit conversion and explicit conversion. Implicit Type Conversion \u00b6 Implicit conversions are automatically performed when a value is copied to a compatible type. Following types can implicitly converted to bool : The conversions from/to bool consider false equivalent to 0 for empty string types, true is equivalent to all other values. The conversions from/to bool consider false equivalent to 0 for int types, true is equivalent to all other values. The conversions from/to bool consider false equivalent to 0.0 for float types, true is equivalent to all other values. int can implicitly converted to double . Explicit Type Conversion \u00b6 In addition to implicit type conversion, explicit type conversion is also supported in case of semantics compliance. The syntax is similar to the C language: (type_name)expression . For example, the results of YIELD length((string)(123)), (int)\"123\" + 1 are 3, 124 respectively. And YIELD (int)(\"12ab3\") fails in conversion.","title":"Type Conversion"},{"location":"manual-EN/2.query-language/1.data-types/type-conversion/#type_conversion","text":"Converting an expression of a given type to another type is known as type-conversion. In nGQL, type conversion is divided into implicit conversion and explicit conversion.","title":"Type Conversion"},{"location":"manual-EN/2.query-language/1.data-types/type-conversion/#implicit_type_conversion","text":"Implicit conversions are automatically performed when a value is copied to a compatible type. Following types can implicitly converted to bool : The conversions from/to bool consider false equivalent to 0 for empty string types, true is equivalent to all other values. The conversions from/to bool consider false equivalent to 0 for int types, true is equivalent to all other values. The conversions from/to bool consider false equivalent to 0.0 for float types, true is equivalent to all other values. int can implicitly converted to double .","title":"Implicit Type Conversion"},{"location":"manual-EN/2.query-language/1.data-types/type-conversion/#explicit_type_conversion","text":"In addition to implicit type conversion, explicit type conversion is also supported in case of semantics compliance. The syntax is similar to the C language: (type_name)expression . For example, the results of YIELD length((string)(123)), (int)\"123\" + 1 are 3, 124 respectively. And YIELD (int)(\"12ab3\") fails in conversion.","title":"Explicit Type Conversion"},{"location":"manual-EN/2.query-language/2.functions-and-operators/bitwise-operators/","text":"Bitwise Operators \u00b6 Name Description & Bitwise AND | Bitwise OR ^ Bitwise exclusive OR (XOR)","title":"Bitwise Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/bitwise-operators/#bitwise_operators","text":"Name Description & Bitwise AND | Bitwise OR ^ Bitwise exclusive OR (XOR)","title":"Bitwise Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/built-in-functions/","text":"Built-in Functions \u00b6 Nebula Graph supports calling built-in functions of the following types: Math \u00b6 Function Description double abs(double x) Return absolute value of the argument double floor(double x) Return the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Return the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Return integral value nearest to the argument, returns a number farther away from 0 if the parameter is in the middle double sqrt(double x) Return the square root of the argument double cbrt(double x) Return the cubic root of the argument double hypot(double x, double x) Return the hypotenuse of a right-angled triangle double pow(double x, double y) Compute the power of the argument double exp(double x) Return the value of e raised to the x power double exp2(double x) Return 2 raised to the argument double log(double x) Return natural logarithm of the argument double log2(double x) Return the base-2 logarithm of the argument double log10(double x) Return the base-10 logarithm of the argument double sin(double x) Return sine of the argument double asin(double x) Return inverse sine of the argument double cos(double x) Return cosine of the argument double acos(double x) Return inverse cosine of the argument double tan(double x) Return tangent of the argument double atan(double x) Return inverse tangent the argument int rand32() Return a random 32 bit integer int rand32(int max) Return a random 32 bit integer in [0, max) int rand32(int min, int max) Return a random 32 bit integer in [min, max) int rand64() Return a random 64 bit integer int rand64(int max) Return a random 64 bit integer in [0, max) int rand64(int min, int max) Return a random 64 bit integer in [min, max) String \u00b6 NOTE: Like SQL, nGQL's character index (location) starts at 1 , not like C language from 0 . Function Description int strcasecmp(string a, string b) Compare strings without case sensitivity, when a = b, return 0, when a > b returned value is greater than 0, otherwise less than 0 string lower(string a) Return the argument in lowercase string upper(string a) Return the argument in uppercase int length(string a) Return length (int) of given string in bytes string trim(string a) Remove leading and trailing spaces string ltrim(string a) Remove leading spaces string rtrim(string a) Remove trailing spaces string left(string a, int count) Return the substring in [1, count], if length a is less than count, return a string right(string a, int count) Return the substring in [size - count + 1, size], if length a is less than count, return a string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length string substr(string a, int pos, int count) Extract a substring from a string, starting at the specified position, extract the specified length characters int hash(string a) Encode the data into integer value Explanations on the returns of function substr : If pos is 0, return empty string If the absolute value of pos is greater than the string, return empty string If pos is greater than 0, return substring in [pos, pos + count) If pos is less than 0, and set position N as length(a) + pos + 1, return substring in [N, N + count) If count is greater than length(a), return the whole string Timestamp \u00b6 Function Description int now() Return the current date and time","title":"Built in Functions"},{"location":"manual-EN/2.query-language/2.functions-and-operators/built-in-functions/#built-in_functions","text":"Nebula Graph supports calling built-in functions of the following types:","title":"Built-in Functions"},{"location":"manual-EN/2.query-language/2.functions-and-operators/built-in-functions/#math","text":"Function Description double abs(double x) Return absolute value of the argument double floor(double x) Return the largest integer value smaller than or equal to the argument. (Rounds down) double ceil(double x) Return the smallest integer greater than or equal to the argument. (Rounds up) double round(double x) Return integral value nearest to the argument, returns a number farther away from 0 if the parameter is in the middle double sqrt(double x) Return the square root of the argument double cbrt(double x) Return the cubic root of the argument double hypot(double x, double x) Return the hypotenuse of a right-angled triangle double pow(double x, double y) Compute the power of the argument double exp(double x) Return the value of e raised to the x power double exp2(double x) Return 2 raised to the argument double log(double x) Return natural logarithm of the argument double log2(double x) Return the base-2 logarithm of the argument double log10(double x) Return the base-10 logarithm of the argument double sin(double x) Return sine of the argument double asin(double x) Return inverse sine of the argument double cos(double x) Return cosine of the argument double acos(double x) Return inverse cosine of the argument double tan(double x) Return tangent of the argument double atan(double x) Return inverse tangent the argument int rand32() Return a random 32 bit integer int rand32(int max) Return a random 32 bit integer in [0, max) int rand32(int min, int max) Return a random 32 bit integer in [min, max) int rand64() Return a random 64 bit integer int rand64(int max) Return a random 64 bit integer in [0, max) int rand64(int min, int max) Return a random 64 bit integer in [min, max)","title":"Math"},{"location":"manual-EN/2.query-language/2.functions-and-operators/built-in-functions/#string","text":"NOTE: Like SQL, nGQL's character index (location) starts at 1 , not like C language from 0 . Function Description int strcasecmp(string a, string b) Compare strings without case sensitivity, when a = b, return 0, when a > b returned value is greater than 0, otherwise less than 0 string lower(string a) Return the argument in lowercase string upper(string a) Return the argument in uppercase int length(string a) Return length (int) of given string in bytes string trim(string a) Remove leading and trailing spaces string ltrim(string a) Remove leading spaces string rtrim(string a) Remove trailing spaces string left(string a, int count) Return the substring in [1, count], if length a is less than count, return a string right(string a, int count) Return the substring in [size - count + 1, size], if length a is less than count, return a string lpad(string a, int size, string letters) Left-pads a string with another string to a certain length string rpad(string a, int size, string letters) Reft-pads a string with another string to a certain length string substr(string a, int pos, int count) Extract a substring from a string, starting at the specified position, extract the specified length characters int hash(string a) Encode the data into integer value Explanations on the returns of function substr : If pos is 0, return empty string If the absolute value of pos is greater than the string, return empty string If pos is greater than 0, return substring in [pos, pos + count) If pos is less than 0, and set position N as length(a) + pos + 1, return substring in [N, N + count) If count is greater than length(a), return the whole string","title":"String"},{"location":"manual-EN/2.query-language/2.functions-and-operators/built-in-functions/#timestamp","text":"Function Description int now() Return the current date and time","title":"Timestamp"},{"location":"manual-EN/2.query-language/2.functions-and-operators/comparison-functions-and-operators/","text":"Comparison Functions and Operators \u00b6 Name Description = Assign a value / Division operator == Equal operator != Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument udf_is_in() Whether a value is within a set of values Comparison operations result in a value of true and false . == Equal. String comparisons are case-sensitive. Values of different types are not equal. nebula> YIELD 'A' == 'a'; ============== | (\"A\"==\"a\") | ============== | false | -------------- nebula> YIELD '2' == 2; [ERROR (-8)]: A string type can not be compared with a non-string type. > Greater than\uff1a nebula> YIELD 3 > 2; ========= | (3>2) | ========= | true | --------- \u2265 Greater than or equal to: nebula> YIELD 2 >= 2; [ERROR (-8)]: A string type can not be compared with a non-string type. < Less than: nebula> YIELD 2.0 < 1.9; ======================= | (2.000000<1.900000) | ======================= |false | ----------------------- \u2264 Less than or equal to: nebula> YIELD 0.11 <= 0.11; ======================== | (0.110000<=0.110000) | ======================== |true | ------------------------ != Not equal: nebula> YIELD 1 != '1'; A string type can not be compared with a non-string type. udf_is_in() Returns true if the first value is equal to any of the values in the list, otherwise, returns false. nebula> YIELD udf_is_in(1,0,1,2); ====================== | udf_is_in(1,0,1,2) | ====================== | true | ---------------------- nebula> GO FROM 100 OVER follow WHERE udf_is_in($$.player.name, \"Tony Parker\"); /* This example might not work because udf_is_in might be changed in the future.*/ =============== | follow._dst | =============== | 101 | --------------- nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | GO FROM $-.id OVER follow WHERE udf_is_in($-.id, 102, 102 + 1); =============== | follow._dst | =============== | 100 | --------------- | 101 | ---------------","title":"Comparison Functions and Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/comparison-functions-and-operators/#comparison_functions_and_operators","text":"Name Description = Assign a value / Division operator == Equal operator != Not equal operator < Less than operator <= Less than or equal operator - Minus operator % Modulo operator + Addition operator * Multiplication operator - Change the sign of the argument udf_is_in() Whether a value is within a set of values Comparison operations result in a value of true and false . == Equal. String comparisons are case-sensitive. Values of different types are not equal. nebula> YIELD 'A' == 'a'; ============== | (\"A\"==\"a\") | ============== | false | -------------- nebula> YIELD '2' == 2; [ERROR (-8)]: A string type can not be compared with a non-string type. > Greater than\uff1a nebula> YIELD 3 > 2; ========= | (3>2) | ========= | true | --------- \u2265 Greater than or equal to: nebula> YIELD 2 >= 2; [ERROR (-8)]: A string type can not be compared with a non-string type. < Less than: nebula> YIELD 2.0 < 1.9; ======================= | (2.000000<1.900000) | ======================= |false | ----------------------- \u2264 Less than or equal to: nebula> YIELD 0.11 <= 0.11; ======================== | (0.110000<=0.110000) | ======================== |true | ------------------------ != Not equal: nebula> YIELD 1 != '1'; A string type can not be compared with a non-string type. udf_is_in() Returns true if the first value is equal to any of the values in the list, otherwise, returns false. nebula> YIELD udf_is_in(1,0,1,2); ====================== | udf_is_in(1,0,1,2) | ====================== | true | ---------------------- nebula> GO FROM 100 OVER follow WHERE udf_is_in($$.player.name, \"Tony Parker\"); /* This example might not work because udf_is_in might be changed in the future.*/ =============== | follow._dst | =============== | 101 | --------------- nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | GO FROM $-.id OVER follow WHERE udf_is_in($-.id, 102, 102 + 1); =============== | follow._dst | =============== | 100 | --------------- | 101 | ---------------","title":"Comparison Functions and Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/group-by-function/","text":"Aggregate (Group By) Function \u00b6 The GROUP BY functions are similar with SQL. It can only be applied in the YIELD syntax. Name Description AVG() Return the average value of the argument COUNT() Return the number of records COUNT_DISTINCT()) Return the number of different values MAX() Return the maximum value MIN() Return the minimum value STD() Return the population standard deviation SUM() Return the sum BIT_AND() Bitwise AND BIT_OR() Bitwise OR BIT_XOR() Bitwise exclusive OR (XOR) All the functions above only work with int64 and double. Example \u00b6 nebula> GO FROM 100 OVER follow YIELD $$.player.name as Name | GROUP BY $-.Name YIELD $-.Name, COUNT(*); -- Find all the players followed by vertex 100 and return their names as Name. These players are grouped by Name and the number in each group is counted. -- The following result is returned: ================================ | $-.Name | COUNT(*) | ================================ | Kyle Anderson | 1 | -------------------------------- | Tony Parker | 1 | -------------------------------- | LaMarcus Aldridge | 1 | -------------------------------- nebula> GO FROM 101 OVER follow YIELD follow._src AS player, follow.degree AS degree | GROUP BY $-.player YIELD SUM($-.degree); -- Find all the players followed by vertex 101, return these players as player and the property of the follow edge as degree. These players are grouped and the sum of their degree values is returned. -- The following result is returned: ================== | SUM($-.degree) | ================== | 186 | ------------------","title":"Aggregate GROUP BY Functions"},{"location":"manual-EN/2.query-language/2.functions-and-operators/group-by-function/#aggregate_group_by_function","text":"The GROUP BY functions are similar with SQL. It can only be applied in the YIELD syntax. Name Description AVG() Return the average value of the argument COUNT() Return the number of records COUNT_DISTINCT()) Return the number of different values MAX() Return the maximum value MIN() Return the minimum value STD() Return the population standard deviation SUM() Return the sum BIT_AND() Bitwise AND BIT_OR() Bitwise OR BIT_XOR() Bitwise exclusive OR (XOR) All the functions above only work with int64 and double.","title":"Aggregate (Group By) Function"},{"location":"manual-EN/2.query-language/2.functions-and-operators/group-by-function/#example","text":"nebula> GO FROM 100 OVER follow YIELD $$.player.name as Name | GROUP BY $-.Name YIELD $-.Name, COUNT(*); -- Find all the players followed by vertex 100 and return their names as Name. These players are grouped by Name and the number in each group is counted. -- The following result is returned: ================================ | $-.Name | COUNT(*) | ================================ | Kyle Anderson | 1 | -------------------------------- | Tony Parker | 1 | -------------------------------- | LaMarcus Aldridge | 1 | -------------------------------- nebula> GO FROM 101 OVER follow YIELD follow._src AS player, follow.degree AS degree | GROUP BY $-.player YIELD SUM($-.degree); -- Find all the players followed by vertex 101, return these players as player and the property of the follow edge as degree. These players are grouped and the sum of their degree values is returned. -- The following result is returned: ================== | SUM($-.degree) | ================== | 186 | ------------------","title":"Example"},{"location":"manual-EN/2.query-language/2.functions-and-operators/limit-syntax/","text":"LIMIT Syntax \u00b6 LIMIT works the same as in SQL , and must be used with pipe | . The LIMIT clause accepts one or two arguments. The values of both arguments must be zero or positive integers. ORDER BY <expressions> [ASC | DESC] LIMIT [<offset_value>,] <number_rows> expressions The columns or calculations that you wish to sort. number_rows It constrains the number of rows to return. For example, LIMIT 10 would return the first 10 rows. This is where sorting order matters so be sure to use an ORDER BY clause appropriately. offset_value Optional. It defines from which row to start including the rows in the output. The offset starts from zero. When using LIMIT , it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output. For example: nebula> GO FROM 200 OVER serve REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY Age, Friend | LIMIT 3; ========================= | Friend | Age | ========================= | Kyle Anderson | 25 | ------------------------- | Aron Baynes | 32 | ------------------------- | Marco Belinelli | 32 |","title":"LIMIT Syntax"},{"location":"manual-EN/2.query-language/2.functions-and-operators/limit-syntax/#limit_syntax","text":"LIMIT works the same as in SQL , and must be used with pipe | . The LIMIT clause accepts one or two arguments. The values of both arguments must be zero or positive integers. ORDER BY <expressions> [ASC | DESC] LIMIT [<offset_value>,] <number_rows> expressions The columns or calculations that you wish to sort. number_rows It constrains the number of rows to return. For example, LIMIT 10 would return the first 10 rows. This is where sorting order matters so be sure to use an ORDER BY clause appropriately. offset_value Optional. It defines from which row to start including the rows in the output. The offset starts from zero. When using LIMIT , it is important to use an ORDER BY clause that constrains the output into a unique order. Otherwise, you will get an unpredictable subset of the output. For example: nebula> GO FROM 200 OVER serve REVERSELY YIELD $$.player.name AS Friend, $$.player.age AS Age | ORDER BY Age, Friend | LIMIT 3; ========================= | Friend | Age | ========================= | Kyle Anderson | 25 | ------------------------- | Aron Baynes | 32 | ------------------------- | Marco Belinelli | 32 |","title":"LIMIT Syntax"},{"location":"manual-EN/2.query-language/2.functions-and-operators/logical-operators/","text":"Logical Operators \u00b6 Name Description && Logical AND ! Logical NOT || Logical OR XOR Logical XOR In nGQL, non-zero numbers are evaluated to true . For the precedence of the operators, refer to Operator Precedence . && Logical AND: nebula> YIELD -1 && true; ============== | (-1&&true) | ============== | true | -------------- ! Logical NOT: nebula> YIELD !(-1); ========= | !(-1) | ========= | false | --------- || Logical OR: nebula> YIELD 1 || !1; ============= | (1||!(1)) | ============= | true | ------------- ^ Logical XOR: nebula> YIELD (NOT 0 || 0) AND 0 XOR 1 AS ret; ========= | ret | ========= | true | ---------","title":"Logical Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/logical-operators/#logical_operators","text":"Name Description && Logical AND ! Logical NOT || Logical OR XOR Logical XOR In nGQL, non-zero numbers are evaluated to true . For the precedence of the operators, refer to Operator Precedence . && Logical AND: nebula> YIELD -1 && true; ============== | (-1&&true) | ============== | true | -------------- ! Logical NOT: nebula> YIELD !(-1); ========= | !(-1) | ========= | false | --------- || Logical OR: nebula> YIELD 1 || !1; ============= | (1||!(1)) | ============= | true | ------------- ^ Logical XOR: nebula> YIELD (NOT 0 || 0) AND 0 XOR 1 AS ret; ========= | ret | ========= | true | ---------","title":"Logical Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/operator-precedence/","text":"Operator Precedence \u00b6 The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. ! - (unary minus) *, /, % -, + == , >=, >, <=, <, <>, != && || = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses. Examples: nebula> YIELD 2+3*5; nebula> YIELD (2+3)*5;","title":"Operator Precedence"},{"location":"manual-EN/2.query-language/2.functions-and-operators/operator-precedence/#operator_precedence","text":"The following list shows the precedence of nGQL operators in descending order. Operators that are shown together on a line have the same precedence. ! - (unary minus) *, /, % -, + == , >=, >, <=, <, <>, != && || = (assignment) For operators that occur at the same precedence level within an expression, evaluation proceeds left to right, with the exception that assignments evaluate right to left. The precedence of operators determines the order of evaluation of terms in an expression. To override this order and group terms explicitly, use parentheses. Examples: nebula> YIELD 2+3*5; nebula> YIELD (2+3)*5;","title":"Operator Precedence"},{"location":"manual-EN/2.query-language/2.functions-and-operators/order-by-function/","text":"ORDER BY Function \u00b6 Similar with SQL, ORDER BY can be used to sort in ascending ( ASC ) or descending ( DESC ) order for returned results. ORDER BY can only be used in the PIPE -syntax ( | ). ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] By default, ORDER BY sorts the records in ascending order if no ASC or DESC is given. Example \u00b6 nebula> FETCH PROP ON player 100,101,102,103 YIELD player.age AS age, player.name AS name | ORDER BY age, name DESC; -- Fetch four vertices and sort them by their ages in ascending order, and for those in the same age, sort them by name in descending order. -- The following result is returned: ====================================== | VertexID | age | name | ====================================== | 103 | 32 | Rudy Gay | -------------------------------------- | 102 | 33 | LaMarcus Aldridge | -------------------------------------- | 101 | 36 | Tony Parker | -------------------------------------- | 100 | 42 | Tim Duncan | -------------------------------------- (See FETCH for the usage.) nebula> GO FROM 100 OVER follow YIELD $$.player.age AS age, $$.player.name AS name | ORDER BY age DESC, name ASC; -- Search all the players followed by vertex 100 and return their ages and names. The age is in descending order; the name is in ascending order if they have the same name. -- The following result is returned: =========================== | age | name | =========================== | 36 | Tony Parker | --------------------------- | 33 | LaMarcus Aldridge | --------------------------- | 25 | Kyle Anderson | ---------------------------","title":"ORDER BY Function"},{"location":"manual-EN/2.query-language/2.functions-and-operators/order-by-function/#order_by_function","text":"Similar with SQL, ORDER BY can be used to sort in ascending ( ASC ) or descending ( DESC ) order for returned results. ORDER BY can only be used in the PIPE -syntax ( | ). ORDER BY <expression> [ASC | DESC] [, <expression> [ASC | DESC] ...] By default, ORDER BY sorts the records in ascending order if no ASC or DESC is given.","title":"ORDER BY Function"},{"location":"manual-EN/2.query-language/2.functions-and-operators/order-by-function/#example","text":"nebula> FETCH PROP ON player 100,101,102,103 YIELD player.age AS age, player.name AS name | ORDER BY age, name DESC; -- Fetch four vertices and sort them by their ages in ascending order, and for those in the same age, sort them by name in descending order. -- The following result is returned: ====================================== | VertexID | age | name | ====================================== | 103 | 32 | Rudy Gay | -------------------------------------- | 102 | 33 | LaMarcus Aldridge | -------------------------------------- | 101 | 36 | Tony Parker | -------------------------------------- | 100 | 42 | Tim Duncan | -------------------------------------- (See FETCH for the usage.) nebula> GO FROM 100 OVER follow YIELD $$.player.age AS age, $$.player.name AS name | ORDER BY age DESC, name ASC; -- Search all the players followed by vertex 100 and return their ages and names. The age is in descending order; the name is in ascending order if they have the same name. -- The following result is returned: =========================== | age | name | =========================== | 36 | Tony Parker | --------------------------- | 33 | LaMarcus Aldridge | --------------------------- | 25 | Kyle Anderson | ---------------------------","title":"Example"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/","text":"Set Operations ( UNION , INTERSECT , and MINUS ) \u00b6 UNION, UNION DISTINCT, and UNION ALL \u00b6 Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B (denoted by A \u22c3 B in mathematics), with the distinct element belongs to set A or set B, or both. Meanwhile, operation UNION ALL returns the union set with duplicated elements. The UNION syntax is <left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] where <left> and <right> must have the same number of columns and pair-wise data types. If the data types are different, Nebula Graph will convert according to Type Conversion . Example \u00b6 The following statement nebula> GO FROM 1 OVER e1 \\ UNION \\ GO FROM 2 OVER e1 returns the neighbors' id of vertex 1 and 2 (along with edge e1 ) without duplication. While nebula> GO FROM 1 OVER e1 \\ UNION ALL\\ GO FROM 2 OVER e1 returns all the neighbors of vertex 1 and 2 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 -- query 1 ========================== | id | left_1 | left_2 | ========================== | 104 | 1 | 2 | -- line 1 -------------------------- | 215 | 4 | 3 | -- line 3 -------------------------- nebula> GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 -- query 2 =========================== | id | right_1 | right_2 | =========================== | 104 | 1 | 2 | -- line 1 --------------------------- | 104 | 2 | 2 | -- line 2 --------------------------- And the following statement nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 \\ UNION /* DISTINCT */ \\ GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 will return as follows: ========================= | id | left_1 | left_2 | -- UNION or UNION DISTINCT. The column names come from query 1 ========================= | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 2 | 2 | -- line 2 ------------------------- | 215 | 4 | 3 | -- line 3 ------------------------- Notice that line 1 and line 2 return the same id (104) with different column values. The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different. You can expect the UNION ALL result nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 \\ UNION ALL \\ GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 ========================= | id | left_1 | left_2 | -- UNION ALL ========================= | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 2 | 2 | -- line 2 ------------------------- | 215 | 4 | 3 | -- line 3 ------------------------- INTERSECT \u00b6 Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B), if the elements belong both to set A and set B. <left> INTERSECT <right> Alike UNION , <left> and <right> must have the same number of columns and data types. Besides, only the same line of <left> and <right> will be returned. For example, the following query nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 INTERSECT GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 returns ========================= | id | left_1 | left_2 | ========================= | 104 | 1 | 2 | -- line 1 ------------------------- MINUS \u00b6 The set subtraction (or difference), A - B, consists of elements that are in A but not in B. So the operation order matters. For example, the following query nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 MINUS GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 comes out ========================== | id | left_1 | left_2 | ========================== | 215 | 4 | 3 | -- line 3 -------------------------- And if we reverse the MINUS order, the query nebula> GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 MINUS GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 returns =========================== | id | right_1 | right_2 | -- column named from query 2 =========================== | 104 | 2 | 2 | -- line 2 ---------------------------","title":"Set Operations"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/#set_operations_union_intersect_and_minus","text":"","title":"Set Operations (UNION, INTERSECT, and MINUS)"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/#union_union_distinct_and_union_all","text":"Operator UNION DISTINCT (or by short UNION ) returns the union of two sets A and B (denoted by A \u22c3 B in mathematics), with the distinct element belongs to set A or set B, or both. Meanwhile, operation UNION ALL returns the union set with duplicated elements. The UNION syntax is <left> UNION [DISTINCT | ALL] <right> [ UNION [DISTINCT | ALL] <right> ...] where <left> and <right> must have the same number of columns and pair-wise data types. If the data types are different, Nebula Graph will convert according to Type Conversion .","title":"UNION, UNION DISTINCT, and UNION ALL"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/#example","text":"The following statement nebula> GO FROM 1 OVER e1 \\ UNION \\ GO FROM 2 OVER e1 returns the neighbors' id of vertex 1 and 2 (along with edge e1 ) without duplication. While nebula> GO FROM 1 OVER e1 \\ UNION ALL\\ GO FROM 2 OVER e1 returns all the neighbors of vertex 1 and 2 , with all possible duplications. UNION can also work with the YIELD statement. For example, let's suppose the results of the following two queries. nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 -- query 1 ========================== | id | left_1 | left_2 | ========================== | 104 | 1 | 2 | -- line 1 -------------------------- | 215 | 4 | 3 | -- line 3 -------------------------- nebula> GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 -- query 2 =========================== | id | right_1 | right_2 | =========================== | 104 | 1 | 2 | -- line 1 --------------------------- | 104 | 2 | 2 | -- line 2 --------------------------- And the following statement nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 \\ UNION /* DISTINCT */ \\ GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 will return as follows: ========================= | id | left_1 | left_2 | -- UNION or UNION DISTINCT. The column names come from query 1 ========================= | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 2 | 2 | -- line 2 ------------------------- | 215 | 4 | 3 | -- line 3 ------------------------- Notice that line 1 and line 2 return the same id (104) with different column values. The DISTINCT check duplication by all the columns for every line. So line 1 and line 2 are different. You can expect the UNION ALL result nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 \\ UNION ALL \\ GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 ========================= | id | left_1 | left_2 | -- UNION ALL ========================= | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 1 | 2 | -- line 1 ------------------------- | 104 | 2 | 2 | -- line 2 ------------------------- | 215 | 4 | 3 | -- line 3 -------------------------","title":"Example"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/#intersect","text":"Operator INTERSECT returns the intersection of two sets A and B (denoted by A \u22c2 B), if the elements belong both to set A and set B. <left> INTERSECT <right> Alike UNION , <left> and <right> must have the same number of columns and data types. Besides, only the same line of <left> and <right> will be returned. For example, the following query nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 INTERSECT GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 returns ========================= | id | left_1 | left_2 | ========================= | 104 | 1 | 2 | -- line 1 -------------------------","title":"INTERSECT"},{"location":"manual-EN/2.query-language/2.functions-and-operators/set-operations/#minus","text":"The set subtraction (or difference), A - B, consists of elements that are in A but not in B. So the operation order matters. For example, the following query nebula> GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 MINUS GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 comes out ========================== | id | left_1 | left_2 | ========================== | 215 | 4 | 3 | -- line 3 -------------------------- And if we reverse the MINUS order, the query nebula> GO FROM 2,3 OVER e1 YIELD e1._dst AS id, e1.prop1 AS right_1, $$.tag.prop2 AS right_2 MINUS GO FROM 1 OVER e1 YIELD e1._dst AS id, e1.prop1 AS left_1, $$.tag.prop2 AS left_2 returns =========================== | id | right_1 | right_2 | -- column named from query 2 =========================== | 104 | 2 | 2 | -- line 2 ---------------------------","title":"MINUS"},{"location":"manual-EN/2.query-language/2.functions-and-operators/string-comparison-functions-and-operators/","text":"String Comparison Functions and Operators \u00b6 Name Description CONTAINS Perform case-sensitive inclusion searching in strings CONTAINS The CONTAINS operator is used to perform case-sensitive matching regardless of location within a string. The CONTAINS operator requires string type in both left and right side. nebula> GO FROM 107 OVER serve WHERE $$.team.name CONTAINS \"riors\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2001 | 2009 | Warriors | --------------------------------------------------------------------- nebula> GO FROM 107 OVER serve WHERE $$.team.name CONTAINS \"Riors\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; -- The follow query returns nothing. nebula> GO FROM 107 OVER serve WHERE (STRING)serve.start_year CONTAINS \"07\" && \\ $^.player.name CONTAINS \"Aron\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2007 | 2010 | Nuggets | --------------------------------------------------------------------- nebula> GO FROM 107 OVER serve WHERE !($$.team.name CONTAINS \"riors\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2007 | 2010 | Nuggets | ---------------------------------------------------------------------","title":"String Comparison Functions and Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/string-comparison-functions-and-operators/#string_comparison_functions_and_operators","text":"Name Description CONTAINS Perform case-sensitive inclusion searching in strings CONTAINS The CONTAINS operator is used to perform case-sensitive matching regardless of location within a string. The CONTAINS operator requires string type in both left and right side. nebula> GO FROM 107 OVER serve WHERE $$.team.name CONTAINS \"riors\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2001 | 2009 | Warriors | --------------------------------------------------------------------- nebula> GO FROM 107 OVER serve WHERE $$.team.name CONTAINS \"Riors\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; -- The follow query returns nothing. nebula> GO FROM 107 OVER serve WHERE (STRING)serve.start_year CONTAINS \"07\" && \\ $^.player.name CONTAINS \"Aron\" \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2007 | 2010 | Nuggets | --------------------------------------------------------------------- nebula> GO FROM 107 OVER serve WHERE !($$.team.name CONTAINS \"riors\") \\ YIELD $^.player.name, serve.start_year, serve.end_year, $$.team.name; ===================================================================== | $^.player.name | serve.start_year | serve.end_year | $$.team.name | ===================================================================== | Aron Baynes | 2007 | 2010 | Nuggets | ---------------------------------------------------------------------","title":"String Comparison Functions and Operators"},{"location":"manual-EN/2.query-language/2.functions-and-operators/uuid/","text":"UUID \u00b6 UUID is used to generate the global unique identifiers. When the number of vertices reaches billions, using Hash Function to generate vids has a certain conflict probability. Therefore, Nebula Graph provides UUID Function to avoid vid conflicts in a large number of vertices. UUID Function is composed of the Murmur hash function and the current timestamp (in seconds). Values generated with the UUID are stored in the Nebula Graph Storage service in key-value mode. When called, it checks whether this key exists or conflicts. So the performance may be slower than hash. Insert with UUID : -- Insert a vertex with the UUID function. nebula> INSERT VERTEX player (name, age) VALUES uuid(\"n0\"):(\"n0\", 21); -- Insert an edge with the UUID function. nebula> INSERT EDGE follow(degree) VALUES uuid(\"n0\") -> uuid(\"n1\"): (90); Fetch with UUID : nebula> FETCH PROP ON player uuid(\"n0\") YIELD player.name, player.age; -- The following result is returned: =================================================== | VertexID | player.name | player.age | =================================================== | -5057115778034027261 | n0 | 21 | --------------------------------------------------- nebula> FETCH PROP ON follow uuid(\"n0\") -> uuid(\"n1\"); -- The following result is returned: ============================================================================= | follow._src | follow._dst | follow._rank | follow.degree | ============================================================================= | -5057115778034027261 | 4039977434270020867 | 0 | 90 | ----------------------------------------------------------------------------- Go with UUID : nebula> GO FROM uuid(\"n0\") OVER follow; --The following result is returned: ======================= | follow._dst | ======================= | 4039977434270020867 | -----------------------","title":"UUID Function"},{"location":"manual-EN/2.query-language/2.functions-and-operators/uuid/#uuid","text":"UUID is used to generate the global unique identifiers. When the number of vertices reaches billions, using Hash Function to generate vids has a certain conflict probability. Therefore, Nebula Graph provides UUID Function to avoid vid conflicts in a large number of vertices. UUID Function is composed of the Murmur hash function and the current timestamp (in seconds). Values generated with the UUID are stored in the Nebula Graph Storage service in key-value mode. When called, it checks whether this key exists or conflicts. So the performance may be slower than hash. Insert with UUID : -- Insert a vertex with the UUID function. nebula> INSERT VERTEX player (name, age) VALUES uuid(\"n0\"):(\"n0\", 21); -- Insert an edge with the UUID function. nebula> INSERT EDGE follow(degree) VALUES uuid(\"n0\") -> uuid(\"n1\"): (90); Fetch with UUID : nebula> FETCH PROP ON player uuid(\"n0\") YIELD player.name, player.age; -- The following result is returned: =================================================== | VertexID | player.name | player.age | =================================================== | -5057115778034027261 | n0 | 21 | --------------------------------------------------- nebula> FETCH PROP ON follow uuid(\"n0\") -> uuid(\"n1\"); -- The following result is returned: ============================================================================= | follow._src | follow._dst | follow._rank | follow.degree | ============================================================================= | -5057115778034027261 | 4039977434270020867 | 0 | 90 | ----------------------------------------------------------------------------- Go with UUID : nebula> GO FROM uuid(\"n0\") OVER follow; --The following result is returned: ======================= | follow._dst | ======================= | 4039977434270020867 | -----------------------","title":"UUID"},{"location":"manual-EN/2.query-language/3.language-structure/comment-syntax/","text":"Comment Syntax \u00b6 Nebula Graph supports four comment styles: From a # character to the end of the line. From a -- sequence to the end of the line. From a // sequence to the end of the line, as in the C programming language. From a / sequence to the following / sequence. This syntax enables a comment to extend over multiple lines because the beginning and closing sequences need not be on the same line. Nested comments are not supported. The following example demonstrates all these comment styles: nebula> -- Do nothing this line nebula> YIELD 1+1 # This comment continues to the end of line nebula> YIELD 1+1 -- This comment continues to the end of line nebula> YIELD 1+1 // This comment continues to the end of line nebula> YIELD 1 /* This is an in-line comment */ + 1 nebula> YIELD 11 + \\ /* Multiple-line comment \\ Use backslash as line break. \\ */ 12 The backslash \\ in a line indicates a line break.","title":"Comment Syntax"},{"location":"manual-EN/2.query-language/3.language-structure/comment-syntax/#comment_syntax","text":"Nebula Graph supports four comment styles: From a # character to the end of the line. From a -- sequence to the end of the line. From a // sequence to the end of the line, as in the C programming language. From a / sequence to the following / sequence. This syntax enables a comment to extend over multiple lines because the beginning and closing sequences need not be on the same line. Nested comments are not supported. The following example demonstrates all these comment styles: nebula> -- Do nothing this line nebula> YIELD 1+1 # This comment continues to the end of line nebula> YIELD 1+1 -- This comment continues to the end of line nebula> YIELD 1+1 // This comment continues to the end of line nebula> YIELD 1 /* This is an in-line comment */ + 1 nebula> YIELD 11 + \\ /* Multiple-line comment \\ Use backslash as line break. \\ */ 12 The backslash \\ in a line indicates a line break.","title":"Comment Syntax"},{"location":"manual-EN/2.query-language/3.language-structure/identifier-case-sensitivity/","text":"Identifer Case Sensitivity \u00b6 Identifiers are Case-Sensitive \u00b6 The following statements would not work because they refer to two different spaces, i.e. my_space and MY_SPACE : nebula> CREATE SPACE my_space; nebula> use MY_SPACE; -- my_space and MY_SPACE are two different spaces Keywords and Reserved Words are Case-Insensitive \u00b6 The following statements are equivalent: nebula> show spaces; -- show and spaces are keywords. nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES;","title":"Identifer Case Sensitivity"},{"location":"manual-EN/2.query-language/3.language-structure/identifier-case-sensitivity/#identifer_case_sensitivity","text":"","title":"Identifer Case Sensitivity"},{"location":"manual-EN/2.query-language/3.language-structure/identifier-case-sensitivity/#identifiers_are_case-sensitive","text":"The following statements would not work because they refer to two different spaces, i.e. my_space and MY_SPACE : nebula> CREATE SPACE my_space; nebula> use MY_SPACE; -- my_space and MY_SPACE are two different spaces","title":"Identifiers are Case-Sensitive"},{"location":"manual-EN/2.query-language/3.language-structure/identifier-case-sensitivity/#keywords_and_reserved_words_are_case-insensitive","text":"The following statements are equivalent: nebula> show spaces; -- show and spaces are keywords. nebula> SHOW SPACES; nebula> SHOW spaces; nebula> show SPACES;","title":"Keywords and Reserved Words are Case-Insensitive"},{"location":"manual-EN/2.query-language/3.language-structure/keywords-and-reserved-words/","text":"Keywords and Reserved Words \u00b6 Keywords are words that have significance in nGQL. Certain keywords are reserved and require special treatment for use as identifiers. Non-reserved keywords are permitted as identifiers without quoting. All the non-reserved keywords are automatically converted to lower case. Non-reserved keywords are case-insensitive. Reserved words are permitted as identifiers if you quote them with back quotes such as `AND`. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' nebula> CREATE TAG SPACE(name string); -- SPACE is an unreserved KEY WORD Execution succeeded nebula> SHOW TAGS; -- All the non-reserved keywords are automatically converted to lower case. ============= | ID | Name | ============= | 25 | space| ------------- TAG is a reserved keyword and must be quoted with backtick to be used as an identifier. SPACE is keyword but not reserved, so its use as identifiers does not require quoting. nebula> CREATE TAG `TAG` (name string); -- TAG is a reserved word here Execution succeeded Reserved Words \u00b6 The following list shows reserved words in nGQL. ADD ALTER AND AS ASC BALANCE BIGINT BOOL BY CHANGE COMPACT CREATE DELETE DESC DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS FETCH FIND FLUSH FROM GET GO GRANT IF IN INDEX INDEXES INGEST INSERT INT INTERSECT IS LIMIT LOOKUP MATCH MINUS NO NOT NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEPS STOP STRING SUBMIT TAG TAGS TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX WHEN WHERE WITH XOR YIELD Non-Reserved Keywords \u00b6 ACCOUNT ADMIN ALL AVG BIDIRECT BIT_AND BIT_OR BIT_XOR CHARSET COLLATE COLLATION CONFIGS COUNT COUNT_DISTINCT DATA DBA DEFAULT FORCE GOD GRAPH GROUP GUEST HDFS HOSTS JOB JOBS LEADER MAX META MIN OFFLINE PART PARTITION_NUM PARTS PASSWORD PATH REPLICA_FACTOR ROLE ROLES SHORTEST SNAPSHOT SNAPSHOTS SPACE SPACES STATUS STD STORAGE SUM TTL_COL TTL_DURATION USER USERS UUID VALUES","title":"Keywords and Reserved Words"},{"location":"manual-EN/2.query-language/3.language-structure/keywords-and-reserved-words/#keywords_and_reserved_words","text":"Keywords are words that have significance in nGQL. Certain keywords are reserved and require special treatment for use as identifiers. Non-reserved keywords are permitted as identifiers without quoting. All the non-reserved keywords are automatically converted to lower case. Non-reserved keywords are case-insensitive. Reserved words are permitted as identifiers if you quote them with back quotes such as `AND`. nebula> CREATE TAG TAG(name string); [ERROR (-7)]: SyntaxError: syntax error near `TAG' nebula> CREATE TAG SPACE(name string); -- SPACE is an unreserved KEY WORD Execution succeeded nebula> SHOW TAGS; -- All the non-reserved keywords are automatically converted to lower case. ============= | ID | Name | ============= | 25 | space| ------------- TAG is a reserved keyword and must be quoted with backtick to be used as an identifier. SPACE is keyword but not reserved, so its use as identifiers does not require quoting. nebula> CREATE TAG `TAG` (name string); -- TAG is a reserved word here Execution succeeded","title":"Keywords and Reserved Words"},{"location":"manual-EN/2.query-language/3.language-structure/keywords-and-reserved-words/#reserved_words","text":"The following list shows reserved words in nGQL. ADD ALTER AND AS ASC BALANCE BIGINT BOOL BY CHANGE COMPACT CREATE DELETE DESC DESCRIBE DISTINCT DOUBLE DOWNLOAD DROP EDGE EDGES EXISTS FETCH FIND FLUSH FROM GET GO GRANT IF IN INDEX INDEXES INGEST INSERT INT INTERSECT IS LIMIT LOOKUP MATCH MINUS NO NOT NULL OF OFFSET ON OR ORDER OVER OVERWRITE PROP REBUILD RECOVER REMOVE RETURN REVERSELY REVOKE SET SHOW STEPS STOP STRING SUBMIT TAG TAGS TIMESTAMP TO UNION UPDATE UPSERT UPTO USE VERTEX WHEN WHERE WITH XOR YIELD","title":"Reserved Words"},{"location":"manual-EN/2.query-language/3.language-structure/keywords-and-reserved-words/#non-reserved_keywords","text":"ACCOUNT ADMIN ALL AVG BIDIRECT BIT_AND BIT_OR BIT_XOR CHARSET COLLATE COLLATION CONFIGS COUNT COUNT_DISTINCT DATA DBA DEFAULT FORCE GOD GRAPH GROUP GUEST HDFS HOSTS JOB JOBS LEADER MAX META MIN OFFLINE PART PARTITION_NUM PARTS PASSWORD PATH REPLICA_FACTOR ROLE ROLES SHORTEST SNAPSHOT SNAPSHOTS SPACE SPACES STATUS STD STORAGE SUM TTL_COL TTL_DURATION USER USERS UUID VALUES","title":"Non-Reserved Keywords"},{"location":"manual-EN/2.query-language/3.language-structure/pipe-syntax/","text":"PIPE Syntax \u00b6 One major difference between nGQL and SQL is how sub-queries are composed. In SQL, sub-queries are nested (embedded) to form a statement. Meanwhile, nGQL uses shell style PIPE (|) . Examples \u00b6 nebula> GO FROM 100 OVER follow YIELD follow._dst AS dstid, $$.player.name AS Name | \\ GO FROM $-.dstid OVER follow YIELD follow._dst, follow.degree, $-.Name The dest (vertex) id will be given as the default value if no YIELD is used. But if YIELD is declared explicitly, (the default value) id will not be given. The alias name mentioned right after placeholder $-. must be defined in the previews YIELD statement, such as dstid or Name as shown in the above example.","title":"Pipe Syntax"},{"location":"manual-EN/2.query-language/3.language-structure/pipe-syntax/#pipe_syntax","text":"One major difference between nGQL and SQL is how sub-queries are composed. In SQL, sub-queries are nested (embedded) to form a statement. Meanwhile, nGQL uses shell style PIPE (|) .","title":"PIPE Syntax"},{"location":"manual-EN/2.query-language/3.language-structure/pipe-syntax/#examples","text":"nebula> GO FROM 100 OVER follow YIELD follow._dst AS dstid, $$.player.name AS Name | \\ GO FROM $-.dstid OVER follow YIELD follow._dst, follow.degree, $-.Name The dest (vertex) id will be given as the default value if no YIELD is used. But if YIELD is declared explicitly, (the default value) id will not be given. The alias name mentioned right after placeholder $-. must be defined in the previews YIELD statement, such as dstid or Name as shown in the above example.","title":"Examples"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/","text":"Property Reference \u00b6 You can refer a vertex or edge's property in WHERE or YIELD syntax. Reference From Vertex \u00b6 For Source Vertex \u00b6 $^.tag_name.prop_name where symbol $^ is used to get a source vertex's property, tag_name indicates the source vertex's tag , and prop_name specifies the property name. For Destination Vertex \u00b6 $$.tag_name.prop_name Symbol $$ indicates the ending vertex, tag_name and prop_name are the vertex's tag and property respectively. Example \u00b6 nebula> GO FROM 100 OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; Use the above query to get the source vertex's property name and ending vertex's property age. Reference From Edge \u00b6 For Property \u00b6 You can use the following syntax to get an edge's property. edge_type.edge_prop edge_type is the edge's type, meanwhile edge_prop is the property. For example, nebula> GO FROM 100 OVER follow YIELD follow.degree; For Built-in Properties \u00b6 There are four built-in properties in the edge: _src: source vertex ID of the edge _dst: destination ID of the edge _type: edge type _ranking: the edge's ranking You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path. For example, nebula> GO FROM 100 OVER follow YIELD follow._src AS startVID /* starting vertex is 100 */, follow._dst AS endVID; This statement returns all the neighbors of vertex 100 over edge type follow , by referencing follow._src as the starting vertex ID (which, of course, is 100 ) and follow._dst as the ending vertex ID.","title":"Property Reference"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#property_reference","text":"You can refer a vertex or edge's property in WHERE or YIELD syntax.","title":"Property Reference"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#reference_from_vertex","text":"","title":"Reference From Vertex"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#for_source_vertex","text":"$^.tag_name.prop_name where symbol $^ is used to get a source vertex's property, tag_name indicates the source vertex's tag , and prop_name specifies the property name.","title":"For Source Vertex"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#for_destination_vertex","text":"$$.tag_name.prop_name Symbol $$ indicates the ending vertex, tag_name and prop_name are the vertex's tag and property respectively.","title":"For Destination Vertex"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#example","text":"nebula> GO FROM 100 OVER follow YIELD $^.player.name AS startName, $$.player.age AS endAge; Use the above query to get the source vertex's property name and ending vertex's property age.","title":"Example"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#reference_from_edge","text":"","title":"Reference From Edge"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#for_property","text":"You can use the following syntax to get an edge's property. edge_type.edge_prop edge_type is the edge's type, meanwhile edge_prop is the property. For example, nebula> GO FROM 100 OVER follow YIELD follow.degree;","title":"For Property"},{"location":"manual-EN/2.query-language/3.language-structure/property-reference/#for_built-in_properties","text":"There are four built-in properties in the edge: _src: source vertex ID of the edge _dst: destination ID of the edge _type: edge type _ranking: the edge's ranking You can use _src and _dst to get the starting and ending vertices' ID, and they are very commonly used to show a graph path. For example, nebula> GO FROM 100 OVER follow YIELD follow._src AS startVID /* starting vertex is 100 */, follow._dst AS endVID; This statement returns all the neighbors of vertex 100 over edge type follow , by referencing follow._src as the starting vertex ID (which, of course, is 100 ) and follow._dst as the ending vertex ID.","title":"For Built-in Properties"},{"location":"manual-EN/2.query-language/3.language-structure/schema-object-names/","text":"Schema Object Names \u00b6 Certain objects within Nebula graph , including space, tag, edge, alias, customer variables and other object names are referred as identifiers. This section describes the rules for identifiers in Nebula Graph : Permitted characters in identifiers: ASCII: [0-9,a-z,A-Z,_] (basic Latin letters, digits 0-9, underscore), other punctuation characters are not supported. All identifiers must begin with a letter of the alphabet. Identifiers are case sensitive. You cannot use a keyword (a reserved word) as an identifier.","title":"Schema Object Names"},{"location":"manual-EN/2.query-language/3.language-structure/schema-object-names/#schema_object_names","text":"Certain objects within Nebula graph , including space, tag, edge, alias, customer variables and other object names are referred as identifiers. This section describes the rules for identifiers in Nebula Graph : Permitted characters in identifiers: ASCII: [0-9,a-z,A-Z,_] (basic Latin letters, digits 0-9, underscore), other punctuation characters are not supported. All identifiers must begin with a letter of the alphabet. Identifiers are case sensitive. You cannot use a keyword (a reserved word) as an identifier.","title":"Schema Object Names"},{"location":"manual-EN/2.query-language/3.language-structure/statement-composition/","text":"Statement Composition \u00b6 There are only two ways to compose statements (or sub-queries): More than one statements can be batched together, separated by semicolon (;). The result of the last statement will be returned as the result of the batch. Statements could be piped together using operator (|), much like the pipe in the shell scripts. The result yielded from the previous statement could be redirected to the next statement as input. Notice that compose statements are not Transactional queries. For example, a statement composed of three sub-queries: A | B | C, where A is a read operation, B is a computation, and C is a write operation. If any part fails in the execution, the whole result could be undefined -- currently, there is no so called roll back -- what was written depends on the query executor. Examples \u00b6 semicolon statements SHOW TAGS; SHOW EDGES; -- only edges are shown INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES 101:(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES 102:(\"LaMarcus Aldridge\", 33); /* multiple vertices are added in a compose statement. */ PIPE statements GO FROM 201 OVER edge_serve | GO FROM $-.id OVER edge_fans | GO FROM $-.id ... Placeholder $-.id takes the result from the first statement GO FROM 201 OVER edge_serve YIELD edge_serve._dst AS id .","title":"Statement Composition"},{"location":"manual-EN/2.query-language/3.language-structure/statement-composition/#statement_composition","text":"There are only two ways to compose statements (or sub-queries): More than one statements can be batched together, separated by semicolon (;). The result of the last statement will be returned as the result of the batch. Statements could be piped together using operator (|), much like the pipe in the shell scripts. The result yielded from the previous statement could be redirected to the next statement as input. Notice that compose statements are not Transactional queries. For example, a statement composed of three sub-queries: A | B | C, where A is a read operation, B is a computation, and C is a write operation. If any part fails in the execution, the whole result could be undefined -- currently, there is no so called roll back -- what was written depends on the query executor.","title":"Statement Composition"},{"location":"manual-EN/2.query-language/3.language-structure/statement-composition/#examples","text":"semicolon statements SHOW TAGS; SHOW EDGES; -- only edges are shown INSERT VERTEX player(name, age) VALUES 100:(\"Tim Duncan\", 42); \\ INSERT VERTEX player(name, age) VALUES 101:(\"Tony Parker\", 36); \\ INSERT VERTEX player(name, age) VALUES 102:(\"LaMarcus Aldridge\", 33); /* multiple vertices are added in a compose statement. */ PIPE statements GO FROM 201 OVER edge_serve | GO FROM $-.id OVER edge_fans | GO FROM $-.id ... Placeholder $-.id takes the result from the first statement GO FROM 201 OVER edge_serve YIELD edge_serve._dst AS id .","title":"Examples"},{"location":"manual-EN/2.query-language/3.language-structure/user-defined-variables/","text":"User-Defined Variables \u00b6 Nebula Graph supports user-defined variables, which allows passing the result of one statement to another. A user-defined variable is written as $var_name , where var_name is a user-defined name/variable that consists of alphanumeric characters, any other characters are not recommended currently. User-defined variables can only be used in one execution (compound statements separated by semicolon ; or pipe | and are submitted to the server to execute together). Be noted that a user-defined variable is valid only at the current session and execution. A user-defined variable in one statement can NOT be used in neither other clients nor other executions, which means that the definition statement and the statements that use it must be submitted together. And when the session ends these variables are automatically expired. User-defined variables are case-sensitive. Example: nebula> $var = GO FROM hash('curry') OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve;","title":"User Defined Variables"},{"location":"manual-EN/2.query-language/3.language-structure/user-defined-variables/#user-defined_variables","text":"Nebula Graph supports user-defined variables, which allows passing the result of one statement to another. A user-defined variable is written as $var_name , where var_name is a user-defined name/variable that consists of alphanumeric characters, any other characters are not recommended currently. User-defined variables can only be used in one execution (compound statements separated by semicolon ; or pipe | and are submitted to the server to execute together). Be noted that a user-defined variable is valid only at the current session and execution. A user-defined variable in one statement can NOT be used in neither other clients nor other executions, which means that the definition statement and the statements that use it must be submitted together. And when the session ends these variables are automatically expired. User-defined variables are case-sensitive. Example: nebula> $var = GO FROM hash('curry') OVER follow YIELD follow._dst AS id; \\ GO FROM $var.id OVER serve;","title":"User-Defined Variables"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/boolean-literals/","text":"Boolean Literals \u00b6 The boolean literals TRUE and FALSE can be written in any letter case. nebula> yield TRUE, true, FALSE, false, FalsE ========================================= | true | true | false | false | false | ========================================= | true | true | false | false | false | -----------------------------------------","title":"Boolean Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/boolean-literals/#boolean_literals","text":"The boolean literals TRUE and FALSE can be written in any letter case. nebula> yield TRUE, true, FALSE, false, FalsE ========================================= | true | true | false | false | false | ========================================= | true | true | false | false | false | -----------------------------------------","title":"Boolean Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/","text":"Numeric Literals \u00b6 Numeric literals include integers literals and floating-point literals (doubles). Integers Literals \u00b6 Integers are 64 bit digitals, and can be preceded by + or - to indicate a positive or negative value, respectively. They're the same as int64_t in the C language. Notice that the maximum value for the positive integers is 9223372036854775807 . It's syntax-error if you try to input any value greater than the maximum. So as the minimum value -9223372036854775808 for the negative integers. Floating-Point Literals (Doubles) \u00b6 Floating-points are the same as double in the C language. The range for double is about -1.79769e+308 to 1.79769e+308 . Scientific notations is not supported yet. Scientific Notations \u00b6 Scientific notations are numbers represented with a mantissa and exponent. Either or both parts may be signed. Examples: 1.2E3 , 1.2E-3 , -1.2E3 , -1.2E-3 . Examples \u00b6 Here are some examples: 1, -5, +10000100000 -2.3, +1.00000000000 1.2E3","title":"Numeric Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/#numeric_literals","text":"Numeric literals include integers literals and floating-point literals (doubles).","title":"Numeric Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/#integers_literals","text":"Integers are 64 bit digitals, and can be preceded by + or - to indicate a positive or negative value, respectively. They're the same as int64_t in the C language. Notice that the maximum value for the positive integers is 9223372036854775807 . It's syntax-error if you try to input any value greater than the maximum. So as the minimum value -9223372036854775808 for the negative integers.","title":"Integers Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/#floating-point_literals_doubles","text":"Floating-points are the same as double in the C language. The range for double is about -1.79769e+308 to 1.79769e+308 . Scientific notations is not supported yet.","title":"Floating-Point Literals (Doubles)"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/#scientific_notations","text":"Scientific notations are numbers represented with a mantissa and exponent. Either or both parts may be signed. Examples: 1.2E3 , 1.2E-3 , -1.2E3 , -1.2E-3 .","title":"Scientific Notations"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/numeric-literals/#examples","text":"Here are some examples: 1, -5, +10000100000 -2.3, +1.00000000000 1.2E3","title":"Examples"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/string-literals/","text":"String Literals \u00b6 A string is a sequence of bytes or characters, enclosed within either single quote (') or double quote (\") characters. Examples: nebula> YIELD 'a string' nebula> YIELD \"another string\" Certain backslash escapes (\\) are supported (also known as the escape characters ). They are shown in the following table: Escape Sequence Character Represented by Sequence \\' A single quote (') character \\\" A double quote (\") character \\t A tab character \\n A newline character \\b A backspace character \\ A backslash (\\) character Here are some examples: nebula> YIELD 'This\\nIs\\nFour\\nLines' ======================== | \"This Is Four Lines\" | ======================== | This Is Four Lines | ------------------------ nebula> YIELD 'disappearing\\ backslash' ============================ | \"disappearing backslash\" | ============================ | disappearing backslash | ----------------------------","title":"String Literals"},{"location":"manual-EN/2.query-language/3.language-structure/literal-values/string-literals/#string_literals","text":"A string is a sequence of bytes or characters, enclosed within either single quote (') or double quote (\") characters. Examples: nebula> YIELD 'a string' nebula> YIELD \"another string\" Certain backslash escapes (\\) are supported (also known as the escape characters ). They are shown in the following table: Escape Sequence Character Represented by Sequence \\' A single quote (') character \\\" A double quote (\") character \\t A tab character \\n A newline character \\b A backspace character \\ A backslash (\\) character Here are some examples: nebula> YIELD 'This\\nIs\\nFour\\nLines' ======================== | \"This Is Four Lines\" | ======================== | This Is Four Lines | ------------------------ nebula> YIELD 'disappearing\\ backslash' ============================ | \"disappearing backslash\" | ============================ | disappearing backslash | ----------------------------","title":"String Literals"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/","text":"Schema Index \u00b6 CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list) Schema indexes are built to fast process graph queries. Nebula Graph supports two different kinds of indexing to speed up query processing: tag indexes and edge type indexes . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Schema indexes make these global retrieval operations efficient on large graphs. Normally, you create indexes on a tag/edge-type at the time the tag/edge-type itself is created with CREATE TAG/EDGE statement. Create Index \u00b6 CREATE INDEX enables you to add indexes to existing tag/edge-type. Create Single-Property Index \u00b6 nebula> CREATE TAG INDEX player_index_0 on player(name); The above statement creates an index for the name property on all vertices carrying the player tag. nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The above statement creates an index for the degree property on all edges carrying the follow edge type. Create Composite Index \u00b6 The schema indexes also support spawning over multiple properties. An index on multiple properties is called a composite index. Note: Index across multiple tags is not yet supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name,age); This statement creates a composite index for the name and age property on all vertices carrying the player tag. Show Index \u00b6 SHOW {TAG | EDGE} INDEXES SHOW INDEXES returns the defined tag/edg-type index information. For example, list the indexes with the following command: nebula> SHOW TAG INDEXES; ============================= | Index ID | Index Name | ============================= | 22 | player_index_0 | ----------------------------- | 23 | player_index_1 | ----------------------------- nebula> SHOW EDGE INDEXES; ============================= | Index ID | Index Name | ============================= | 24 | follow_index_0 | ----------------------------- DESCRIBE INDEX \u00b6 DESCRIBE {TAG | EDGE} INDEX <index_name> DESCRIBE INDEX is used to obtain information about the index. For example, list the index information with the following command: nebula> DESCRIBE TAG INDEX player_index_0; ================== | Field | Type | ================== | name | string | ------------------ nebula> DESCRIBE TAG INDEX player_index_1; ================== | Field | Type | ================== | name | string | ------------------ | age | int | ------------------ DROP INDEX \u00b6 DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> DROP INDEX drops the index named index_name from the tag/edge-type. For example, drop the index player_index_0 with the following command: nebula> DROP TAG INDEX player_index_0; REBUILD INDEX \u00b6 REBUILD {TAG | EDGE} INDEX <index_name> [OFFLINE] Create Index section describes how to build indexes to improve query performance. If the index is created before inserting the data, there is no need to rebuild index and this section can be skipped; if data is updated or newly inserted after the index creation, it is necessary to rebuild the indexes in order to ensure that the indexes contain the previously added data. If the current database does not provide any services, use the OFFLINE keyword to speed up the rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For example: nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index OFFLINE; Execution succeeded (Time spent: 2.352/3.568 ms) nebula> SHOW TAG INDEX STATUS; ========================================== | Name | Tag Index Status | ========================================== | single_person_index | SUCCEEDED | ------------------------------------------ Using Index \u00b6 After the index is created and data is inserted, you can use the LOOKUP statement to query the data. There is usually no need to specify which indexes to use in a query, Nebula Graph will figure that out by itself.","title":"INDEX Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#schema_index","text":"CREATE {TAG | EDGE} INDEX [IF NOT EXISTS] <index_name> ON {<tag_name> | <edge_name>} (prop_name_list) Schema indexes are built to fast process graph queries. Nebula Graph supports two different kinds of indexing to speed up query processing: tag indexes and edge type indexes . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Schema indexes make these global retrieval operations efficient on large graphs. Normally, you create indexes on a tag/edge-type at the time the tag/edge-type itself is created with CREATE TAG/EDGE statement.","title":"Schema Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#create_index","text":"CREATE INDEX enables you to add indexes to existing tag/edge-type.","title":"Create Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#create_single-property_index","text":"nebula> CREATE TAG INDEX player_index_0 on player(name); The above statement creates an index for the name property on all vertices carrying the player tag. nebula> CREATE EDGE INDEX follow_index_0 on follow(degree); The above statement creates an index for the degree property on all edges carrying the follow edge type.","title":"Create Single-Property Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#create_composite_index","text":"The schema indexes also support spawning over multiple properties. An index on multiple properties is called a composite index. Note: Index across multiple tags is not yet supported. Consider the following example: nebula> CREATE TAG INDEX player_index_1 on player(name,age); This statement creates a composite index for the name and age property on all vertices carrying the player tag.","title":"Create Composite Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#show_index","text":"SHOW {TAG | EDGE} INDEXES SHOW INDEXES returns the defined tag/edg-type index information. For example, list the indexes with the following command: nebula> SHOW TAG INDEXES; ============================= | Index ID | Index Name | ============================= | 22 | player_index_0 | ----------------------------- | 23 | player_index_1 | ----------------------------- nebula> SHOW EDGE INDEXES; ============================= | Index ID | Index Name | ============================= | 24 | follow_index_0 | -----------------------------","title":"Show Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#describe_index","text":"DESCRIBE {TAG | EDGE} INDEX <index_name> DESCRIBE INDEX is used to obtain information about the index. For example, list the index information with the following command: nebula> DESCRIBE TAG INDEX player_index_0; ================== | Field | Type | ================== | name | string | ------------------ nebula> DESCRIBE TAG INDEX player_index_1; ================== | Field | Type | ================== | name | string | ------------------ | age | int | ------------------","title":"DESCRIBE INDEX"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#drop_index","text":"DROP {TAG | EDGE} INDEX [IF EXISTS] <index_name> DROP INDEX drops the index named index_name from the tag/edge-type. For example, drop the index player_index_0 with the following command: nebula> DROP TAG INDEX player_index_0;","title":"DROP INDEX"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#rebuild_index","text":"REBUILD {TAG | EDGE} INDEX <index_name> [OFFLINE] Create Index section describes how to build indexes to improve query performance. If the index is created before inserting the data, there is no need to rebuild index and this section can be skipped; if data is updated or newly inserted after the index creation, it is necessary to rebuild the indexes in order to ensure that the indexes contain the previously added data. If the current database does not provide any services, use the OFFLINE keyword to speed up the rebuilding. After rebuilding is complete, you can use the SHOW {TAG | EDGE} INDEX STATUS command to check if the index is successfully rebuilt. For example: nebula> CREATE TAG person(name string, age int, gender string, email string); Execution succeeded (Time spent: 10.051/11.397 ms) nebula> CREATE TAG INDEX single_person_index ON person(name); Execution succeeded (Time spent: 2.168/3.379 ms) nebula> REBUILD TAG INDEX single_person_index OFFLINE; Execution succeeded (Time spent: 2.352/3.568 ms) nebula> SHOW TAG INDEX STATUS; ========================================== | Name | Tag Index Status | ========================================== | single_person_index | SUCCEEDED | ------------------------------------------","title":"REBUILD INDEX"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/#using_index","text":"After the index is created and data is inserted, you can use the LOOKUP statement to query the data. There is usually no need to specify which indexes to use in a query, Nebula Graph will figure that out by itself.","title":"Using Index"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/","text":"TTL (time-to-live) \u00b6 With TTL , Nebula Graph provides the ability to delete the expired vertices or edges automatically. The system will automatically delete the expired data during the compaction phase. Before compaction, query will filter the expired data. TTl requires ttl_col and ttl_duration together. ttl_col indicates the TTL column, while ttl_duration indicates the duration of the TTL. When the sum of the TTL column and the ttl_duration is less than the current time, we consider the data as expired. The ttl_col type is integer or timestamp, and is set in seconds. ttl_duration is also set in seconds. TTL configuration \u00b6 The ttl_duration is set in seconds and ranges from 0 to max(int64). If it is set to 0, the vertex properties of this tag does not expire. If TTL is set, when the sum of the ttl_col and the ttl_duration is less than the current time, we consider the vertex properties of this tag as expired after the specified seconds configured by ttl_duration has passed since the ttl_col field value. When the vertex has multiple tags, the TTL of each tag is processed separately. Setting a TTL Value \u00b6 Setting a TTL value for the existed tag. nebula> CREATE TAG t1(a timestamp); nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; -- Setting ttl nebula> INSERT VERTEX t1(a) values 101:(now()); The vertex 101 property in tag t1 will expire after 5 seconds since specified by now(). Or you can set the TTL attribute when creating the tag. nebula> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; nebula> INSERT VERTEX t2(a, b, c) values 102:(1584441231, 30, \"Word\"); The vertex 102 property in tag t2 will expire after 100 seconds since March 17 2020 at 18:33:51 CST i.e. the timestamp is 1584441231. When a vertex has multiple TAGs, the TTL of each TAG is independent from each other. nebula> CREATE TAG t3(a string); nebula> INSERT VERTEX t1(a),t3(a) values 200:(now(), \"hello\"); The vertex 200 property in tag t1 will expire after 5 seconds. nebula> FETCH PROP ON t1 200; Execution succeeded (Time spent: 5.945/7.492 ms) nebula> FETCH PROP ON t3 200; ====================== | VertexID | t3.a | ====================== | 200 | hello | ---------------------- nebula> FETCH PROP ON * 200; ====================== | VertexID | t3.a | ====================== | 200 | hello | ---------------------- Dropping TTL \u00b6 If you have set a TTL value for a field and later decide do not want it to ever automatically expire, you can drop the TTL value, set it to an empty string or invalidate it by setting it to 0. nebula> ALTER TAG t1 ttl_col = \"\"; -- drop ttl attribute; Drop the field a with the ttl attribute: nebula> ALTER TAG t1 DROP (a); -- drop ttl_col Invalidate the TTL: nebula> ALTER TAG t1 ttl_duration = 0; -- keep the ttl but the data never expires Tips on TTL \u00b6 If a field contains a ttl_col field, you can't make any change on the field. nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> ALTER TAG t1 CHANGE (a string); -- failed Note that the a tag or an edge cannot have both the TTL attribute and index at the same time, even if the ttl_col column is different from that of the index. nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> CREATE TAG INDEX id1 ON t1(a); -- failed nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> CREATE TAG INDEX id1 ON t1(b); -- failed nebula> CREATE TAG t1(a int, b int, c string); nebula> CREATE TAG INDEX id1 ON t1(a); nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 100; -- failed Adding TTL to an edge is similar to a tag.","title":"TTL"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/#ttl_time-to-live","text":"With TTL , Nebula Graph provides the ability to delete the expired vertices or edges automatically. The system will automatically delete the expired data during the compaction phase. Before compaction, query will filter the expired data. TTl requires ttl_col and ttl_duration together. ttl_col indicates the TTL column, while ttl_duration indicates the duration of the TTL. When the sum of the TTL column and the ttl_duration is less than the current time, we consider the data as expired. The ttl_col type is integer or timestamp, and is set in seconds. ttl_duration is also set in seconds.","title":"TTL (time-to-live)"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/#ttl_configuration","text":"The ttl_duration is set in seconds and ranges from 0 to max(int64). If it is set to 0, the vertex properties of this tag does not expire. If TTL is set, when the sum of the ttl_col and the ttl_duration is less than the current time, we consider the vertex properties of this tag as expired after the specified seconds configured by ttl_duration has passed since the ttl_col field value. When the vertex has multiple tags, the TTL of each tag is processed separately.","title":"TTL configuration"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/#setting_a_ttl_value","text":"Setting a TTL value for the existed tag. nebula> CREATE TAG t1(a timestamp); nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 5; -- Setting ttl nebula> INSERT VERTEX t1(a) values 101:(now()); The vertex 101 property in tag t1 will expire after 5 seconds since specified by now(). Or you can set the TTL attribute when creating the tag. nebula> CREATE TAG t2(a int, b int, c string) ttl_duration= 100, ttl_col = \"a\"; nebula> INSERT VERTEX t2(a, b, c) values 102:(1584441231, 30, \"Word\"); The vertex 102 property in tag t2 will expire after 100 seconds since March 17 2020 at 18:33:51 CST i.e. the timestamp is 1584441231. When a vertex has multiple TAGs, the TTL of each TAG is independent from each other. nebula> CREATE TAG t3(a string); nebula> INSERT VERTEX t1(a),t3(a) values 200:(now(), \"hello\"); The vertex 200 property in tag t1 will expire after 5 seconds. nebula> FETCH PROP ON t1 200; Execution succeeded (Time spent: 5.945/7.492 ms) nebula> FETCH PROP ON t3 200; ====================== | VertexID | t3.a | ====================== | 200 | hello | ---------------------- nebula> FETCH PROP ON * 200; ====================== | VertexID | t3.a | ====================== | 200 | hello | ----------------------","title":"Setting a TTL Value"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/#dropping_ttl","text":"If you have set a TTL value for a field and later decide do not want it to ever automatically expire, you can drop the TTL value, set it to an empty string or invalidate it by setting it to 0. nebula> ALTER TAG t1 ttl_col = \"\"; -- drop ttl attribute; Drop the field a with the ttl attribute: nebula> ALTER TAG t1 DROP (a); -- drop ttl_col Invalidate the TTL: nebula> ALTER TAG t1 ttl_duration = 0; -- keep the ttl but the data never expires","title":"Dropping TTL"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/TTL/#tips_on_ttl","text":"If a field contains a ttl_col field, you can't make any change on the field. nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> ALTER TAG t1 CHANGE (a string); -- failed Note that the a tag or an edge cannot have both the TTL attribute and index at the same time, even if the ttl_col column is different from that of the index. nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> CREATE TAG INDEX id1 ON t1(a); -- failed nebula> CREATE TAG t1(a int, b int, c string) ttl_duration = 100, ttl_col = \"a\"; nebula> CREATE TAG INDEX id1 ON t1(b); -- failed nebula> CREATE TAG t1(a int, b int, c string); nebula> CREATE TAG INDEX id1 ON t1(a); nebula> ALTER TAG t1 ttl_col = \"a\", ttl_duration = 100; -- failed Adding TTL to an edge is similar to a tag.","title":"Tips on TTL"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/alter-tag-edge-syntax/","text":"ALTER TAG/EDGE Syntax \u00b6 ALTER TAG | EDGE <tag_name> | <edge_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER statement changes the structure of a tag or an edge. For example, you can add or delete properties, change the data type of an existing property. You can also set a property as TTL (Time-To-Live), or change the TTL duration. Note: Nebula Graph automatically examines indexes when altering a tag or edge. When altering a tag or edge, Nebula Graph first checks whether the tag or edge is associated with any indexes then traverses all of them to check whether the column item to be dropped or changed exists in the index column. If existed, the alter is rejected. Otherwise, it is allowed. Please refer to Index Documentation on details about index. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statements, separated by commas. But do NOT add, drop, change the same property in one statement. If you have to do so, make each operation as a clause of the ALTER statement. nebula> CREATE TAG t1 (name string, age int); nebula> ALTER TAG t1 ADD (id int, address string); nebula> CREATE EDGE e1 (prop3 int, prop4 int, prop5 int); nebula> ALTER EDGE e1 ADD (prop1 int, prop2 string), /* \u6dfb\u52a0 prop1 */ CHANGE (prop3 string), /* \u5c06 prop3 \u7c7b\u578b\u66f4\u6539\u4e3a\u5b57\u7b26 */ DROP (prop4, prop5); /* \u5220\u9664 prop4 \u548c prop5 */ nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = prop1; Notice that TTL_COL only support INT and TIMESTAMP types.","title":"ALTER TAG EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/alter-tag-edge-syntax/#alter_tagedge_syntax","text":"ALTER TAG | EDGE <tag_name> | <edge_name> <alter_definition> [, alter_definition] ...] [ttl_definition [, ttl_definition] ... ] alter_definition: | ADD (prop_name data_type) | DROP (prop_name) | CHANGE (prop_name data_type) ttl_definition: TTL_DURATION = ttl_duration, TTL_COL = prop_name ALTER statement changes the structure of a tag or an edge. For example, you can add or delete properties, change the data type of an existing property. You can also set a property as TTL (Time-To-Live), or change the TTL duration. Note: Nebula Graph automatically examines indexes when altering a tag or edge. When altering a tag or edge, Nebula Graph first checks whether the tag or edge is associated with any indexes then traverses all of them to check whether the column item to be dropped or changed exists in the index column. If existed, the alter is rejected. Otherwise, it is allowed. Please refer to Index Documentation on details about index. Multiple ADD , DROP , and CHANGE clauses are permitted in a single ALTER statements, separated by commas. But do NOT add, drop, change the same property in one statement. If you have to do so, make each operation as a clause of the ALTER statement. nebula> CREATE TAG t1 (name string, age int); nebula> ALTER TAG t1 ADD (id int, address string); nebula> CREATE EDGE e1 (prop3 int, prop4 int, prop5 int); nebula> ALTER EDGE e1 ADD (prop1 int, prop2 string), /* \u6dfb\u52a0 prop1 */ CHANGE (prop3 string), /* \u5c06 prop3 \u7c7b\u578b\u66f4\u6539\u4e3a\u5b57\u7b26 */ DROP (prop4, prop5); /* \u5220\u9664 prop4 \u548c prop5 */ nebula> ALTER EDGE e1 TTL_DURATION = 2, TTL_COL = prop1; Notice that TTL_COL only support INT and TIMESTAMP types.","title":"ALTER TAG/EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/","text":"CREATE SPACE Syntax \u00b6 CREATE SPACE [IF NOT EXISTS] <space_name> [(partition_num = <part_num>, replica_factor = <raft_copy>, charset = <charset>, collate = <collate>)] This statement creates a new space with the given name. SPACE is a region that provides physically isolated graphs in Nebula Graph . An error occurs if the database exists. IF NOT EXISTS \u00b6 You can use the If NOT EXISTS keywords when creating spaces. This keyword automatically detects if the corresponding space exists. If it does not exist, a new one is created. Otherwise, no space is created. Note: The space existence detection here only compares the space name (excluding properties). Space Name \u00b6 space_name The name uniquely identifies the space in a cluster. The rules for the naming are given in Schema Object Names Customized Space Options \u00b6 When creating a space, the following two customized options can be given: partition_num partition_num specifies the number of partitions in one replica. The default value is 100. It is usually 5 times the number of hard disks in the cluster. replica_factor replica_factor specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in cluster. It is usually 3 in production. charset charset is short for character set. A character set is a set of symbols and encodings. The default value is utf8. collate A collation is a set of rules for comparing characters in a character set. The default value is utf8_bin. However, if no option is given, Nebula Graph will create the space with the default partition number, replica factor, charset and collate. Example \u00b6 nebula> CREATE SPACE my_space_1; -- create space with default partition number and replica factor nebula> CREATE SPACE my_space_2(partition_num=10); -- create space with default replica factor nebula> CREATE SPACE my_space_3(replica_factor=1); -- create space with default partition number nebula> CREATE SPACE my_space_4(partition_num=10, replica_factor=1); Checking Partition Distribution \u00b6 On some large clusters, due to the different startup time, the partition distribution may be unbalanced. You can check the machine and distribution by the following command (SHOW HOSTS). nebula> SHOW HOSTS; ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ If all the machines are online status, but the partition distribution is unbalanced, you can use the following command (BALANCE LEADER) to redistribute the partitions. nebula> BALANCE LEADER; Details see SHOW HOSTS and BALANCE .","title":"CREATE SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#create_space_syntax","text":"CREATE SPACE [IF NOT EXISTS] <space_name> [(partition_num = <part_num>, replica_factor = <raft_copy>, charset = <charset>, collate = <collate>)] This statement creates a new space with the given name. SPACE is a region that provides physically isolated graphs in Nebula Graph . An error occurs if the database exists.","title":"CREATE SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#if_not_exists","text":"You can use the If NOT EXISTS keywords when creating spaces. This keyword automatically detects if the corresponding space exists. If it does not exist, a new one is created. Otherwise, no space is created. Note: The space existence detection here only compares the space name (excluding properties).","title":"IF NOT EXISTS"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#space_name","text":"space_name The name uniquely identifies the space in a cluster. The rules for the naming are given in Schema Object Names","title":"Space Name"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#customized_space_options","text":"When creating a space, the following two customized options can be given: partition_num partition_num specifies the number of partitions in one replica. The default value is 100. It is usually 5 times the number of hard disks in the cluster. replica_factor replica_factor specifies the number of replicas in the cluster. The default replica factor is 1. The suggested number is 3 in cluster. It is usually 3 in production. charset charset is short for character set. A character set is a set of symbols and encodings. The default value is utf8. collate A collation is a set of rules for comparing characters in a character set. The default value is utf8_bin. However, if no option is given, Nebula Graph will create the space with the default partition number, replica factor, charset and collate.","title":"Customized Space Options"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#example","text":"nebula> CREATE SPACE my_space_1; -- create space with default partition number and replica factor nebula> CREATE SPACE my_space_2(partition_num=10); -- create space with default replica factor nebula> CREATE SPACE my_space_3(replica_factor=1); -- create space with default partition number nebula> CREATE SPACE my_space_4(partition_num=10, replica_factor=1);","title":"Example"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-space-syntax/#checking_partition_distribution","text":"On some large clusters, due to the different startup time, the partition distribution may be unbalanced. You can check the machine and distribution by the following command (SHOW HOSTS). nebula> SHOW HOSTS; ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ If all the machines are online status, but the partition distribution is unbalanced, you can use the following command (BALANCE LEADER) to redistribute the partitions. nebula> BALANCE LEADER; Details see SHOW HOSTS and BALANCE .","title":"Checking Partition Distribution"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/","text":"CREATE TAG/EDGE Syntax \u00b6 CREATE {TAG | EDGE} [IF NOT EXISTS] {<tag_name> | <edge_name>} ([<create_definition>, ...]) [tag_edge_options] <create_definition> ::= <prop_name> <data_type> <tag_edge_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> Nebula Graph 's schema is composed of tags and edges, either of which may have properties. CREATE TAG statement defines a tag with the given name. While CREATE EDGE statement is to define an edge type. The features of this syntax are described in the following sections: IF NOT EXISTS \u00b6 You can use the If NOT EXISTS keywords when creating tags or edges. This keyword automatically detects if the corresponding tag or edge exists. If it does not exist, a new one is created. Otherwise, no tag or edge is created. Note: The tag or edge existence detection here only compares the tag or edge name (excluding properties). Tag Name and Edge Type Name \u00b6 tag_name and edge_name The name of tags and edgeTypes must be unique within the space. Once the name is defined, it can not be altered. The rules of tag and edgeType names are the same as those for names of spaces. See Schema Object Name for detail. Property Name and Data Type \u00b6 prop_name prop_name indicates the name of properties. It must be unique for each tag or edgeType. data_type data_type represents the data type of each property. For more information about data types that Nebula Graph supports, see data-type section. NULL and NOT NULL constrain are not supported yet when creating tags/edges (comparing with relational databases). Default Constraint You can set the default value of a property when creating a tag/edge with the DEFAULT constraint. The default value will be added to all new vertices and edges if no other value is specified. The default value can be any of the data type supported by Nebula Graph or expressions. Also you can write a user-specified value if you don't want to use the default one. Using Alter to change the default value is not supported. Time-to-Live (TTL) Syntax \u00b6 TTL_DURATION ttl_duration specifies the life cycle of vertices (or edges). Data that exceeds the specified TTL will expire. The expiration threshold is the specified TTL_COL value plus the TTL_DURATION. If the value for ttl_duration is zero or negative, the vertices or edges will not expire. TTL_COL The data type of prop_name must be either int64 or timestamp. single TTL definition Only a single TTL_COL field can be specified. Details about TTL refer to the TTL Doc . Examples \u00b6 nebula> CREATE TAG course(name string, credits int) nebula> CREATE TAG notag() -- empty properties nebula> CREATE EDGE follow(start_time timestamp, grade double) nebula> CREATE EDGE noedge() -- empty properties nebula> CREATE TAG player_with_default(name string, age int DEFAULT 20) -- age is set to 20 by default nebula> CREATE EDGE follow_with_default(start_time timestamp DEFAULT 0, grade double DEFAULT 0.0) -- start_time is set to 0 by default, grade is set to 0.0 by default nebula> CREATE TAG woman(name string, age int, married bool, salary double, create_time timestamp) TTL_DURATION = 100, TTL_COL = \"create_time\" -- time interval is 100s, starting from the create_time filed nebula> CREATE EDGE marriage(location string, since timestamp) TTL_DURATION = 0, TTL_COL = \"since\" -- negative or zero, not expire nebula> CREATE TAG icecream(made timestamp, temperature int) TTL_DURATION = 100, TTL_COL = \"made\", -- Data expires after TTL_DURATION","title":"CREATE TAG EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#create_tagedge_syntax","text":"CREATE {TAG | EDGE} [IF NOT EXISTS] {<tag_name> | <edge_name>} ([<create_definition>, ...]) [tag_edge_options] <create_definition> ::= <prop_name> <data_type> <tag_edge_options> ::= <option> [, <option> ...] <option> ::= TTL_DURATION [=] <ttl_duration> | TTL_COL [=] <prop_name> | DEFAULT <default_value> Nebula Graph 's schema is composed of tags and edges, either of which may have properties. CREATE TAG statement defines a tag with the given name. While CREATE EDGE statement is to define an edge type. The features of this syntax are described in the following sections:","title":"CREATE TAG/EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#if_not_exists","text":"You can use the If NOT EXISTS keywords when creating tags or edges. This keyword automatically detects if the corresponding tag or edge exists. If it does not exist, a new one is created. Otherwise, no tag or edge is created. Note: The tag or edge existence detection here only compares the tag or edge name (excluding properties).","title":"IF NOT EXISTS"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#tag_name_and_edge_type_name","text":"tag_name and edge_name The name of tags and edgeTypes must be unique within the space. Once the name is defined, it can not be altered. The rules of tag and edgeType names are the same as those for names of spaces. See Schema Object Name for detail.","title":"Tag Name and Edge Type Name"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#property_name_and_data_type","text":"prop_name prop_name indicates the name of properties. It must be unique for each tag or edgeType. data_type data_type represents the data type of each property. For more information about data types that Nebula Graph supports, see data-type section. NULL and NOT NULL constrain are not supported yet when creating tags/edges (comparing with relational databases). Default Constraint You can set the default value of a property when creating a tag/edge with the DEFAULT constraint. The default value will be added to all new vertices and edges if no other value is specified. The default value can be any of the data type supported by Nebula Graph or expressions. Also you can write a user-specified value if you don't want to use the default one. Using Alter to change the default value is not supported.","title":"Property Name and Data Type"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#time-to-live_ttl_syntax","text":"TTL_DURATION ttl_duration specifies the life cycle of vertices (or edges). Data that exceeds the specified TTL will expire. The expiration threshold is the specified TTL_COL value plus the TTL_DURATION. If the value for ttl_duration is zero or negative, the vertices or edges will not expire. TTL_COL The data type of prop_name must be either int64 or timestamp. single TTL definition Only a single TTL_COL field can be specified. Details about TTL refer to the TTL Doc .","title":"Time-to-Live (TTL) Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/create-tag-edge-syntax/#examples","text":"nebula> CREATE TAG course(name string, credits int) nebula> CREATE TAG notag() -- empty properties nebula> CREATE EDGE follow(start_time timestamp, grade double) nebula> CREATE EDGE noedge() -- empty properties nebula> CREATE TAG player_with_default(name string, age int DEFAULT 20) -- age is set to 20 by default nebula> CREATE EDGE follow_with_default(start_time timestamp DEFAULT 0, grade double DEFAULT 0.0) -- start_time is set to 0 by default, grade is set to 0.0 by default nebula> CREATE TAG woman(name string, age int, married bool, salary double, create_time timestamp) TTL_DURATION = 100, TTL_COL = \"create_time\" -- time interval is 100s, starting from the create_time filed nebula> CREATE EDGE marriage(location string, since timestamp) TTL_DURATION = 0, TTL_COL = \"since\" -- negative or zero, not expire nebula> CREATE TAG icecream(made timestamp, temperature int) TTL_DURATION = 100, TTL_COL = \"made\", -- Data expires after TTL_DURATION","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-edge-syntax/","text":"DROP EDGE Syntax \u00b6 DROP EDGE [IF EXISTS] <edge_type_name> You must have the DROP privilege for the edge type. Note: When dropping an edge, Nebula Graph only checks whether the edge is associated with any indexes. If so the deletion is rejected. Please refer to Index Documentation on details about index. You can use the If EXISTS keywords when dropping edges. These keywords automatically detect if the corresponding edge exists. If it exists, it will be deleted. Otherwise, no edge is deleted. This statement removes all the edges (connections) within the specific edge type. This operation only deletes the Schema data, all the files and directories in the disk are NOT deleted directly, data is deleted in the next compaction.","title":"DROP EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-edge-syntax/#drop_edge_syntax","text":"DROP EDGE [IF EXISTS] <edge_type_name> You must have the DROP privilege for the edge type. Note: When dropping an edge, Nebula Graph only checks whether the edge is associated with any indexes. If so the deletion is rejected. Please refer to Index Documentation on details about index. You can use the If EXISTS keywords when dropping edges. These keywords automatically detect if the corresponding edge exists. If it exists, it will be deleted. Otherwise, no edge is deleted. This statement removes all the edges (connections) within the specific edge type. This operation only deletes the Schema data, all the files and directories in the disk are NOT deleted directly, data is deleted in the next compaction.","title":"DROP EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-space-syntax/","text":"DROP SPACE Syntax \u00b6 DROP SPACE [IF EXISTS] <space_name> You must have the DROP privilege for the graph space. DROP SPACE deletes everything (all the vertices, edges, indices, and properties) in the specific space. You can use the If EXISTS keywords when dropping spaces. This keyword automatically detects if the corresponding space exists. If it exists, it will be deleted. Otherwise, no space is deleted. Other spaces remain unchanged. This statement does not immediately remove all the files and directories in the storage engine (and release disk space). The deletion depends on the implementation of different storage engines. Be very careful with this statement.","title":"DROP SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-space-syntax/#drop_space_syntax","text":"DROP SPACE [IF EXISTS] <space_name> You must have the DROP privilege for the graph space. DROP SPACE deletes everything (all the vertices, edges, indices, and properties) in the specific space. You can use the If EXISTS keywords when dropping spaces. This keyword automatically detects if the corresponding space exists. If it exists, it will be deleted. Otherwise, no space is deleted. Other spaces remain unchanged. This statement does not immediately remove all the files and directories in the storage engine (and release disk space). The deletion depends on the implementation of different storage engines. Be very careful with this statement.","title":"DROP SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-tag-syntax/","text":"DROP TAG Syntax \u00b6 DROP TAG [IF EXISTS] <tag_name> You must have the DROP privilege for the tag. Be careful with this statement. Note: When dropping a tag, Nebula Graph only checks whether the tag is associated with any indexes. If so the deletion is rejected. Please refer to Index Documentation on details about index. You can use the If EXISTS keywords when dropping tags. These keywords automatically detect if the corresponding tag exists. If it exists, it will be deleted. Otherwise, no tag is deleted. A vertex can have either only one tag (types) or multiple tags (types). In the former case, such a vertex can NOT be accessible after the statement, and edges connected with such vertex may result in DANGLING. In the latter case, the dropped a vertex is still accessible. But all the properties defined by this dropped tag are not accessible. This operation only deletes the Schema data, all the files and directories in the disk are NOT deleted directly, data is deleted in the next compaction.","title":"DROP TAG Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/1.data-definition-statements/drop-tag-syntax/#drop_tag_syntax","text":"DROP TAG [IF EXISTS] <tag_name> You must have the DROP privilege for the tag. Be careful with this statement. Note: When dropping a tag, Nebula Graph only checks whether the tag is associated with any indexes. If so the deletion is rejected. Please refer to Index Documentation on details about index. You can use the If EXISTS keywords when dropping tags. These keywords automatically detect if the corresponding tag exists. If it exists, it will be deleted. Otherwise, no tag is deleted. A vertex can have either only one tag (types) or multiple tags (types). In the former case, such a vertex can NOT be accessible after the statement, and edges connected with such vertex may result in DANGLING. In the latter case, the dropped a vertex is still accessible. But all the properties defined by this dropped tag are not accessible. This operation only deletes the Schema data, all the files and directories in the disk are NOT deleted directly, data is deleted in the next compaction.","title":"DROP TAG Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/delete-edge-syntax/","text":"DELETE EDGE Syntax \u00b6 The DELETE EDGE statement is used to delete edges. Given an edge type, the source vertex and the dest vertex, Nebula Graph supports DELETE the edge, its associated properties and the edge ranking. You can also delete an edge with a certain rank. The syntax is as follows: DELETE EDGE <edge_type> <vid> -> <vid>[@<ranking>] [, <vid> -> <vid> ...] Nebula Graph will find the properties associated with the edge and delete all of them. Atomic operation is not guaranteed during the entire process for now, so please retry when failure occurs.","title":"DELETE EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/delete-edge-syntax/#delete_edge_syntax","text":"The DELETE EDGE statement is used to delete edges. Given an edge type, the source vertex and the dest vertex, Nebula Graph supports DELETE the edge, its associated properties and the edge ranking. You can also delete an edge with a certain rank. The syntax is as follows: DELETE EDGE <edge_type> <vid> -> <vid>[@<ranking>] [, <vid> -> <vid> ...] Nebula Graph will find the properties associated with the edge and delete all of them. Atomic operation is not guaranteed during the entire process for now, so please retry when failure occurs.","title":"DELETE EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/delete-vertex-syntax/","text":"DELETE VERTEX Syntax \u00b6 Given a list of vertices IDs, hash IDs or UUIDs, Nebula Graph supports DELETE the vertices and their associated in and out edges, syntax as the follows: DELETE VERTEX <vid_list> Nebula Graph will find the in and out edges associated with the vertices and delete all of them, then delete information related to the vertices. Atomic operation is not guaranteed during the entire process for now, so please retry when failure occurs.","title":"DELETE VERTEX Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/delete-vertex-syntax/#delete_vertex_syntax","text":"Given a list of vertices IDs, hash IDs or UUIDs, Nebula Graph supports DELETE the vertices and their associated in and out edges, syntax as the follows: DELETE VERTEX <vid_list> Nebula Graph will find the in and out edges associated with the vertices and delete all of them, then delete information related to the vertices. Atomic operation is not guaranteed during the entire process for now, so please retry when failure occurs.","title":"DELETE VERTEX Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/","text":"FETCH Syntax \u00b6 The FETCH syntax is used to get vertex/edge's properties. Fetch Vertex property \u00b6 Use FETCH PROP ON to return a (list of) vertex's properties. Currently, you can get multiple vertices' properties with the same tag in one statement. FETCH PROP ON <tag_name> <vertex_id_list> [YIELD [DISTINCT] <return_list>] FETCH PROP ON * <vertex_id> * indicates returning all the properties of the given vertex. <tag_name> is the tag name. It must be the same tag within return_list. <vertex_id_list>::=[vertex_id [, vertex_id]] is a list of vertex IDs separated by comma (,). [YIELD [DISTINCT] <return_list>] is the property list returned. Please refer YIELD Syntax for usage. Examples \u00b6 -- return all the properties of vertex id 100. nebula> FETCH PROP ON * 100; -- return all the properties in tag player of vertex id 100 if no yield field is given. nebula> FETCH PROP ON player 100; -- return property name and age of vertex id 100. nebula> FETCH PROP ON player 100 YIELD player.name, player.age; -- hash string to int64 as vertex id, fetch name and player. nebula> FETCH PROP ON player hash(\"nebula\") YIELD player.name, player.age; -- find all neighbors of vertex 100 through edge follow. Then get the neighbors' name and age. nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | FETCH PROP ON player $-.id YIELD player.name, player.age; -- the same as above statement. nebula> $var = GO FROM 100 OVER follow YIELD follow._dst AS id; FETCH PROP ON player $var.id YIELD player.name, player.age; -- get three vertices 100, 101, 102 and return by unique(distinct) name and age. nebula> FETCH PROP ON player 100,101,102 YIELD DISTINCT player.name, player.age; Fetch Edge Property \u00b6 The FETCH usage of an edge is almost the same with vertex. You can get properties from multiple edges with the same type. FETCH PROP ON <edge_type> <vid> -> <vid>[@<ranking>] [, <vid> -> <vid> ...] [YIELD [DISTINCT] <return_list>] <edge_type> specifies the edge's type. It must be the same as those in <return_list> . <vid> -> <vid> denotes a starting vertex to (->) an ending vertex. Multiple edges are separated by comma(,). <ranking> specifies the edge weight of the same edge type; it's optional. [YIELD [DISTINCT] <return_list>] is the property list returned. Example \u00b6 -- from vertex 100 to 200 with edge type serve, get all the properties since no YIELD is given. nebula> FETCH PROP ON serve 100 -> 200; -- only return property start_year. nebula> FETCH PROP ON serve 100 -> 200 YIELD serve.start_year; -- for all the out going edges of vertex 100, get edge property degree. nebula> GO FROM 100 OVER follow YIELD follow.degree; -- the same as above statement. nebula> GO FROM 100 OVER follow YIELD follow._src AS s, follow._dst AS d \\ | FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree; -- the same as above. nebula> $var = GO FROM 100 OVER follow YIELD follow._src AS s, follow._dst AS d;\\ FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree;","title":"FETCH Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/#fetch_syntax","text":"The FETCH syntax is used to get vertex/edge's properties.","title":"FETCH Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/#fetch_vertex_property","text":"Use FETCH PROP ON to return a (list of) vertex's properties. Currently, you can get multiple vertices' properties with the same tag in one statement. FETCH PROP ON <tag_name> <vertex_id_list> [YIELD [DISTINCT] <return_list>] FETCH PROP ON * <vertex_id> * indicates returning all the properties of the given vertex. <tag_name> is the tag name. It must be the same tag within return_list. <vertex_id_list>::=[vertex_id [, vertex_id]] is a list of vertex IDs separated by comma (,). [YIELD [DISTINCT] <return_list>] is the property list returned. Please refer YIELD Syntax for usage.","title":"Fetch Vertex property"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/#examples","text":"-- return all the properties of vertex id 100. nebula> FETCH PROP ON * 100; -- return all the properties in tag player of vertex id 100 if no yield field is given. nebula> FETCH PROP ON player 100; -- return property name and age of vertex id 100. nebula> FETCH PROP ON player 100 YIELD player.name, player.age; -- hash string to int64 as vertex id, fetch name and player. nebula> FETCH PROP ON player hash(\"nebula\") YIELD player.name, player.age; -- find all neighbors of vertex 100 through edge follow. Then get the neighbors' name and age. nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | FETCH PROP ON player $-.id YIELD player.name, player.age; -- the same as above statement. nebula> $var = GO FROM 100 OVER follow YIELD follow._dst AS id; FETCH PROP ON player $var.id YIELD player.name, player.age; -- get three vertices 100, 101, 102 and return by unique(distinct) name and age. nebula> FETCH PROP ON player 100,101,102 YIELD DISTINCT player.name, player.age;","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/#fetch_edge_property","text":"The FETCH usage of an edge is almost the same with vertex. You can get properties from multiple edges with the same type. FETCH PROP ON <edge_type> <vid> -> <vid>[@<ranking>] [, <vid> -> <vid> ...] [YIELD [DISTINCT] <return_list>] <edge_type> specifies the edge's type. It must be the same as those in <return_list> . <vid> -> <vid> denotes a starting vertex to (->) an ending vertex. Multiple edges are separated by comma(,). <ranking> specifies the edge weight of the same edge type; it's optional. [YIELD [DISTINCT] <return_list>] is the property list returned.","title":"Fetch Edge Property"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/fetch-syntax/#example","text":"-- from vertex 100 to 200 with edge type serve, get all the properties since no YIELD is given. nebula> FETCH PROP ON serve 100 -> 200; -- only return property start_year. nebula> FETCH PROP ON serve 100 -> 200 YIELD serve.start_year; -- for all the out going edges of vertex 100, get edge property degree. nebula> GO FROM 100 OVER follow YIELD follow.degree; -- the same as above statement. nebula> GO FROM 100 OVER follow YIELD follow._src AS s, follow._dst AS d \\ | FETCH PROP ON follow $-.s -> $-.d YIELD follow.degree; -- the same as above. nebula> $var = GO FROM 100 OVER follow YIELD follow._src AS s, follow._dst AS d;\\ FETCH PROP ON follow $var.s -> $var.d YIELD follow.degree;","title":"Example"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/","text":"GO Syntax \u00b6 GO statement is the MOST commonly used clause in Nebula Graph . It indicates to traverse in a graph with specific filters (the WHERE clause), to fetch properties of vertices and edges, and return results (the YIELD clause) with given order (the ORDER BY ASC | DESC clause) and numbers (the LIMIT clause). The syntax of GO statement is very similar to SELECT in SQL. Notice that the major difference is that GO must start traversing from a (set of) vertex (vertices). GO [ <N> STEPS ] FROM <node_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [ WHERE <expression> [ AND | OR expression ...]) ] YIELD [DISTINCT] <return_list> <node_list> | <vid> [, <vid> ...] | $-.id <edge_type_list> edge_type [, edge_type ...] * # `*` selects all the available edge types <return_list> <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] [ \\ STEPS ] specifies the N query hops is either a list of node's vid separated by comma(,), or a special place holder $-.id (refer PIPE syntax). is a list of edge types which graph traversal can go through. [ WHERE \\ ] extracts only those results that fulfill the specified conditions. WHERE syntax can be conditions for src-vertex, the edges, and dst-vertex. The logical AND, OR, NOT are also supported. See WHERE Syntax for more information. YIELD [DISTINCT] statement returns the result in column format and rename as an alias name. See YIELD syntax for more information. The DISTINCT syntax works the same as SQL. Examples \u00b6 nebula> GO FROM 107 OVER serve; \\ /* start from vertex 107 along with edge type serve, and get vertex 200, 201 */ ============== | serve._dst | ============== | 200 | -------------- | 201 | -------------- nebula> GO 2 STEPS FROM 103 OVER follow; \\ /* return the 2 hop friends of the vertex 103 */ =============== | follow._dst | =============== | 101 | --------------- nebula> GO FROM 109 OVER serve \\ WHERE serve.start_year > 1990 /* check edge (serve) property ( start_year) */ \\ YIELD $$.team.name AS team_name, serve.start_year as start_year; /* target vertex (team) property serve.start_year */ ========================== | team_name | start_year | ========================== | Nuggets | 2011 | -------------------------- | Rockets | 2017 | -------------------------- nebula> GO FROM 100,102 OVER serve \\ WHERE serve.start_year > 1995 /* check edge property */ \\ YIELD DISTINCT $$.team.name AS team_name, /* DISTINCT as SQL */ \\ serve.start_year as start_year, /* edge property */ \\ $^.player.name AS player_name /* source vertex (player) property */ ============================================== | team_name | start_year | player_name | ============================================== | Warriors | 2001 | LaMarcus Aldridge | ---------------------------------------------- | Warriors | 1997 | Tim Duncan | ---------------------------------------------- Traverse Along Multiple Edges Types \u00b6 Currently, Nebula Graph also supports traversing via multiple edge types with GO . The syntax is: GO FROM <node_list> OVER <edge_type_list | *> YIELD [DISTINCT] <return_list> For example: nebula> GO OVER FROM <node_list> edge1, edge2.... // traverse alone edge1 and edge2 or nebula> GO OVER FROM <node_list> * // * indicates traversing along all edge types Please note that when traversing along multiple edges, there are some special restrictions on the use of filters(namely the WHERE statement), for example filters like WHERE edge1.prop1 > edge2.prop2 is not supported. As for return results, if multiple edge properties are to be returned, Nebula Graph will place them in different rows. For example: nebula> GO FROM 100 OVER follow, serve YIELD follow.degree, serve.start_year; The following result is returned: ==================================== | follow.degree | serve.start_year | ==================================== | 0 | 1997 | ------------------------------------ | 95 | 0 | ------------------------------------ | 89 | 0 | ------------------------------------ | 90 | 0 | ------------------------------------ If there is no property, the default value will be placed. The default value for numeric type is 0, and for string type is an empty string, for bool is false, for timestamp is 0 (namely \u201c1970-01-01 00:00:00\u201d) and for double is 0.0. Of course, you can query without specifying `YIELD`, which returns the vids of the dest vertices of each edge. Again, default values (here is 0) will be placed if there is no property. For example, query `GO FROM 100 OVER follow, serve;` returns the follow lines: ============================ | follow._dst | serve._dst | ============================ | 0 | 200 | ---------------------------- | 101 | 0 | ---------------------------- | 102 | 0 | ---------------------------- | 106 | 0 | ---------------------------- For query statement GO FROM 100 OVER * , the result is similar to the above example: the non-existing property or vid is populated with default values. Please note that we can't tell which row belongs to which edge in the results. The future version will show the edge type in the result. Traverse Reversely \u00b6 Currently, Nebula Graph supports traversing reversely using keyword REVERSELY . The syntax is: GO FROM <node_list> OVER <edge_type_list> REVERSELY WHERE (expression [ AND | OR expression ...]) YIELD [DISTINCT] <return_list> For example: nebula> GO FROM 100 OVER follow REVERSELY YIELD follow._src; -- returns 100 nebula> GO FROM 100 OVER follow REVERSELY YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve WHERE $^.player.age > 20 YIELD $^.player.name AS FriendOf, $$.team.name AS Team; ============================ | FriendOf | Team | ============================ | Tony Parker | Warriors | ---------------------------- | Kyle Anderson | Warriors | ---------------------------- The above query first traverses players that follow player 100 and finds the teams they serve, then filter players who are older than 20, and finally it returns their names and teams. Of course, you can query without specifying YIELD, which will return the vids of the dest vertices of each edge by default. Traverse Bidirect \u00b6 Currently, Nebula Graph supports traversing along in and out edges using keyword BIDIRECT , the syntax is: GO FROM <node_list> OVER <edge_type_list> BIDIRECT WHERE (expression [ AND | OR expression ...]) YIELD [DISTINCT] <return_list> For example: nebula> GO FROM 102 OVER follow BIDIRECT; =============== | follow._dst | =============== | 101 | --------------- | 103 | --------------- | 135 | --------------- The above query returns players followed by 102 and follow 102 at the same time.","title":"GO Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/#go_syntax","text":"GO statement is the MOST commonly used clause in Nebula Graph . It indicates to traverse in a graph with specific filters (the WHERE clause), to fetch properties of vertices and edges, and return results (the YIELD clause) with given order (the ORDER BY ASC | DESC clause) and numbers (the LIMIT clause). The syntax of GO statement is very similar to SELECT in SQL. Notice that the major difference is that GO must start traversing from a (set of) vertex (vertices). GO [ <N> STEPS ] FROM <node_list> OVER <edge_type_list> [REVERSELY] [BIDIRECT] [ WHERE <expression> [ AND | OR expression ...]) ] YIELD [DISTINCT] <return_list> <node_list> | <vid> [, <vid> ...] | $-.id <edge_type_list> edge_type [, edge_type ...] * # `*` selects all the available edge types <return_list> <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] [ \\ STEPS ] specifies the N query hops is either a list of node's vid separated by comma(,), or a special place holder $-.id (refer PIPE syntax). is a list of edge types which graph traversal can go through. [ WHERE \\ ] extracts only those results that fulfill the specified conditions. WHERE syntax can be conditions for src-vertex, the edges, and dst-vertex. The logical AND, OR, NOT are also supported. See WHERE Syntax for more information. YIELD [DISTINCT] statement returns the result in column format and rename as an alias name. See YIELD syntax for more information. The DISTINCT syntax works the same as SQL.","title":"GO Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/#examples","text":"nebula> GO FROM 107 OVER serve; \\ /* start from vertex 107 along with edge type serve, and get vertex 200, 201 */ ============== | serve._dst | ============== | 200 | -------------- | 201 | -------------- nebula> GO 2 STEPS FROM 103 OVER follow; \\ /* return the 2 hop friends of the vertex 103 */ =============== | follow._dst | =============== | 101 | --------------- nebula> GO FROM 109 OVER serve \\ WHERE serve.start_year > 1990 /* check edge (serve) property ( start_year) */ \\ YIELD $$.team.name AS team_name, serve.start_year as start_year; /* target vertex (team) property serve.start_year */ ========================== | team_name | start_year | ========================== | Nuggets | 2011 | -------------------------- | Rockets | 2017 | -------------------------- nebula> GO FROM 100,102 OVER serve \\ WHERE serve.start_year > 1995 /* check edge property */ \\ YIELD DISTINCT $$.team.name AS team_name, /* DISTINCT as SQL */ \\ serve.start_year as start_year, /* edge property */ \\ $^.player.name AS player_name /* source vertex (player) property */ ============================================== | team_name | start_year | player_name | ============================================== | Warriors | 2001 | LaMarcus Aldridge | ---------------------------------------------- | Warriors | 1997 | Tim Duncan | ----------------------------------------------","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/#traverse_along_multiple_edges_types","text":"Currently, Nebula Graph also supports traversing via multiple edge types with GO . The syntax is: GO FROM <node_list> OVER <edge_type_list | *> YIELD [DISTINCT] <return_list> For example: nebula> GO OVER FROM <node_list> edge1, edge2.... // traverse alone edge1 and edge2 or nebula> GO OVER FROM <node_list> * // * indicates traversing along all edge types Please note that when traversing along multiple edges, there are some special restrictions on the use of filters(namely the WHERE statement), for example filters like WHERE edge1.prop1 > edge2.prop2 is not supported. As for return results, if multiple edge properties are to be returned, Nebula Graph will place them in different rows. For example: nebula> GO FROM 100 OVER follow, serve YIELD follow.degree, serve.start_year; The following result is returned: ==================================== | follow.degree | serve.start_year | ==================================== | 0 | 1997 | ------------------------------------ | 95 | 0 | ------------------------------------ | 89 | 0 | ------------------------------------ | 90 | 0 | ------------------------------------ If there is no property, the default value will be placed. The default value for numeric type is 0, and for string type is an empty string, for bool is false, for timestamp is 0 (namely \u201c1970-01-01 00:00:00\u201d) and for double is 0.0. Of course, you can query without specifying `YIELD`, which returns the vids of the dest vertices of each edge. Again, default values (here is 0) will be placed if there is no property. For example, query `GO FROM 100 OVER follow, serve;` returns the follow lines: ============================ | follow._dst | serve._dst | ============================ | 0 | 200 | ---------------------------- | 101 | 0 | ---------------------------- | 102 | 0 | ---------------------------- | 106 | 0 | ---------------------------- For query statement GO FROM 100 OVER * , the result is similar to the above example: the non-existing property or vid is populated with default values. Please note that we can't tell which row belongs to which edge in the results. The future version will show the edge type in the result.","title":"Traverse Along Multiple Edges Types"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/#traverse_reversely","text":"Currently, Nebula Graph supports traversing reversely using keyword REVERSELY . The syntax is: GO FROM <node_list> OVER <edge_type_list> REVERSELY WHERE (expression [ AND | OR expression ...]) YIELD [DISTINCT] <return_list> For example: nebula> GO FROM 100 OVER follow REVERSELY YIELD follow._src; -- returns 100 nebula> GO FROM 100 OVER follow REVERSELY YIELD follow._dst AS id | \\ GO FROM $-.id OVER serve WHERE $^.player.age > 20 YIELD $^.player.name AS FriendOf, $$.team.name AS Team; ============================ | FriendOf | Team | ============================ | Tony Parker | Warriors | ---------------------------- | Kyle Anderson | Warriors | ---------------------------- The above query first traverses players that follow player 100 and finds the teams they serve, then filter players who are older than 20, and finally it returns their names and teams. Of course, you can query without specifying YIELD, which will return the vids of the dest vertices of each edge by default.","title":"Traverse Reversely"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/go-syntax/#traverse_bidirect","text":"Currently, Nebula Graph supports traversing along in and out edges using keyword BIDIRECT , the syntax is: GO FROM <node_list> OVER <edge_type_list> BIDIRECT WHERE (expression [ AND | OR expression ...]) YIELD [DISTINCT] <return_list> For example: nebula> GO FROM 102 OVER follow BIDIRECT; =============== | follow._dst | =============== | 101 | --------------- | 103 | --------------- | 135 | --------------- The above query returns players followed by 102 and follow 102 at the same time.","title":"Traverse Bidirect"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-edge-syntax/","text":"INSERT EDGE Syntax \u00b6 INSERT EDGE <edge_name> ( <prop_name_list> ) VALUES | VALUE <src_vid> -> <dst_vid>[@<ranking>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid> : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] INSERT EDGE statement inserts a (directed) edge from a starting vertex (given by src_vid) to an ending vertex (given by dst_vid). <edge_name> denotes the edge type, which must be created before INSERT EDGE . <prop_name_list> is the property name list as the given <edge_name> . <prop_value_list> must provide the value list according to <prop_name_list> . If no value matches the type, an error will be returned. ranking is optional, it specifies the edge ranking of the same edge type, if not specified, the default value is 0. Examples \u00b6 nebula> CREATE EDGE e1() -- create edge t1 with empty property or default values nebula> INSERT EDGE e1 () VALUES 10->11:() -- insert an edge from vertex 10 to vertex 11 with empty property nebula> INSERT EDGE e1 () VALUES 10->11@1:() -- insert an edge from vertex 10 to vertex 11 with empty property, the edge ranking is 1 nebula> CREATE EDGE e2 (name string, age int) -- create edge e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 1) -- insert edge from 11 to 13 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ 12->13:(\"n1\", 1), 13->14:(\"n2\", 2) -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", \"a13\") -- ERROR. \"a13\" is not int An edge can be inserted/wrote multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 12) nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 13) nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 14) -- the last version can be read","title":"INSERT EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-edge-syntax/#insert_edge_syntax","text":"INSERT EDGE <edge_name> ( <prop_name_list> ) VALUES | VALUE <src_vid> -> <dst_vid>[@<ranking>] : ( <prop_value_list> ) [, <src_vid> -> <dst_vid> : ( <prop_value_list> ), ...] <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...] <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...] INSERT EDGE statement inserts a (directed) edge from a starting vertex (given by src_vid) to an ending vertex (given by dst_vid). <edge_name> denotes the edge type, which must be created before INSERT EDGE . <prop_name_list> is the property name list as the given <edge_name> . <prop_value_list> must provide the value list according to <prop_name_list> . If no value matches the type, an error will be returned. ranking is optional, it specifies the edge ranking of the same edge type, if not specified, the default value is 0.","title":"INSERT EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-edge-syntax/#examples","text":"nebula> CREATE EDGE e1() -- create edge t1 with empty property or default values nebula> INSERT EDGE e1 () VALUES 10->11:() -- insert an edge from vertex 10 to vertex 11 with empty property nebula> INSERT EDGE e1 () VALUES 10->11@1:() -- insert an edge from vertex 10 to vertex 11 with empty property, the edge ranking is 1 nebula> CREATE EDGE e2 (name string, age int) -- create edge e2 with two properties nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 1) -- insert edge from 11 to 13 with two properties nebula> INSERT EDGE e2 (name, age) VALUES \\ 12->13:(\"n1\", 1), 13->14:(\"n2\", 2) -- insert two edges nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", \"a13\") -- ERROR. \"a13\" is not int An edge can be inserted/wrote multiple times. Only the last written values can be read. -- insert edge with the new values. nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 12) nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 13) nebula> INSERT EDGE e2 (name, age) VALUES 11->13:(\"n1\", 14) -- the last version can be read","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-vertex-syntax/","text":"INSERT VERTEX Syntax \u00b6 INSERT VERTEX <tag_name> [, <tag_name>, ...] (prop_name_list[, prop_name_list]) {VALUES | VALUE} vid: (prop_value_list[, prop_value_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] INSERT VERTEX statement inserts one vertex into Nebula Graph . tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list is the property name list in the given tag_name . prop_value_list must provide the value list according to the prop_name_list . If no value matches the type, an error will be returned. Examples \u00b6 nebula> CREATE TAG t1() -- create tag t1 with empty property nebula> INSERT VERTEX t1 () VALUES 10:() -- insert vertex 10 with no property nebula> CREATE TAG t2 (name string, age int) -- create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n1\", 12) -- insert vertex 11 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES 12:(\"n1\", \"a13\") -- ERROR. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES 13:(\"n3\", 12), 14:(\"n4\", 8) -- insert two vertices nebula> CREATE TAG t1(i1 int) nebula> CREATE TAG t2(s2 string) nebula> INSERT VERTEX t1 (i1), t2(s2) VALUES 21: (321, \"hello\") -- insert vertex 21 with two tags. A vertex can be inserted/wrote multiple times. Only the last written values can be read. -- insert vertex 11 with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n2\", 13) nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n3\", 14) nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n4\", 15) -- the last version can be read","title":"INSERT VERTEX Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-vertex-syntax/#insert_vertex_syntax","text":"INSERT VERTEX <tag_name> [, <tag_name>, ...] (prop_name_list[, prop_name_list]) {VALUES | VALUE} vid: (prop_value_list[, prop_value_list]) prop_name_list: [prop_name [, prop_name] ...] prop_value_list: [prop_value [, prop_value] ...] INSERT VERTEX statement inserts one vertex into Nebula Graph . tag_name denotes the tag (vertex type), which must be created before INSERT VERTEX . prop_name_list is the property name list in the given tag_name . prop_value_list must provide the value list according to the prop_name_list . If no value matches the type, an error will be returned.","title":"INSERT VERTEX Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/insert-vertex-syntax/#examples","text":"nebula> CREATE TAG t1() -- create tag t1 with empty property nebula> INSERT VERTEX t1 () VALUES 10:() -- insert vertex 10 with no property nebula> CREATE TAG t2 (name string, age int) -- create tag t2 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n1\", 12) -- insert vertex 11 with two properties nebula> INSERT VERTEX t2 (name, age) VALUES 12:(\"n1\", \"a13\") -- ERROR. \"a13\" is not int nebula> INSERT VERTEX t2 (name, age) VALUES 13:(\"n3\", 12), 14:(\"n4\", 8) -- insert two vertices nebula> CREATE TAG t1(i1 int) nebula> CREATE TAG t2(s2 string) nebula> INSERT VERTEX t1 (i1), t2(s2) VALUES 21: (321, \"hello\") -- insert vertex 21 with two tags. A vertex can be inserted/wrote multiple times. Only the last written values can be read. -- insert vertex 11 with the new values. nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n2\", 13) nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n3\", 14) nebula> INSERT VERTEX t2 (name, age) VALUES 11:(\"n4\", 15) -- the last version can be read","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/lookup-syntax/","text":"LOOKUP Syntax \u00b6 The LOOKUP statement is used to search for the filter condition in it. LOOKUP is often coupled with a WHERE clause which adds filters or predicates. Note: Before using the LOOKUP statement, please make sure that indexes are created. Read more about indexes in Index Documentation . LOOKUP ON {<vertex_tag> | <edge_type>} WHERE <expression> [ AND | OR expression ...]) ] [YIELD <return_list>] <return_list> <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] LOOKUP clause finds the vertices or edges. WHERE extracts only those results that fulfill the specified conditions. The logical AND, OR, NOT are also supported. See WHERE Syntax for more information. Note: WHERE clause does not support the following operations in LOOKUP : $- and $^ In relational expressions, expressions with field-names on both sides of the operator are not currently supported, such as (tagName.column1> tagName.column2) Nested AliasProp expressions in operation expressions and function expressions are not supported at this time. YIELD clause returns particular results. If not specified, vertex ID is returned when LOOKUP tags, source vertex ID, dest vertex ID and ranking of the edges are returned when LOOKUP edges. Retrieve Vertices \u00b6 The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name, age); nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name== \"Kobe Bryant\" YIELD player.name AS name | \\ GO FROM $-.VertexID OVER serve YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------ Retrieve Edges \u00b6 The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree | \\ GO FROM $-.DstVID OVER serve YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ----------------------------------------------------------------","title":"LOOK UP Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/lookup-syntax/#lookup_syntax","text":"The LOOKUP statement is used to search for the filter condition in it. LOOKUP is often coupled with a WHERE clause which adds filters or predicates. Note: Before using the LOOKUP statement, please make sure that indexes are created. Read more about indexes in Index Documentation . LOOKUP ON {<vertex_tag> | <edge_type>} WHERE <expression> [ AND | OR expression ...]) ] [YIELD <return_list>] <return_list> <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] LOOKUP clause finds the vertices or edges. WHERE extracts only those results that fulfill the specified conditions. The logical AND, OR, NOT are also supported. See WHERE Syntax for more information. Note: WHERE clause does not support the following operations in LOOKUP : $- and $^ In relational expressions, expressions with field-names on both sides of the operator are not currently supported, such as (tagName.column1> tagName.column2) Nested AliasProp expressions in operation expressions and function expressions are not supported at this time. YIELD clause returns particular results. If not specified, vertex ID is returned when LOOKUP tags, source vertex ID, dest vertex ID and ranking of the edges are returned when LOOKUP edges.","title":"LOOKUP Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/lookup-syntax/#retrieve_vertices","text":"The following example returns vertices whose name is Tony Parker and tagged with player . nebula> CREATE TAG INDEX index_player ON player(name, age); nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\"; ============ | VertexID | ============ | 101 | ------------ nebula> LOOKUP ON player WHERE player.name == \"Tony Parker\" \\ YIELD player.name, player.age; ======================================= | VertexID | player.name | player.age | ======================================= | 101 | Tony Parker | 36 | --------------------------------------- nebula> LOOKUP ON player WHERE player.name== \"Kobe Bryant\" YIELD player.name AS name | \\ GO FROM $-.VertexID OVER serve YIELD $-.name, serve.start_year, serve.end_year, $$.team.name; ================================================================== | $-.name | serve.start_year | serve.end_year | $$.team.name | ================================================================== | Kobe Bryant | 1996 | 2016 | Lakers | ------------------------------------------------------------------","title":"Retrieve Vertices"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/lookup-syntax/#retrieve_edges","text":"The following example returns edges whose degree is 90 and the edge type is follow . nebula> CREATE EDGE INDEX index_follow ON follow(degree); nebula> LOOKUP ON follow WHERE follow.degree == 90; ============================= | SrcVID | DstVID | Ranking | ============================= | 100 | 106 | 0 | ----------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 90 YIELD follow.degree; ============================================= | SrcVID | DstVID | Ranking | follow.degree | ============================================= | 100 | 106 | 0 | 90 | --------------------------------------------- nebula> LOOKUP ON follow WHERE follow.degree == 60 YIELD follow.degree AS Degree | \\ GO FROM $-.DstVID OVER serve YIELD $-.DstVID, serve.start_year, serve.end_year, $$.team.name; ================================================================ | $-.DstVID | serve.start_year | serve.end_year | $$.team.name | ================================================================ | 105 | 2010 | 2018 | Spurs | ---------------------------------------------------------------- | 105 | 2009 | 2010 | Cavaliers | ---------------------------------------------------------------- | 105 | 2018 | 2019 | Raptors | ----------------------------------------------------------------","title":"Retrieve Edges"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/return-syntax/","text":"RETURN Syntax \u00b6 The RETURN statement is used to return the result when the condition is true. If the condition is false, no result is returned. RETURN <var_ref> IF <var_ref> IS NOT NULL is a variable name, e.g. $var . Examples \u00b6 nebula> $A = GO FROM 100 OVER follow YIELD follow._dst AS dst; \\ $rA = YIELD $A.* WHERE $A.dst == 101; \\ RETURN $rA IF $rA is NOT NULL; \\ /* Returns the result because $rA is not empty */ GO FROM $A.dst OVER follow; /* As the RETURN statement returns the result, the GO FROM statement is not executed*/ ========== | $A.dst | ========== | 101 | ---------- nebula> $A = GO FROM 100 OVER follow YIELD follow._dst AS dst; \\ $rA = YIELD $A.* WHERE $A.dst == 300; \\ RETURN $rA IF $rA is NOT NULL; \\ /* Does not return the result because $rA is empty */ GO FROM $A.dst OVER follow; /* As the RETURN statement does not return the result, the GO FROM statement is executed */ =============== | follow._dst | =============== | 100 | --------------- | 101 | --------------- | 100 | --------------- | 102 | --------------- | 100 | --------------- | 107 | ---------------","title":"RETURN Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/return-syntax/#return_syntax","text":"The RETURN statement is used to return the result when the condition is true. If the condition is false, no result is returned. RETURN <var_ref> IF <var_ref> IS NOT NULL is a variable name, e.g. $var .","title":"RETURN Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/return-syntax/#examples","text":"nebula> $A = GO FROM 100 OVER follow YIELD follow._dst AS dst; \\ $rA = YIELD $A.* WHERE $A.dst == 101; \\ RETURN $rA IF $rA is NOT NULL; \\ /* Returns the result because $rA is not empty */ GO FROM $A.dst OVER follow; /* As the RETURN statement returns the result, the GO FROM statement is not executed*/ ========== | $A.dst | ========== | 101 | ---------- nebula> $A = GO FROM 100 OVER follow YIELD follow._dst AS dst; \\ $rA = YIELD $A.* WHERE $A.dst == 300; \\ RETURN $rA IF $rA is NOT NULL; \\ /* Does not return the result because $rA is empty */ GO FROM $A.dst OVER follow; /* As the RETURN statement does not return the result, the GO FROM statement is executed */ =============== | follow._dst | =============== | 100 | --------------- | 101 | --------------- | 100 | --------------- | 102 | --------------- | 100 | --------------- | 107 | ---------------","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/update-vertex-edge-syntax/","text":"UPDATE Syntax \u00b6 Nebula Graph supports UPDATE properties of a vertex or an edge, as well as CAS operation and returning related properties. Update Vertex \u00b6 UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] NOTE: WHEN and YIELD are optional. vid is the id of the vertex to be updated. update_columns is the properties of the vertex to be updated, for example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates vertex to be updated. condition is some constraints, only when met, UPDATE will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX 101 SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tony Parker\" \\ YIELD $^.player.name AS name, $^.player.age AS age; There are one tag in vertex 101, namely player. Update Edge \u00b6 UPDATE EDGE <edge> SET <update_columns> [WHEN <condition>] [YIELD <columns>] NOTE: WHEN and YIELD are optional. edge is the edge to be updated, the syntax is <src> -> <dst> [@ranking] OF <edge_type> . update_columns is the properties of the edge to be updated. condition is some constraints, only when met, UPDATE will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE 100 -> 200@0 OF serve SET start_year = serve.start_year + 1 \\ YIELD $^.player.name AS name, serve.start_year AS start;","title":"UPDATE VERTEX EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/update-vertex-edge-syntax/#update_syntax","text":"Nebula Graph supports UPDATE properties of a vertex or an edge, as well as CAS operation and returning related properties.","title":"UPDATE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/update-vertex-edge-syntax/#update_vertex","text":"UPDATE VERTEX <vid> SET <update_columns> [WHEN <condition>] [YIELD <columns>] NOTE: WHEN and YIELD are optional. vid is the id of the vertex to be updated. update_columns is the properties of the vertex to be updated, for example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates vertex to be updated. condition is some constraints, only when met, UPDATE will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE VERTEX 101 SET player.age = $^.player.age + 1 \\ WHEN $^.player.name == \"Tony Parker\" \\ YIELD $^.player.name AS name, $^.player.age AS age; There are one tag in vertex 101, namely player.","title":"Update Vertex"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/update-vertex-edge-syntax/#update_edge","text":"UPDATE EDGE <edge> SET <update_columns> [WHEN <condition>] [YIELD <columns>] NOTE: WHEN and YIELD are optional. edge is the edge to be updated, the syntax is <src> -> <dst> [@ranking] OF <edge_type> . update_columns is the properties of the edge to be updated. condition is some constraints, only when met, UPDATE will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> UPDATE EDGE 100 -> 200@0 OF serve SET start_year = serve.start_year + 1 \\ YIELD $^.player.name AS name, serve.start_year AS start;","title":"Update Edge"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/upsert-syntax/","text":"UPSERT Syntax \u00b6 UPSERT is used to insert a new vertex or edge or update an existing one. If the vertex or edge doesn\u2019t exist it will be created. UPSERT is a combination of INSERT and UPDATE . If the vertex or edge does not exist, a new one will be created regardless of whether the condition in WHEN clause is met; If the vertex or edge exists and the WHEN condition is met, the vertex or edge will be updated; If the vertex or edge exists and the WHEN condition is not met, nothing will be done. UPSERT {VERTEX <vid> | EDGE <edge>} SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. edge is the edge to be updated, the syntax is <src> -> <dst> [@ranking] OF <edge_type> . update_columns is the properties of the vertex or edge to be updated, for example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates vertex to be updated. condition is some constraints, only when met, UPSERT will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES 111:(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX 111 SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" && $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do upsert on the vertex. ======================= | Name | Age | ======================= | Dwight Howard | 33 | -----------------------","title":"UPSERT Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/upsert-syntax/#upsert_syntax","text":"UPSERT is used to insert a new vertex or edge or update an existing one. If the vertex or edge doesn\u2019t exist it will be created. UPSERT is a combination of INSERT and UPDATE . If the vertex or edge does not exist, a new one will be created regardless of whether the condition in WHEN clause is met; If the vertex or edge exists and the WHEN condition is met, the vertex or edge will be updated; If the vertex or edge exists and the WHEN condition is not met, nothing will be done. UPSERT {VERTEX <vid> | EDGE <edge>} SET <update_columns> [WHEN <condition>] [YIELD <columns>] vid is the ID of the vertex to be updated. edge is the edge to be updated, the syntax is <src> -> <dst> [@ranking] OF <edge_type> . update_columns is the properties of the vertex or edge to be updated, for example, tag1.col1 = $^.tag2.col2 + 1 means to update tag1.col1 to tag2.col2+1 . NOTE: $^ indicates vertex to be updated. condition is some constraints, only when met, UPSERT will run successfully and expression operations are supported. columns is the columns to be returned, YIELD returns the latest updated values. Consider the following example: nebula> INSERT VERTEX player(name, age) VALUES 111:(\"Ben Simmons\", 22); -- Insert a new vertex. nebula> UPSERT VERTEX 111 SET player.name = \"Dwight Howard\", player.age = $^.player.age + 11 WHEN $^.player.name == \"Ben Simmons\" && $^.player.age > 20 YIELD $^.player.name AS Name, $^.player.age AS Age; -- Do upsert on the vertex. ======================= | Name | Age | ======================= | Dwight Howard | 33 | -----------------------","title":"UPSERT Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/where-syntax/","text":"WHERE Syntax \u00b6 Currently, the WHERE statement only applies to the GO statement. WHERE <expression> [ AND | OR <expression> ...]) Usually, WHERE is a set of logical combination that filters vertex or edge properties. As syntactic sugar, logic AND is represented by AND or && and logic OR is represented by OR or || . Examples \u00b6 -- the degree property of edge follow is greater than 90. nebula> GO FROM 100 OVER follow WHERE follow.degree > 90; -- the following result is returned: =============== | follow._dst | =============== | 101 | --------------- -- find the dest vertex whose age is equal to the source vertex, player 104. nebula> GO FROM 104 OVER follow WHERE $^.player.age == $$.player.age; -- the following result is returned: =============== | follow._dst | =============== | 103 | --------------- -- logical combination is allowed. nebula> GO FROM 100 OVER follow WHERE follow.degree > 90 OR $$.player.age != 33 AND $$.player.name != \"Tony Parker\"; -- the following result is returned: =============== | follow._dst | =============== | 101 | --------------- | 106 | --------------- -- the condition in the WHERE clause is always TRUE. nebula> GO FROM 101 OVER follow WHERE 1 == 1 OR TRUE; -- the following result is returned: =============== | follow._dst | =============== | 100 | --------------- | 102 | ---------------","title":"WHERE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/where-syntax/#where_syntax","text":"Currently, the WHERE statement only applies to the GO statement. WHERE <expression> [ AND | OR <expression> ...]) Usually, WHERE is a set of logical combination that filters vertex or edge properties. As syntactic sugar, logic AND is represented by AND or && and logic OR is represented by OR or || .","title":"WHERE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/where-syntax/#examples","text":"-- the degree property of edge follow is greater than 90. nebula> GO FROM 100 OVER follow WHERE follow.degree > 90; -- the following result is returned: =============== | follow._dst | =============== | 101 | --------------- -- find the dest vertex whose age is equal to the source vertex, player 104. nebula> GO FROM 104 OVER follow WHERE $^.player.age == $$.player.age; -- the following result is returned: =============== | follow._dst | =============== | 103 | --------------- -- logical combination is allowed. nebula> GO FROM 100 OVER follow WHERE follow.degree > 90 OR $$.player.age != 33 AND $$.player.name != \"Tony Parker\"; -- the following result is returned: =============== | follow._dst | =============== | 101 | --------------- | 106 | --------------- -- the condition in the WHERE clause is always TRUE. nebula> GO FROM 101 OVER follow WHERE 1 == 1 OR TRUE; -- the following result is returned: =============== | follow._dst | =============== | 100 | --------------- | 102 | ---------------","title":"Examples"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/","text":"YIELD Syntax \u00b6 Keyword YIELD can be used as a clause in a FETCH or GO statement, or as a separate statement in PIPE ( | ), or as a stand-alone statement for calculation. As Clause (With GO-Syntax) \u00b6 YIELD [DISTINCT] <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] YIELD is commonly used to return results generated with GO (Refer GO ). nebula> GO FROM 100 OVER follow YIELD $$.player.name AS Friend, $$.player.age AS Age; =========================== | Friend | Age | =========================== | Tony Parker | 36 | --------------------------- | LaMarcus Aldridge | 33 | --------------------------- | Kyle Anderson | 25 | --------------------------- For example: $$.player.name is used to get the property of the dest vertex ($$). As Statement \u00b6 Reference Inputs or Variables \u00b6 You can use the YIELD statement in PIPE . You can use the YIELD statement to reference variables. For statements that do not support YIELD statement, you can use it as a tool to control the output. YIELD [DISTINCT] <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] [WHERE <conditions>] nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | YIELD $-.* WHERE $-.id == 106; ========= | $-.id | ========= | 106 | --------- nebula> $var1 = GO FROM 101 OVER follow; $var2 = GO FROM 105 OVER follow; YIELD $var1.* UNION YIELD $var2.*; ===================== | $var1.follow._dst | ===================== | 100 | --------------------- | 102 | --------------------- | 104 | --------------------- | 110 | --------------------- As Stand-alone Statement \u00b6 YIELD statement can be used independently to retrieve computation results without reference to any graph. You can use AS to rename it an alias. nebula> YIELD 1 + 1; ========= | (1+1) | ========= | 2 | --------- nebula> YIELD \"Hel\" + \"\\tlo\" AS HELLO_1, \", World!\" AS WORLD_2; ====================== | HELLO_1 | WORLD_2 | ====================== | Hel lo | , World! | ---------------------- nebula> YIELD hash(\"Tim\") % 100; ===================== | (hash(\"Tim\")%100) | ===================== | 42 | --------------------- Note: You can not use YIELD DISTINCT as a stand-alone statement. The following is a syntax error. nebula> YIELD DISTINCT 1 --- syntax error!","title":"YIELD Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/#yield_syntax","text":"Keyword YIELD can be used as a clause in a FETCH or GO statement, or as a separate statement in PIPE ( | ), or as a stand-alone statement for calculation.","title":"YIELD Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/#as_clause_with_go-syntax","text":"YIELD [DISTINCT] <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] YIELD is commonly used to return results generated with GO (Refer GO ). nebula> GO FROM 100 OVER follow YIELD $$.player.name AS Friend, $$.player.age AS Age; =========================== | Friend | Age | =========================== | Tony Parker | 36 | --------------------------- | LaMarcus Aldridge | 33 | --------------------------- | Kyle Anderson | 25 | --------------------------- For example: $$.player.name is used to get the property of the dest vertex ($$).","title":"As Clause (With GO-Syntax)"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/#as_statement","text":"","title":"As Statement"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/#reference_inputs_or_variables","text":"You can use the YIELD statement in PIPE . You can use the YIELD statement to reference variables. For statements that do not support YIELD statement, you can use it as a tool to control the output. YIELD [DISTINCT] <col_name> [AS <col_alias>] [, <col_name> [AS <col_alias>] ...] [WHERE <conditions>] nebula> GO FROM 100 OVER follow YIELD follow._dst AS id | YIELD $-.* WHERE $-.id == 106; ========= | $-.id | ========= | 106 | --------- nebula> $var1 = GO FROM 101 OVER follow; $var2 = GO FROM 105 OVER follow; YIELD $var1.* UNION YIELD $var2.*; ===================== | $var1.follow._dst | ===================== | 100 | --------------------- | 102 | --------------------- | 104 | --------------------- | 110 | ---------------------","title":"Reference Inputs or Variables"},{"location":"manual-EN/2.query-language/4.statement-syntax/2.data-query-and-manipulation-statements/yield-syntax/#as_stand-alone_statement","text":"YIELD statement can be used independently to retrieve computation results without reference to any graph. You can use AS to rename it an alias. nebula> YIELD 1 + 1; ========= | (1+1) | ========= | 2 | --------- nebula> YIELD \"Hel\" + \"\\tlo\" AS HELLO_1, \", World!\" AS WORLD_2; ====================== | HELLO_1 | WORLD_2 | ====================== | Hel lo | , World! | ---------------------- nebula> YIELD hash(\"Tim\") % 100; ===================== | (hash(\"Tim\")%100) | ===================== | 42 | --------------------- Note: You can not use YIELD DISTINCT as a stand-alone statement. The following is a syntax error. nebula> YIELD DISTINCT 1 --- syntax error!","title":"As Stand-alone Statement"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/describe-syntax/","text":"DESCRIBE Syntax \u00b6 DESCRIBE SPACE <space_name> DESCRIBE TAG <tag_name> DESCRIBE EDGE <edge_name> DESCRIBE {TAG | EDGE} INDEX <index_name> The DESCRIBE keyword is used to obtain information about space, tag and edge structure. Also notice that DESCRIBE is different from SHOW. Refer SHOW . Example \u00b6 Obtain information about space. nebula> DESCRIBE SPACE nba; ======================================================== | ID | Name | Partition number | Replica Factor | ======================================================== | 1 | nba | 100 | 1 | -------------------------------------------------------- Obtain information about tag in a given space. nebula> DESCRIBE TAG player; ================================================== | Field | Type | Null | Key | Default | Extra | ================================================== | name | string | false | | | | -------------------------------------------------- | age | int | false | | | | -------------------------------------------------- Obtain information about edge in a given space. nebula> DESCRIBE EDGE serve; ====================================================== | Field | Type | Null | Key | Default | Extra | ====================================================== | start_year | int | false | | | | ------------------------------------------------------ | end_year | int | false | | | | ------------------------------------------------------ Obtain information about the index. nebula> DESCRIBE TAG INDEX player_index_0; ================== | Field | Type | ================== | name | string | ------------------","title":"DESCRIBE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/describe-syntax/#describe_syntax","text":"DESCRIBE SPACE <space_name> DESCRIBE TAG <tag_name> DESCRIBE EDGE <edge_name> DESCRIBE {TAG | EDGE} INDEX <index_name> The DESCRIBE keyword is used to obtain information about space, tag and edge structure. Also notice that DESCRIBE is different from SHOW. Refer SHOW .","title":"DESCRIBE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/describe-syntax/#example","text":"Obtain information about space. nebula> DESCRIBE SPACE nba; ======================================================== | ID | Name | Partition number | Replica Factor | ======================================================== | 1 | nba | 100 | 1 | -------------------------------------------------------- Obtain information about tag in a given space. nebula> DESCRIBE TAG player; ================================================== | Field | Type | Null | Key | Default | Extra | ================================================== | name | string | false | | | | -------------------------------------------------- | age | int | false | | | | -------------------------------------------------- Obtain information about edge in a given space. nebula> DESCRIBE EDGE serve; ====================================================== | Field | Type | Null | Key | Default | Extra | ====================================================== | start_year | int | false | | | | ------------------------------------------------------ | end_year | int | false | | | | ------------------------------------------------------ Obtain information about the index. nebula> DESCRIBE TAG INDEX player_index_0; ================== | Field | Type | ================== | name | string | ------------------","title":"Example"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/use-syntax/","text":"USE Syntax \u00b6 USE <graph_space_name> The USE statement tells Nebula Graph to use the named (graph) space as the current working space for subsequent statements. This statement requires some privileges. The named space remains the default until the end of the session or another USE statement is issued: nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; Different from SQL, making a space as the working space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. SPACES are FULLY ISOLATED from each other. Unlike SQL, which allows you to select two tables from different databases in one statement, in Nebula Graph , you can only touch one space at a time.","title":"USE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/use-syntax/#use_syntax","text":"USE <graph_space_name> The USE statement tells Nebula Graph to use the named (graph) space as the current working space for subsequent statements. This statement requires some privileges. The named space remains the default until the end of the session or another USE statement is issued: nebula> USE space1; -- Traverse in graph space1. nebula> GO FROM 1 OVER edge1; nebula> USE space2; -- Traverse in graph space2. These vertices and edges have no relevance with space1. nebula> GO FROM 2 OVER edge2; -- Now you are back to space1. Hereafter, you can not read any data from space2. nebula> USE space1; Different from SQL, making a space as the working space prevents you from accessing other spaces. The only way to traverse in a new graph space is to switch by the USE statement. SPACES are FULLY ISOLATED from each other. Unlike SQL, which allows you to select two tables from different databases in one statement, in Nebula Graph , you can only touch one space at a time.","title":"USE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-charset-syntax/","text":"SHOW CHARSET Syntax \u00b6 SHOW CHARSET SHOW CHARSET displays the available character sets. Currently available types are: utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four byte characters. Therefore utf8 and utf8mb4 equivalent. nebula> SHOW CHARSET; ======================================================== | Charset | Description | Default collation | Maxlen | ======================================================== | utf8 | UTF-8 Unicode | utf8_bin | 4 | -------------------------------------------------------- SHOW CHARSET output has these columns: Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"SHOW CHARSET Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-charset-syntax/#show_charset_syntax","text":"SHOW CHARSET SHOW CHARSET displays the available character sets. Currently available types are: utf8 and utf8mb4. The default charset type is utf8. Nebula Graph extends the uft8 to support four byte characters. Therefore utf8 and utf8mb4 equivalent. nebula> SHOW CHARSET; ======================================================== | Charset | Description | Default collation | Maxlen | ======================================================== | utf8 | UTF-8 Unicode | utf8_bin | 4 | -------------------------------------------------------- SHOW CHARSET output has these columns: Charset The character set name. Description A description of the character set. Default collation The default collation for the character set. Maxlen The maximum number of bytes required to store one character.","title":"SHOW CHARSET Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-collation-syntax/","text":"SHOW COLLATION Syntax \u00b6 SHOW COLLATION SHOW COLLATION displays the collations supported by Nebula Graph . Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive comparisons and behave the same as MySQL. nebula> SHOW COLLATION; ======================= | Collation | Charset | ======================= | utf8_bin | utf8 | ----------------------- SHOW COLLATION output has these columns: Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"SHOW COLLATION Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-collation-syntax/#show_collation_syntax","text":"SHOW COLLATION SHOW COLLATION displays the collations supported by Nebula Graph . Currently available types are: utf8_bin, utf8_general_ci, utf8mb4_bin and utf8mb4_general_ci. When the character set is utf8, the default collate is utf8_bin; when the character set is utf8mb4, the default collate is utf8mb4_bin. Both utf8_general_ci and utf8mb4_general_ci are case-insensitive comparisons and behave the same as MySQL. nebula> SHOW COLLATION; ======================= | Collation | Charset | ======================= | utf8_bin | utf8 | ----------------------- SHOW COLLATION output has these columns: Collation The collation name. Charset The name of the character set with which the collation is associated.","title":"SHOW COLLATION Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-configs-syntax/","text":"SHOW CONFIGS Syntax \u00b6 SHOW CONFIGS [graph|meta|storage] SHOW CONFIGS lists the configuration information. SHOW CONFIGS output has these columns: module, name, type, mode and value. For example: nebula> SHOW CONFIGS meta; ============================================================================================================================ | module | name | type | mode | value | ============================================================================================================================ | META | v | INT64 | IMMUTABLE | 4 | ---------------------------------------------------------------------------------------------------------------------------- | META | help | BOOL | IMMUTABLE | False | ---------------------------------------------------------------------------------------------------------------------------- | META | port | INT64 | IMMUTABLE | 45500 | ---------------------------------------------------------------------------------------------------------------------------- For more information about SHOW CONFIGS [graph|meta|storage] , please refer to configs syntax .","title":"SHOW CONFIGS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-configs-syntax/#show_configs_syntax","text":"SHOW CONFIGS [graph|meta|storage] SHOW CONFIGS lists the configuration information. SHOW CONFIGS output has these columns: module, name, type, mode and value. For example: nebula> SHOW CONFIGS meta; ============================================================================================================================ | module | name | type | mode | value | ============================================================================================================================ | META | v | INT64 | IMMUTABLE | 4 | ---------------------------------------------------------------------------------------------------------------------------- | META | help | BOOL | IMMUTABLE | False | ---------------------------------------------------------------------------------------------------------------------------- | META | port | INT64 | IMMUTABLE | 45500 | ---------------------------------------------------------------------------------------------------------------------------- For more information about SHOW CONFIGS [graph|meta|storage] , please refer to configs syntax .","title":"SHOW CONFIGS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-create-space-syntax/","text":"SHOW CREATE SPACE Syntax \u00b6 SHOW CREATE SPACE <space_name> SHOW CREATE SPACE statement returns the specified graph space and its creation syntax. If the graph space contains a default value, the default value is also returned. nebula> SHOW CREATE SPACE NBA; ========================================================================================================= | Space | Create Space | ========================================================================================================= | gods | CREATE SPACE gods (partition_num = 1, replica_factor = 1, charset = utf8, collate = utf8_bin) | ---------------------------------------------------------------------------------------------------------","title":"SHOW CREATE SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-create-space-syntax/#show_create_space_syntax","text":"SHOW CREATE SPACE <space_name> SHOW CREATE SPACE statement returns the specified graph space and its creation syntax. If the graph space contains a default value, the default value is also returned. nebula> SHOW CREATE SPACE NBA; ========================================================================================================= | Space | Create Space | ========================================================================================================= | gods | CREATE SPACE gods (partition_num = 1, replica_factor = 1, charset = utf8, collate = utf8_bin) | ---------------------------------------------------------------------------------------------------------","title":"SHOW CREATE SPACE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-create-tag-edge-syntax/","text":"SHOW CREATE TAGS/EDGES Syntax \u00b6 SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} SHOW CREATE TAG and SHOW CREATE EDGE return the specified tag or edge type and their creation syntax in a given space. If the tag or edge type contains a default value, the default value is also returned. nebula> SHOW CREATE TAG player; ========================================================================================== | Tag | Create Tag | ========================================================================================== | player | CREATE TAG player ( name string, age int ) ttl_duration = 0, ttl_col = \"\" | ------------------------------------------------------------------------------------------","title":"SHOW CREATE TAG EDGE Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-create-tag-edge-syntax/#show_create_tagsedges_syntax","text":"SHOW CREATE {TAG <tag_name> | EDGE <edge_name>} SHOW CREATE TAG and SHOW CREATE EDGE return the specified tag or edge type and their creation syntax in a given space. If the tag or edge type contains a default value, the default value is also returned. nebula> SHOW CREATE TAG player; ========================================================================================== | Tag | Create Tag | ========================================================================================== | player | CREATE TAG player ( name string, age int ) ttl_duration = 0, ttl_col = \"\" | ------------------------------------------------------------------------------------------","title":"SHOW CREATE TAGS/EDGES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-hosts-syntax/","text":"SHOW HOSTS Syntax \u00b6 SHOW HOSTS SHOW HOSTS statement lists storage hosts registered by the meta server. SHOW HOSTS output has these columns:: ip, port, status (online/offline), leader count, leader distribution, partition distribution. nebula> SHOW HOSTS; ============================================================================================= | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ============================================================================================= | 172.28.2.1 | 44500 | online | 0 | No valid partition | No valid partition | --------------------------------------------------------------------------------------------- | 172.28.2.2 | 44500 | online | 2 | NBA: 1, gods: 1 | NBA: 1, gods: 1 | --------------------------------------------------------------------------------------------- | 172.28.2.3 | 44500 | online | 0 | No valid partition | No valid partition | --------------------------------------------------------------------------------------------- | Total | | | 2 | gods: 1, NBA: 1 | gods: 1, NBA: 1 | ---------------------------------------------------------------------------------------------","title":"SHOW HOSTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-hosts-syntax/#show_hosts_syntax","text":"SHOW HOSTS SHOW HOSTS statement lists storage hosts registered by the meta server. SHOW HOSTS output has these columns:: ip, port, status (online/offline), leader count, leader distribution, partition distribution. nebula> SHOW HOSTS; ============================================================================================= | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ============================================================================================= | 172.28.2.1 | 44500 | online | 0 | No valid partition | No valid partition | --------------------------------------------------------------------------------------------- | 172.28.2.2 | 44500 | online | 2 | NBA: 1, gods: 1 | NBA: 1, gods: 1 | --------------------------------------------------------------------------------------------- | 172.28.2.3 | 44500 | online | 0 | No valid partition | No valid partition | --------------------------------------------------------------------------------------------- | Total | | | 2 | gods: 1, NBA: 1 | gods: 1, NBA: 1 | ---------------------------------------------------------------------------------------------","title":"SHOW HOSTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-index-status/","text":"SHOW INDEX STATUS Syntax \u00b6 SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the defined tag/edg-type index status. For example, list the tag index status with the following command: nebula> SHOW TAG INDEX STATUS; ========================================== | Name | Tag Index Status | ========================================== | single_person_index | SUCCEEDED | ------------------------------------------ Details on creating index refer to the Index doc.","title":"SHOW INDEX STATUS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-index-status/#show_index_status_syntax","text":"SHOW {TAG | EDGE} INDEX STATUS SHOW INDEX STATUS returns the defined tag/edg-type index status. For example, list the tag index status with the following command: nebula> SHOW TAG INDEX STATUS; ========================================== | Name | Tag Index Status | ========================================== | single_person_index | SUCCEEDED | ------------------------------------------ Details on creating index refer to the Index doc.","title":"SHOW INDEX STATUS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-indexes-syntax/","text":"SHOW INDEXES Syntax \u00b6 SHOW {TAG | EDGE} INDEXES SHOW INDEXES returns the defined tag/edg-type index information. SHOW INDEXES returns the following fields: index ID and index name.","title":"SHOW INDEXES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-indexes-syntax/#show_indexes_syntax","text":"SHOW {TAG | EDGE} INDEXES SHOW INDEXES returns the defined tag/edg-type index information. SHOW INDEXES returns the following fields: index ID and index name.","title":"SHOW INDEXES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-parts-syntax/","text":"SHOW PARTS Syntax \u00b6 SHOW PARTS <part_id> SHOW PARTS lists the partition information of the given SPACE. nebula> SHOW PARTS 1; ============================================================== | Partition ID | Leader | Peers | Losts | ============================================================== | 1 | 172.28.2.2:44500 | 172.28.2.2:44500 | | -------------------------------------------------------------- SHOW PARTS output has these columns: Partition ID Leader Peers Losts","title":"SHOW PARTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-parts-syntax/#show_parts_syntax","text":"SHOW PARTS <part_id> SHOW PARTS lists the partition information of the given SPACE. nebula> SHOW PARTS 1; ============================================================== | Partition ID | Leader | Peers | Losts | ============================================================== | 1 | 172.28.2.2:44500 | 172.28.2.2:44500 | | -------------------------------------------------------------- SHOW PARTS output has these columns: Partition ID Leader Peers Losts","title":"SHOW PARTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-roles-syntax/","text":"SHOW ROLES Syntax \u00b6 SHOW ROLES IN <space_name>> SHOW ROLES statement displays the roles that are assigned to a user account. SHOW ROLES output has these columns: account and role type.","title":"SHOW ROLES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-roles-syntax/#show_roles_syntax","text":"SHOW ROLES IN <space_name>> SHOW ROLES statement displays the roles that are assigned to a user account. SHOW ROLES output has these columns: account and role type.","title":"SHOW ROLES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-snapshots-syntax/","text":"SHOW SNAPSHOTS Syntax \u00b6 SHOW SNAPSHOTS SHOW SNAPSHOTS statement lists all the snapshots.","title":"SHOW SNAPSHOTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-snapshots-syntax/#show_snapshots_syntax","text":"SHOW SNAPSHOTS SHOW SNAPSHOTS statement lists all the snapshots.","title":"SHOW SNAPSHOTS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-spaces-syntax/","text":"SHOW SPACES Syntax \u00b6 SHOW SPACES SHOW SPACES lists the SPACES on the Nebula Graph cluster.","title":"SHOW SPACES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-spaces-syntax/#show_spaces_syntax","text":"SHOW SPACES SHOW SPACES lists the SPACES on the Nebula Graph cluster.","title":"SHOW SPACES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-tags-edges-syntax/","text":"SHOW TAGS/EDGES Syntax \u00b6 SHOW {TAGS | EDGES} SHOW TAGS and SHOW EDGES return the defined tags and edge types in a given space, respectively.","title":"SHOW TAGS EDGES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-tags-edges-syntax/#show_tagsedges_syntax","text":"SHOW {TAGS | EDGES} SHOW TAGS and SHOW EDGES return the defined tags and edge types in a given space, respectively.","title":"SHOW TAGS/EDGES Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-users-syntax/","text":"SHOW USERS Syntax \u00b6 SHOW USERS SHOW USERS lists the users information. SHOW USERS output has these columns: account names.","title":"SHOW USERS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/3.utility-statements/show-statements/show-users-syntax/#show_users_syntax","text":"SHOW USERS SHOW USERS lists the users information. SHOW USERS output has these columns: account names.","title":"SHOW USERS Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/4.graph-algorithms/find-path-syntax/","text":"FIND PATH Syntax \u00b6 FIND PATH statement can be used to get the shortest path and all paths. FIND SHORTEST | ALL PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [UPTO <N> STEPS] SHORTEST is the keyword to find the shortest path. ALL is the keyword to find all paths. <vertex_id_list>::=[vertex_id [, vertex_id]] is the vertex id list,multiple ids should be separated with commas, and $- and $var are supported. <edge_type_list> is the edge type list, multiple edge types should be separated with commas, and * can be referred as all edge types. <N> is hop number, and the default value is 5. Note \u00b6 When source and dest vertices are id lists, it means to find the shortest path from any source vertices to the dest vertices. There may be cycles when searching all paths. Examples \u00b6 Path is displayed as id <edge_name, ranking> id in console. nebula> FIND SHORTEST PATH FROM 100 to 200 OVER *; ============================= | _path_ | ============================= | 100 <serve,0> 200 ----------------------------- nebula>FIND ALL PATH FROM 100 to 200 OVER *; ============================================================================================================= | _path_ | ============================================================================================================= | 100 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 101 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 102 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 106 < serve,0> 200 -------------------------------------------------------------------------------------------------------------","title":"FIND PATH Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/4.graph-algorithms/find-path-syntax/#find_path_syntax","text":"FIND PATH statement can be used to get the shortest path and all paths. FIND SHORTEST | ALL PATH FROM <vertex_id_list> TO <vertex_id_list> OVER <edge_type_list> [UPTO <N> STEPS] SHORTEST is the keyword to find the shortest path. ALL is the keyword to find all paths. <vertex_id_list>::=[vertex_id [, vertex_id]] is the vertex id list,multiple ids should be separated with commas, and $- and $var are supported. <edge_type_list> is the edge type list, multiple edge types should be separated with commas, and * can be referred as all edge types. <N> is hop number, and the default value is 5.","title":"FIND PATH Syntax"},{"location":"manual-EN/2.query-language/4.statement-syntax/4.graph-algorithms/find-path-syntax/#note","text":"When source and dest vertices are id lists, it means to find the shortest path from any source vertices to the dest vertices. There may be cycles when searching all paths.","title":"Note"},{"location":"manual-EN/2.query-language/4.statement-syntax/4.graph-algorithms/find-path-syntax/#examples","text":"Path is displayed as id <edge_name, ranking> id in console. nebula> FIND SHORTEST PATH FROM 100 to 200 OVER *; ============================= | _path_ | ============================= | 100 <serve,0> 200 ----------------------------- nebula>FIND ALL PATH FROM 100 to 200 OVER *; ============================================================================================================= | _path_ | ============================================================================================================= | 100 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 101 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 102 < serve,0> 200 ------------------------------------------------------------------------------------------------------------- | 100 <follow,0> 106 < serve,0> 200 -------------------------------------------------------------------------------------------------------------","title":"Examples"},{"location":"manual-EN/3.build-develop-and-administration/0.README/","text":"Reader \u00b6 This chapter is mainly for engineers and administrators who want to build, deploy and manage the system.","title":"Reader"},{"location":"manual-EN/3.build-develop-and-administration/0.README/#reader","text":"This chapter is mainly for engineers and administrators who want to build, deploy and manage the system.","title":"Reader"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/","text":"Build From Source Code \u00b6 Overview \u00b6 We have tested building on various environments, including CentOS 6 to 8, Ubuntu 16.04 to 19.04, Fedora 28 to 30, GCC 7.1.0 to 9.2.0 and recent Clang++ and the devtoolset of Red Hat and CentOS. But due to the complexity of building environments, we still cannot guarantee that we have covered all kinds of situations. If any problem encountered, please fire an issue or open a pull request to let us know. Requirements \u00b6 The following are the configuration requirements for compiling Nebula Graph . For the configuration requirements of the operating environment, see here . CPU: x86_64 Memory: 4GB at least Disk space: 10GB at least Linux: 3.9 or higher, check with uname -r glibc: 2.12 or higher, check with ldd --version GCC: 7.1.0 or higher, check with g++ --version CMake: 3.5.0 or higher, check with cmake --version Access to the Internet For the time being, Nebula Graph can only run in a x86_64 box, with Linux 2.3.32+ and glibc 2.12+. Quick Steps to Build \u00b6 Installing Dependencies \u00b6 Please note that it requires root privileges to install packages. For CentOS, RedHat and Fedora users: $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core # For CentOS 8+, RedHat 8+, and Fedora, you need to install libstdc++-static, libasan $ yum install -y libstdc++-static libasan For Debian and Ubuntu users: $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext For Arch and Gentoo users, you can definitely handle all of these on your own, right? To make sure your GCC and CMake are in the right version: $ g++ --version $ cmake --version If not, please refer to Install an Applicable CMake and Install an Applicable GCC . Cloning the Repo \u00b6 $ git clone https://github.com/vesoft-inc/nebula.git If you don't care about the commit history of the repo, and to make the cloning faster, you could perform a shallow clone: $ git clone --depth = 1 https://github.com/vesoft-inc/nebula.git Configuring and Building \u00b6 $ cd nebula $ mkdir build $ cd build $ cmake -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = $PWD /install .. # Assuming cores is the number of cores and mem_gb is the memory size (in GB), the value of N is recommended to select the smaller one from cores and mem_gb / 2 # We suggest choosing release build type to speed up compilation $ make -jN $ make install $ ls install/ etc/ bin/ share/ scripts/ Since C++ templates are heavily used by Nebula Graph and its third party dependencies, especially Folly, fbthrift and boost, the building is very time-consuming. For your reference, it is expected to take about 35 minutes in CPU time(less than 4 minutes with -j16 ), given an Intel E5-2697 v3 processor and unit tests are disabled. Ways to Tweak the Building \u00b6 Until now, you might already have built Nebula Graph successfully. If so or not, we also provide ways to tweak the building process. CMake Arguments/Variables \u00b6 We provide several options to make one tweak the building, while some of these are builtins from CMake. These arguments are used at the configure(cmake) stage, like cmake -DArgument=Value .. . ENABLE_WERROR \u00b6 By default, Nebula Graph turns on the -Werror compiler option to regard any warnings as errors. If the building fails with such errors, you could still continue the building by set ENABLE_WERROR to OFF . ENABLE_TESTING \u00b6 This option allows to enable or disable the build of unit tests. We suggest to turn it off if you just need the service modules of Nebula Graph . Options are ON and OFF , and the default value is ON . ENABLE_ASAN \u00b6 This option enables or disables the ASan building, a.k.a AddressSanitizer, which is a memory error detector. It is meant to be used by Nebula Graph developers. This option is OFF by default. CMAKE_BUILD_TYPE \u00b6 There are a few building types supported: Debug , to build with debug info but without optimization, which is by default Release , to build with optimization but without debug info RelWithDebInfo , to build with optimization AND debug info MinSizeRel , to build with optimizations for code size CMAKE_INSTALL_PREFIX \u00b6 This option is to specify the location where the service modules, scripts, configuration files and tools are installed when make install is performed. It is set to /usr/local/nebula by default. CMAKE_CXX_COMPILER \u00b6 Normally, CMake will figure out and locate an applicable C++ compiler for us. But if your compiler installation is not at the standard location, or if you want to use a different one, you have to specify it explicitly as follows: $ cmake -DCMAKE_C_COMPILER = /path/to/gcc/bin/gcc -DCMAKE_CXX_COMPILER = /path/to/gcc/bin/g++ .. $ cmake -DCMAKE_C_COMPILER = /path/to/clang/bin/clang -DCMAKE_CXX_COMPILER = /path/to/clang/bin/clang++ .. ENABLE_CCACHE \u00b6 This option is to enable the use of ccache , which is for speeding up the compilation during daily development. By default, Nebula Graph will take advantage of ccache if it's found. So you don't have to enable it for yourself. If you want to disable ccache , it might be not enough to just turn ENABLE_CCACHE off. Since on some platforms, the ccache installation hooks up or precedes the compiler. For such a case, you have to set an environment variable export CCACHE_DISABLE=true , or add a line disable=true to ~/.ccache/ccache.conf . We may do this for you automatically in future. Please see the official documentation for more details. NEBULA_USE_LINKER \u00b6 This option allows users to use an alternative linker, e.g. gold . Options are bfd , lld and gold for now. Among them, bfd and gold belong to GNU binutils, while lld needs to install LLVM / Clang. In addition, you can use this parameter to specify the absolute path of the linker when needed. NEBULA_THIRDPARTY_ROOT \u00b6 This option is to explicitly specify the location of the third party. Installing Third Party Manually \u00b6 By default, at the configure(cmake) stage, a prebuilt third party will be downloaded and installed to the current build directory. If you would like to install it into another location for some reason, e.g. to rebuild by removing the whole build directory without downloading the third party again, you could perform the installation manually. Assume you are now at the build directory, run: # To install third party to /opt requires root privilege, you could change it to another location with --prefix. $ ../third-party/install-third-party.sh --prefix = /opt/vesoft/third-party If the third party is installed to /opt/vesoft/third-party , which is by default if no --prefix given, the building system of Nebula Graph would find it automatically. Otherwise, you need to specify the location with the CMake argument NEBULA_THIRDPARTY_ROOT as mentioned above, or set an environment variable to the location and export it. The precedence for Nebula Graph to find and choose the third party is: The CMake argument NEBULA_THIRDPARTY_ROOT third-party/install in the current build directory The NEBULA_THIRDPARTY_ROOT environment variable /opt/vesoft/third-party Install an Applicable CMake \u00b6 For users who don't have a usable CMake installation, we provide a script to automatically download and install one for you. Assuming you are now at the build directory, run: $ ../third-party/install-cmake.sh cmake-install CMake has been installed to prefix = cmake-install Run 'source cmake-install/bin/enable-cmake.sh' to make it ready to use. Run 'source cmake-install/bin/disable-cmake.sh' to disable it. $ source cmake-install/bin/enable-cmake.sh $ cmake --version cmake version 3 .15.5 Now you have an applicable CMake ready to use. At any time, you could run the command source cmake-install/bin/disable-cmake.sh to disable it. Install an Applicable GCC \u00b6 For users who don't have a usable GCC installation, we provide a prebuilt GCC and a script to automatically download and install it. Assuming you are now at the build directory, run: # To install GCC to /opt requires root privilege, you could change it to other locations $ ../third-party/install-gcc.sh --prefix = /opt GCC-7.5.0 has been installed to /opt/vesoft/toolset/gcc/7.5.0 Performing usability tests Performing regular C++14 tests...OK Performing LeakSanitizer tests...OK Run 'source /opt/vesoft/toolset/gcc/7.5.0/enable' to start using. Run 'source /opt/vesoft/toolset/gcc/7.5.0/disable' to stop using. # Please note that the path and specific version might be different from your environment $ source /opt/vesoft/toolset/gcc/7.5.0/enable # Only PATH was setup so as not to pollute your library path # You could run 'export LD_LIBRARY_PATH=/opt/vesoft/toolset/gcc/7.5.0/lib64:$LD_LIBRARY_PATH' if needed $ g++ --version g++ ( Nebula Graph Build ) 7 .5.0 Copyright ( C ) 2017 Free Software Foundation, Inc. Now you have an applicable GCC compiler ready to use. At any time, you could run the command source /opt/vesoft/toolset/gcc/7.5.0/disable to disable it. FAQ \u00b6 error: invalid argument type 'auto' to unary expression \u00b6 This error happens when building with Clang 9.0, as shown below: [ 5 % ] Building CXX object src/common/fs/CMakeFiles/fs_obj.dir/FileUtils.cpp.o In file included from src/common/fs/FileUtils.cpp:8: In file included from src/common/fs/FileUtils.h:12: src/common/base/StatusOr.h:57:19: error: invalid argument type 'auto' to unary expression static_assert ( !is_status_v<T>, \"`T' must not be of type `Status'\" ) ; ^~~~~~~~~~~~~~~ src/common/fs/FileUtils.cpp:90:34: note: in instantiation of template class 'nebula::StatusOr<std::__cxx11::basic_string<char> >' requested here StatusOr<std::string> FileUtils::readLink ( const char *path ) { ... It is due to a known bug of Clang 9.0 to deal with auto template variables , which has not been fixed until now(2020-01-20).","title":"Build Source Code"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#build_from_source_code","text":"","title":"Build From Source Code"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#overview","text":"We have tested building on various environments, including CentOS 6 to 8, Ubuntu 16.04 to 19.04, Fedora 28 to 30, GCC 7.1.0 to 9.2.0 and recent Clang++ and the devtoolset of Red Hat and CentOS. But due to the complexity of building environments, we still cannot guarantee that we have covered all kinds of situations. If any problem encountered, please fire an issue or open a pull request to let us know.","title":"Overview"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#requirements","text":"The following are the configuration requirements for compiling Nebula Graph . For the configuration requirements of the operating environment, see here . CPU: x86_64 Memory: 4GB at least Disk space: 10GB at least Linux: 3.9 or higher, check with uname -r glibc: 2.12 or higher, check with ldd --version GCC: 7.1.0 or higher, check with g++ --version CMake: 3.5.0 or higher, check with cmake --version Access to the Internet For the time being, Nebula Graph can only run in a x86_64 box, with Linux 2.3.32+ and glibc 2.12+.","title":"Requirements"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#quick_steps_to_build","text":"","title":"Quick Steps to Build"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#installing_dependencies","text":"Please note that it requires root privileges to install packages. For CentOS, RedHat and Fedora users: $ yum update $ yum install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz \\ readline-devel \\ ncurses-devel \\ zlib-devel \\ gcc \\ gcc-c++ \\ cmake \\ gettext \\ curl \\ redhat-lsb-core # For CentOS 8+, RedHat 8+, and Fedora, you need to install libstdc++-static, libasan $ yum install -y libstdc++-static libasan For Debian and Ubuntu users: $ apt-get update $ apt-get install -y make \\ m4 \\ git \\ wget \\ unzip \\ xz-utils \\ curl \\ lsb-core \\ build-essential \\ libreadline-dev \\ ncurses-dev \\ cmake \\ gettext For Arch and Gentoo users, you can definitely handle all of these on your own, right? To make sure your GCC and CMake are in the right version: $ g++ --version $ cmake --version If not, please refer to Install an Applicable CMake and Install an Applicable GCC .","title":"Installing Dependencies"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#cloning_the_repo","text":"$ git clone https://github.com/vesoft-inc/nebula.git If you don't care about the commit history of the repo, and to make the cloning faster, you could perform a shallow clone: $ git clone --depth = 1 https://github.com/vesoft-inc/nebula.git","title":"Cloning the Repo"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#configuring_and_building","text":"$ cd nebula $ mkdir build $ cd build $ cmake -DENABLE_TESTING = OFF -DCMAKE_BUILD_TYPE = Release -DCMAKE_INSTALL_PREFIX = $PWD /install .. # Assuming cores is the number of cores and mem_gb is the memory size (in GB), the value of N is recommended to select the smaller one from cores and mem_gb / 2 # We suggest choosing release build type to speed up compilation $ make -jN $ make install $ ls install/ etc/ bin/ share/ scripts/ Since C++ templates are heavily used by Nebula Graph and its third party dependencies, especially Folly, fbthrift and boost, the building is very time-consuming. For your reference, it is expected to take about 35 minutes in CPU time(less than 4 minutes with -j16 ), given an Intel E5-2697 v3 processor and unit tests are disabled.","title":"Configuring and Building"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#ways_to_tweak_the_building","text":"Until now, you might already have built Nebula Graph successfully. If so or not, we also provide ways to tweak the building process.","title":"Ways to Tweak the Building"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#cmake_argumentsvariables","text":"We provide several options to make one tweak the building, while some of these are builtins from CMake. These arguments are used at the configure(cmake) stage, like cmake -DArgument=Value .. .","title":"CMake Arguments/Variables"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#enable_werror","text":"By default, Nebula Graph turns on the -Werror compiler option to regard any warnings as errors. If the building fails with such errors, you could still continue the building by set ENABLE_WERROR to OFF .","title":"ENABLE_WERROR"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#enable_testing","text":"This option allows to enable or disable the build of unit tests. We suggest to turn it off if you just need the service modules of Nebula Graph . Options are ON and OFF , and the default value is ON .","title":"ENABLE_TESTING"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#enable_asan","text":"This option enables or disables the ASan building, a.k.a AddressSanitizer, which is a memory error detector. It is meant to be used by Nebula Graph developers. This option is OFF by default.","title":"ENABLE_ASAN"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#cmake_build_type","text":"There are a few building types supported: Debug , to build with debug info but without optimization, which is by default Release , to build with optimization but without debug info RelWithDebInfo , to build with optimization AND debug info MinSizeRel , to build with optimizations for code size","title":"CMAKE_BUILD_TYPE"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#cmake_install_prefix","text":"This option is to specify the location where the service modules, scripts, configuration files and tools are installed when make install is performed. It is set to /usr/local/nebula by default.","title":"CMAKE_INSTALL_PREFIX"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#cmake_cxx_compiler","text":"Normally, CMake will figure out and locate an applicable C++ compiler for us. But if your compiler installation is not at the standard location, or if you want to use a different one, you have to specify it explicitly as follows: $ cmake -DCMAKE_C_COMPILER = /path/to/gcc/bin/gcc -DCMAKE_CXX_COMPILER = /path/to/gcc/bin/g++ .. $ cmake -DCMAKE_C_COMPILER = /path/to/clang/bin/clang -DCMAKE_CXX_COMPILER = /path/to/clang/bin/clang++ ..","title":"CMAKE_CXX_COMPILER"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#enable_ccache","text":"This option is to enable the use of ccache , which is for speeding up the compilation during daily development. By default, Nebula Graph will take advantage of ccache if it's found. So you don't have to enable it for yourself. If you want to disable ccache , it might be not enough to just turn ENABLE_CCACHE off. Since on some platforms, the ccache installation hooks up or precedes the compiler. For such a case, you have to set an environment variable export CCACHE_DISABLE=true , or add a line disable=true to ~/.ccache/ccache.conf . We may do this for you automatically in future. Please see the official documentation for more details.","title":"ENABLE_CCACHE"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#nebula_use_linker","text":"This option allows users to use an alternative linker, e.g. gold . Options are bfd , lld and gold for now. Among them, bfd and gold belong to GNU binutils, while lld needs to install LLVM / Clang. In addition, you can use this parameter to specify the absolute path of the linker when needed.","title":"NEBULA_USE_LINKER"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#nebula_thirdparty_root","text":"This option is to explicitly specify the location of the third party.","title":"NEBULA_THIRDPARTY_ROOT"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#installing_third_party_manually","text":"By default, at the configure(cmake) stage, a prebuilt third party will be downloaded and installed to the current build directory. If you would like to install it into another location for some reason, e.g. to rebuild by removing the whole build directory without downloading the third party again, you could perform the installation manually. Assume you are now at the build directory, run: # To install third party to /opt requires root privilege, you could change it to another location with --prefix. $ ../third-party/install-third-party.sh --prefix = /opt/vesoft/third-party If the third party is installed to /opt/vesoft/third-party , which is by default if no --prefix given, the building system of Nebula Graph would find it automatically. Otherwise, you need to specify the location with the CMake argument NEBULA_THIRDPARTY_ROOT as mentioned above, or set an environment variable to the location and export it. The precedence for Nebula Graph to find and choose the third party is: The CMake argument NEBULA_THIRDPARTY_ROOT third-party/install in the current build directory The NEBULA_THIRDPARTY_ROOT environment variable /opt/vesoft/third-party","title":"Installing Third Party Manually"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#install_an_applicable_cmake","text":"For users who don't have a usable CMake installation, we provide a script to automatically download and install one for you. Assuming you are now at the build directory, run: $ ../third-party/install-cmake.sh cmake-install CMake has been installed to prefix = cmake-install Run 'source cmake-install/bin/enable-cmake.sh' to make it ready to use. Run 'source cmake-install/bin/disable-cmake.sh' to disable it. $ source cmake-install/bin/enable-cmake.sh $ cmake --version cmake version 3 .15.5 Now you have an applicable CMake ready to use. At any time, you could run the command source cmake-install/bin/disable-cmake.sh to disable it.","title":"Install an Applicable CMake"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#install_an_applicable_gcc","text":"For users who don't have a usable GCC installation, we provide a prebuilt GCC and a script to automatically download and install it. Assuming you are now at the build directory, run: # To install GCC to /opt requires root privilege, you could change it to other locations $ ../third-party/install-gcc.sh --prefix = /opt GCC-7.5.0 has been installed to /opt/vesoft/toolset/gcc/7.5.0 Performing usability tests Performing regular C++14 tests...OK Performing LeakSanitizer tests...OK Run 'source /opt/vesoft/toolset/gcc/7.5.0/enable' to start using. Run 'source /opt/vesoft/toolset/gcc/7.5.0/disable' to stop using. # Please note that the path and specific version might be different from your environment $ source /opt/vesoft/toolset/gcc/7.5.0/enable # Only PATH was setup so as not to pollute your library path # You could run 'export LD_LIBRARY_PATH=/opt/vesoft/toolset/gcc/7.5.0/lib64:$LD_LIBRARY_PATH' if needed $ g++ --version g++ ( Nebula Graph Build ) 7 .5.0 Copyright ( C ) 2017 Free Software Foundation, Inc. Now you have an applicable GCC compiler ready to use. At any time, you could run the command source /opt/vesoft/toolset/gcc/7.5.0/disable to disable it.","title":"Install an Applicable GCC"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#faq","text":"","title":"FAQ"},{"location":"manual-EN/3.build-develop-and-administration/1.build/1.build-source-code/#error_invalid_argument_type_auto_to_unary_expression","text":"This error happens when building with Clang 9.0, as shown below: [ 5 % ] Building CXX object src/common/fs/CMakeFiles/fs_obj.dir/FileUtils.cpp.o In file included from src/common/fs/FileUtils.cpp:8: In file included from src/common/fs/FileUtils.h:12: src/common/base/StatusOr.h:57:19: error: invalid argument type 'auto' to unary expression static_assert ( !is_status_v<T>, \"`T' must not be of type `Status'\" ) ; ^~~~~~~~~~~~~~~ src/common/fs/FileUtils.cpp:90:34: note: in instantiation of template class 'nebula::StatusOr<std::__cxx11::basic_string<char> >' requested here StatusOr<std::string> FileUtils::readLink ( const char *path ) { ... It is due to a known bug of Clang 9.0 to deal with auto template variables , which has not been fixed until now(2020-01-20).","title":"error: invalid argument type 'auto' to unary expression"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/","text":"Building With Docker Container \u00b6 Nebula Graph has provided a docker image with the whole compiling environment vesoft/nebula-dev , which will make it possible to change source code locally, build and debug within the container. Performing the following steps to start quick development: Pull Image From Docker Hub \u00b6 bash> docker pull vesoft/nebula-dev Run Docker Container \u00b6 Run docker container and mount your local source code directory into the container working_dir /home/nebula with the following command. bash> docker run --rm -ti \\ --security-opt seccomp = unconfined \\ -v /path/to/nebula/directory:/home/nebula \\ -w /home/nebula \\ vesoft/nebula-dev \\ bash Replace /path/to/nebula/directory with your local nebula source code directory . Compiling Within the Container \u00b6 docker> mkdir _build && cd _build docker> cmake .. docker> make docker> make install Run Nebula Graph service \u00b6 Once the preceding installation is completed, you can run Nebula Graph service within the container, the default installation directory is /usr/local/nebula/ . docker> cd /usr/local/nebula Rename config files of Nebula Graph service. docker> cp etc/nebula-graphd.conf.default etc/nebula-graphd.conf docker> cp etc/nebula-metad.conf.default etc/nebula-metad.conf docker> cp etc/nebula-storaged.conf.default etc/nebula-storaged.conf Start service. docker> ./scripts/nebula.service start all docker> ./bin/nebula -u user -p password --port 3699 --addr = \"127.0.0.1\" nebula> SHOW HOSTS ;","title":"Build by Docker"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/#building_with_docker_container","text":"Nebula Graph has provided a docker image with the whole compiling environment vesoft/nebula-dev , which will make it possible to change source code locally, build and debug within the container. Performing the following steps to start quick development:","title":"Building With Docker Container"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/#pull_image_from_docker_hub","text":"bash> docker pull vesoft/nebula-dev","title":"Pull Image From Docker Hub"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/#run_docker_container","text":"Run docker container and mount your local source code directory into the container working_dir /home/nebula with the following command. bash> docker run --rm -ti \\ --security-opt seccomp = unconfined \\ -v /path/to/nebula/directory:/home/nebula \\ -w /home/nebula \\ vesoft/nebula-dev \\ bash Replace /path/to/nebula/directory with your local nebula source code directory .","title":"Run Docker Container"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/#compiling_within_the_container","text":"docker> mkdir _build && cd _build docker> cmake .. docker> make docker> make install","title":"Compiling Within the Container"},{"location":"manual-EN/3.build-develop-and-administration/1.build/2.build-by-docker/#run_nebula_graph_service","text":"Once the preceding installation is completed, you can run Nebula Graph service within the container, the default installation directory is /usr/local/nebula/ . docker> cd /usr/local/nebula Rename config files of Nebula Graph service. docker> cp etc/nebula-graphd.conf.default etc/nebula-graphd.conf docker> cp etc/nebula-metad.conf.default etc/nebula-metad.conf docker> cp etc/nebula-storaged.conf.default etc/nebula-storaged.conf Start service. docker> ./scripts/nebula.service start all docker> ./bin/nebula -u user -p password --port 3699 --addr = \"127.0.0.1\" nebula> SHOW HOSTS ;","title":"Run Nebula Graph service"},{"location":"manual-EN/3.build-develop-and-administration/2.install/1.install-with-rpm-deb/","text":"Nebula Graph Installation with rpm/deb Package \u00b6 Overview \u00b6 This guide will walk you through the process of installing Nebula Graph with rpm/deb packages. Prerequisites \u00b6 Before getting started, ensure that you meet the following requirements: Hard disk: 50 GB Memory: 8 GB Installing Nebula Graph \u00b6 To install Nebula Graph with a rpm/deb package, you must complete the following steps: Download packages. Method one: Download via GitHub. Log in to GitHub and click rpm/deb to locate the rpm/deb package Under the Actions tab, click package on the left. All packages available are displayed. Click the latest package on the top of the package list. Click the Artifacts list on the upper right corner to select a package to download. Method two: Download via OSS. Obtaining the release version information. The URL format is as follows: * Centos 6: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.el6-5.x86_64.rpm * Centos 7: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.el7-5.x86_64.rpm * Ubuntu 1604: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.ubuntu1604.amd64.deb * Ubuntu 1804: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.ubuntu1804.amd64.deb The ${release_version} in the link is the release version information. For example, use the follow command to download the 1.0.0-rc4 Centos 7 package. $ wget https://oss-cdn.nebula-graph.io/package/1.0.0-rc4/nebula-1.0.0-rc2.el7-5.x86_64.rpm b. Obtaining the nightly (latest) version. The URL format is as follows: * Centos 6: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.el6-5.x86_64.rpm * Centos 7: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.el7-5.x86_64.rpm * Ubuntu 1604: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.ubuntu1604.amd64.deb * Ubuntu 1804: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.ubuntu1804.amd64.deb The ${date} in the link specifies the date. For example, use the follow command to download the 2020-4-1 Centos 7.5 package. $ wget https://oss-cdn.nebula-graph.io/package/nightly/2020.04.01/nebula-2020.04.01-nightly.el7-5.x86_64.rpm Install Nebula Graph . For a rpm file, install Nebula Graph with the following command: sudo rpm -ivh nebula-2019.12.23-nightly.el6-5.x86_64.rpm For a deb file, install Nebula Graph with the following command: sudo dpkg -i nebula-2019.12.23-nightly.ubuntu1604.amd64.deb Install Nebula Graph to your customized directory with the following command: rpm -ivh --prefix = ${ your_dir } nebula-graph- ${ version } .rpm Package Nebula Graph to one package with the following command: cd nebula/package ./package.sh -v <version> Package Nebula Graph to multiple packages with the following command: cd nebula/package ./package.sh -v <version> -n OFF Note : Replace the above file name with your own file name, otherwise, this command might fail. Nebula Graph is installed in the /usr/local/nebula directory by default.","title":"rpm Installation"},{"location":"manual-EN/3.build-develop-and-administration/2.install/1.install-with-rpm-deb/#nebula_graph_installation_with_rpmdeb_package","text":"","title":"Nebula Graph Installation with rpm/deb Package"},{"location":"manual-EN/3.build-develop-and-administration/2.install/1.install-with-rpm-deb/#overview","text":"This guide will walk you through the process of installing Nebula Graph with rpm/deb packages.","title":"Overview"},{"location":"manual-EN/3.build-develop-and-administration/2.install/1.install-with-rpm-deb/#prerequisites","text":"Before getting started, ensure that you meet the following requirements: Hard disk: 50 GB Memory: 8 GB","title":"Prerequisites"},{"location":"manual-EN/3.build-develop-and-administration/2.install/1.install-with-rpm-deb/#installing_nebula_graph","text":"To install Nebula Graph with a rpm/deb package, you must complete the following steps: Download packages. Method one: Download via GitHub. Log in to GitHub and click rpm/deb to locate the rpm/deb package Under the Actions tab, click package on the left. All packages available are displayed. Click the latest package on the top of the package list. Click the Artifacts list on the upper right corner to select a package to download. Method two: Download via OSS. Obtaining the release version information. The URL format is as follows: * Centos 6: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.el6-5.x86_64.rpm * Centos 7: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.el7-5.x86_64.rpm * Ubuntu 1604: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.ubuntu1604.amd64.deb * Ubuntu 1804: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/${release_version}/nebula-${release_version}.ubuntu1804.amd64.deb The ${release_version} in the link is the release version information. For example, use the follow command to download the 1.0.0-rc4 Centos 7 package. $ wget https://oss-cdn.nebula-graph.io/package/1.0.0-rc4/nebula-1.0.0-rc2.el7-5.x86_64.rpm b. Obtaining the nightly (latest) version. The URL format is as follows: * Centos 6: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.el6-5.x86_64.rpm * Centos 7: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.el7-5.x86_64.rpm * Ubuntu 1604: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.ubuntu1604.amd64.deb * Ubuntu 1804: https://nebula-graph.oss-cn-hangzhou.aliyuncs.com/package/nightly/${date}/nebula-${date}-nightly.ubuntu1804.amd64.deb The ${date} in the link specifies the date. For example, use the follow command to download the 2020-4-1 Centos 7.5 package. $ wget https://oss-cdn.nebula-graph.io/package/nightly/2020.04.01/nebula-2020.04.01-nightly.el7-5.x86_64.rpm Install Nebula Graph . For a rpm file, install Nebula Graph with the following command: sudo rpm -ivh nebula-2019.12.23-nightly.el6-5.x86_64.rpm For a deb file, install Nebula Graph with the following command: sudo dpkg -i nebula-2019.12.23-nightly.ubuntu1604.amd64.deb Install Nebula Graph to your customized directory with the following command: rpm -ivh --prefix = ${ your_dir } nebula-graph- ${ version } .rpm Package Nebula Graph to one package with the following command: cd nebula/package ./package.sh -v <version> Package Nebula Graph to multiple packages with the following command: cd nebula/package ./package.sh -v <version> -n OFF Note : Replace the above file name with your own file name, otherwise, this command might fail. Nebula Graph is installed in the /usr/local/nebula directory by default.","title":"Installing Nebula Graph"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/","text":"Start and Stop Nebula Graph Services \u00b6 Inputting the Following Commands to Start Nebula Graph Services \u00b6 sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done Listing Nebula Graph Services \u00b6 Listing Nebula Graph services with the following command: sudo /usr/local/nebula/scripts/nebula.service status all [ INFO ] nebula-metad: Running as 9576 , Listening on 45500 [ INFO ] nebula-graphd: Running as 9679 , Listening on 3699 [ INFO ] nebula-storaged: Running as 9812 , Listening on 44500 Connecting Nebula Graph Service \u00b6 Connecting Nebula Graph service with the following command: sudo /usr/local/nebula/bin/nebula -u <user> -p <password> [ --addr = <graphd IP> --port = <graphd port> ] Welcome to Nebula Graph ( Version RC4 ) nebula> SHOW HOSTS ; -u is to set the user name, user is the default Nebula Graph user account -p is to set password, password is the default password for account user --addr is the graphd IP address --port is the the graphd server port and the default value is 3699 Checking the successfully connected services with command SHOW HOSTS Stop Nebula Graph Services \u00b6 Stop Nebula Graph services with the following command: sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done Start/Stop Single Nebula Graph Module \u00b6 Start/stop single module with script nebula.service. sudo /usr/local/nebula/scripts/nebula.service Usage: ./nebula.service [ -v ] [ -c /path/to/config ] <start | stop | restart | status | kill> <metad | graphd | storaged | all> -v Detailed debugging information of this script -c Configuration file path, the default is the etc/ directory under the installation path (/usr/local/nebula/).","title":"Start and Stop Services"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#start_and_stop_nebula_graph_services","text":"","title":"Start and Stop Nebula Graph Services"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#inputting_the_following_commands_to_start_nebula_graph_services","text":"sudo /usr/local/nebula/scripts/nebula.service start all [ INFO ] Starting nebula-metad... [ INFO ] Done [ INFO ] Starting nebula-graphd... [ INFO ] Done [ INFO ] Starting nebula-storaged... [ INFO ] Done","title":"Inputting the Following Commands to Start Nebula Graph Services"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#listing_nebula_graph_services","text":"Listing Nebula Graph services with the following command: sudo /usr/local/nebula/scripts/nebula.service status all [ INFO ] nebula-metad: Running as 9576 , Listening on 45500 [ INFO ] nebula-graphd: Running as 9679 , Listening on 3699 [ INFO ] nebula-storaged: Running as 9812 , Listening on 44500","title":"Listing Nebula Graph Services"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#connecting_nebula_graph_service","text":"Connecting Nebula Graph service with the following command: sudo /usr/local/nebula/bin/nebula -u <user> -p <password> [ --addr = <graphd IP> --port = <graphd port> ] Welcome to Nebula Graph ( Version RC4 ) nebula> SHOW HOSTS ; -u is to set the user name, user is the default Nebula Graph user account -p is to set password, password is the default password for account user --addr is the graphd IP address --port is the the graphd server port and the default value is 3699 Checking the successfully connected services with command SHOW HOSTS","title":"Connecting Nebula Graph Service"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#stop_nebula_graph_services","text":"Stop Nebula Graph services with the following command: sudo /usr/local/nebula/scripts/nebula.service stop all [ INFO ] Stopping nebula-metad... [ INFO ] Done [ INFO ] Stopping nebula-graphd... [ INFO ] Done [ INFO ] Stopping nebula-storaged... [ INFO ] Done","title":"Stop Nebula Graph Services"},{"location":"manual-EN/3.build-develop-and-administration/2.install/2.start-stop-service/#startstop_single_nebula_graph_module","text":"Start/stop single module with script nebula.service. sudo /usr/local/nebula/scripts/nebula.service Usage: ./nebula.service [ -v ] [ -c /path/to/config ] <start | stop | restart | status | kill> <metad | graphd | storaged | all> -v Detailed debugging information of this script -c Configuration file path, the default is the etc/ directory under the installation path (/usr/local/nebula/).","title":"Start/Stop Single Nebula Graph Module"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/","text":"Operating Configuration Requirements \u00b6 Production Environment \u00b6 Production Environment Deployment Method \u00b6 3 metadata service process metad At least 3 storage service processes storaged At least 3 query engine service processes graphd None of the above processes need to a single machine. For example, a cluster of 5 machines: A, B, C, D, E can be deployed as follows: A: metad, storaged, graphd B: metad, storaged, graphd C: metad, storaged, graphd D: storaged, graphd E: storaged, graphd Do not deploy the same cluster across two IDCs. Each metad process automatically creates and maintains a copy of the metadata, so usually only 3 metead processes are enough. Meanwhile, the number of storage processes does not affect the copy count of a graph space. Server Configuration Requirements (Standard) \u00b6 Take AWS EC2 c5d.12xlarge as an example: CPU: 48 core Memory: 96 GB Storage: 2 * 900 GB, NVMe SSD Linux kernel: 3.9 or higher, check with the command uname -r glibc: 2.12 or higher, check with the command ldd --version Test Environment \u00b6 1 metadata service process metad At least 1 storage service process storaged At least 1 query engine service process graphd For example, a cluster with 3 machines: A, B, C can be deployed as follows: A: metad, storaged, graphd B: storaged, graphd C: storaged, graphd Server Configuration Requirements (Minimum) \u00b6 Take AWS EC2 c5d.xlarge as an example: CPU: 4 core Memory: 8 GB Storage: 100 GB, SSD Resource Estimation \u00b6 Storage space (the cluster): number of edges * bytes of edge attributes * 2.5 Memory (full cluster): number of edges * 4 bytes + number of RocksDB instances * (write_buffer_size * max_write_buffer_number + rocksdb_block_cache), where each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance Partitions number of a graph space: number of disks in the cluster * (2 to 10), the better performance of the hard disk, the larger the value. About HDD and Gigabit Networks \u00b6 Nebula Graph is designed for NVMe SSD and 10 Gigabit Network. There is no special adaptation for HDD and gigabit networks. The following are some parameters to be tuned: etc/nebula-storage.conf: --raft_rpc_timeout_ms= 5000 to 10000 --rocksdb_batch_size= 4096 to 16384 --heartbeat_interval_secs = 30 to 60 --raft_heartbeat_interval_secs = 30 to 60 Spark Writer: rate: { timeout: 5000 to 10000 } go-importer: batchSize: 10 to 50 concurrency: 1 to 10 channelBufferSize: 100 to 500 The partition value is 2 * cluster HDD number","title":"System Requirement"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#operating_configuration_requirements","text":"","title":"Operating Configuration Requirements"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#production_environment","text":"","title":"Production Environment"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#production_environment_deployment_method","text":"3 metadata service process metad At least 3 storage service processes storaged At least 3 query engine service processes graphd None of the above processes need to a single machine. For example, a cluster of 5 machines: A, B, C, D, E can be deployed as follows: A: metad, storaged, graphd B: metad, storaged, graphd C: metad, storaged, graphd D: storaged, graphd E: storaged, graphd Do not deploy the same cluster across two IDCs. Each metad process automatically creates and maintains a copy of the metadata, so usually only 3 metead processes are enough. Meanwhile, the number of storage processes does not affect the copy count of a graph space.","title":"Production Environment Deployment Method"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#server_configuration_requirements_standard","text":"Take AWS EC2 c5d.12xlarge as an example: CPU: 48 core Memory: 96 GB Storage: 2 * 900 GB, NVMe SSD Linux kernel: 3.9 or higher, check with the command uname -r glibc: 2.12 or higher, check with the command ldd --version","title":"Server Configuration Requirements (Standard)"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#test_environment","text":"1 metadata service process metad At least 1 storage service process storaged At least 1 query engine service process graphd For example, a cluster with 3 machines: A, B, C can be deployed as follows: A: metad, storaged, graphd B: storaged, graphd C: storaged, graphd","title":"Test Environment"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#server_configuration_requirements_minimum","text":"Take AWS EC2 c5d.xlarge as an example: CPU: 4 core Memory: 8 GB Storage: 100 GB, SSD","title":"Server Configuration Requirements (Minimum)"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#resource_estimation","text":"Storage space (the cluster): number of edges * bytes of edge attributes * 2.5 Memory (full cluster): number of edges * 4 bytes + number of RocksDB instances * (write_buffer_size * max_write_buffer_number + rocksdb_block_cache), where each directory in the --data_path item in the etc/nebula-storaged.conf file corresponds to a RocksDB instance Partitions number of a graph space: number of disks in the cluster * (2 to 10), the better performance of the hard disk, the larger the value.","title":"Resource Estimation"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/0.system-requirement/#about_hdd_and_gigabit_networks","text":"Nebula Graph is designed for NVMe SSD and 10 Gigabit Network. There is no special adaptation for HDD and gigabit networks. The following are some parameters to be tuned: etc/nebula-storage.conf: --raft_rpc_timeout_ms= 5000 to 10000 --rocksdb_batch_size= 4096 to 16384 --heartbeat_interval_secs = 30 to 60 --raft_heartbeat_interval_secs = 30 to 60 Spark Writer: rate: { timeout: 5000 to 10000 } go-importer: batchSize: 10 to 50 concurrency: 1 to 10 channelBufferSize: 100 to 500 The partition value is 2 * cluster HDD number","title":"About HDD and Gigabit Networks"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority/","text":"Configuration Persistency and Priority \u00b6 Configuration Persistency (For Production) \u00b6 When starting Nebula Graph services for the first time, Nebula will read the configuration file from the local (the default path /usr/local/nebula/etc/ ). Then all configuration items (including dynamically changed configuration items) will be persisted in the Meta Service. After that, even if restarting Nebula Graph , it will only read the configuration from Meta Service. Getting the Configuration Locally (For Debugging) \u00b6 In some debugging scenarios, you need to get the configuration from local instead of Meta Service. In this case, add --local_config = true at the top of the configuration file. You need to restart the services to make the modifications take effect. Changing Method and Read priority \u00b6 You can also modify Nebula Graph configurations with command lines ( UPDATE CONFIG syntax) in Nebula console or the environment variables. The read priority rules are as follows: For a configuration: Default configuration precedence: meta > UPDATE CONFIG > environment variable > configuration files. If set --local_config to true, the configuration files take precedence.","title":"Config Persistency and Priority"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority/#configuration_persistency_and_priority","text":"","title":"Configuration Persistency and Priority"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority/#configuration_persistency_for_production","text":"When starting Nebula Graph services for the first time, Nebula will read the configuration file from the local (the default path /usr/local/nebula/etc/ ). Then all configuration items (including dynamically changed configuration items) will be persisted in the Meta Service. After that, even if restarting Nebula Graph , it will only read the configuration from Meta Service.","title":"Configuration Persistency (For Production)"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority/#getting_the_configuration_locally_for_debugging","text":"In some debugging scenarios, you need to get the configuration from local instead of Meta Service. In this case, add --local_config = true at the top of the configuration file. You need to restart the services to make the modifications take effect.","title":"Getting the Configuration Locally (For Debugging)"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority/#changing_method_and_read_priority","text":"You can also modify Nebula Graph configurations with command lines ( UPDATE CONFIG syntax) in Nebula console or the environment variables. The read priority rules are as follows: For a configuration: Default configuration precedence: meta > UPDATE CONFIG > environment variable > configuration files. If set --local_config to true, the configuration files take precedence.","title":"Changing Method and Read priority"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/","text":"CONFIG Syntax \u00b6 Introduction to Configuration \u00b6 Nebula Graph gets configuration from meta by default. If you want to get configuration locally, please add the --local_config=true option in the configuration files metad.conf , storaged.conf , graphd.conf (directory is /home/user/nebula/build/install/etc ) respectively. Note: Configuration precedence: meta > console > environment variable > configuration files. If set --local_config to true, the configuration files take precedence. Restart the services after changing the configuration files to take effect. Configuration changes in console take effect in real time. gflag Parameters \u00b6 Nebula Graph uses gflags for run-time configurations. gflags parameters see the following table. Name Type Description max_edge_returned_per_vertex MUTABLE Control the max edges returned by a certain vertex. minloglevel MUTABLE Minimum log level. v MUTABLE Debug log level. heartbeat_interval_secs MUTABLE Heartbeat interval. meta_client_retry_times MUTABLE Meta client retry times. slow_op_threshhold_ms MUTABLE Default threshold for slow operation, set in ms wal_ttl MUTABLE Life time of write-ahead-log in disks. Default value is 14400 secondes rocksdb_db_options NESTED Parameter in json format, and the key and value of them are in string format. rocksdb_column_family_options NESTED Parameter in json format, and the key and value of them are in string format. rocksdb_block_based_table_options NESTED Parameter in json format, and the key and value of them are in string format. For example, you can set as follows in the conf file of storage: rocksdb_db_options = {\"stats_dump_period_sec\":\"200\", \"enable_write_thread_adaptive_yield\":\"false\", \"write_thread_max_yield_usec\":\"600\"} rocksdb_column_family_options = {\"max_write_buffer_number\":\"4\", \"min_write_buffer_number_to_merge\":\"2\", \"max_write_buffer_number_to_maintain\":\"1\"} rocksdb_block_based_table_options = {\"block_restart_interval\":\"2\"} \"max_edge_returned_per_vertex\":\"INT_MAX\" Nebula Graph supports changing some rocksdb parameters in storage service as follows: // rocksdb_column_family_options disable_auto_compactions write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier // rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files For example nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = \\ { disable_auto_compactions = false, level0_file_num_compaction_trigger = 10 } Reservoir Sampling Parameters \u00b6 Set the following parameters in the configuration file storaged-conf : enable_reservoir_sampling = true/false # Enable reservoir sampling with true. max_edge_returned_per_vertex = number # Set the sampling number. For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is effected compared to direct truncation due to the probability calculation. SHOW CONFIGS \u00b6 SHOW CONFIGS [graph|meta|storage] For example nebula> SHOW CONFIGS meta; ============================================================================================================================ | module | name | type | mode | value | ============================================================================================================================ | META | v | INT64 | IMMUTABLE | 4 | ---------------------------------------------------------------------------------------------------------------------------- | META | help | BOOL | IMMUTABLE | False | ---------------------------------------------------------------------------------------------------------------------------- | META | port | INT64 | IMMUTABLE | 45500 | ---------------------------------------------------------------------------------------------------------------------------- Get CONFIGS \u00b6 GET CONFIGS [graph|meta|storage :] var For example nebula> GET CONFIGS storage:local_ip; ======================================================= | module | name | type | mode | value | ======================================================= | STORAGE | local_ip | STRING | IMMUTABLE | 127.0.0.1 | ------------------------------------------------------- nebula> GET CONFIGS heartbeat_interval_secs; ================================================================= | module | name | type | mode | value | ================================================================= | GRAPH | heartbeat_interval_secs | INT64 | MUTABLE | 10 | ----------------------------------------------------------------- | STORAGE | heartbeat_interval_secs | INT64 | MUTABLE | 10 | ----------------------------------------------------------------- Update CONFIGS \u00b6 UPDATE CONFIGS [graph|meta|storage :] var = value The updated CONFIGS will be stored into meta-service permanently. If the configuration's mode is MUTABLE , the change will take effects immediately. Otherwise, if the mode is REBOOT , the change will not work until server restart. Expression is supported in UPDATE CONFIGS. For example nebula> UPDATE CONFIGS storage:heartbeat_interval_secs=1; nebula> GET CONFIGS storage:heartbeat_interval_secs; =============================================================== | module | name | type | mode | value | =============================================================== | STORAGE | heartbeat_interval_secs | INT64 | MUTABLE | 1 | ---------------------------------------------------------------","title":"CONFIG Syntax"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#config_syntax","text":"","title":"CONFIG Syntax"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#introduction_to_configuration","text":"Nebula Graph gets configuration from meta by default. If you want to get configuration locally, please add the --local_config=true option in the configuration files metad.conf , storaged.conf , graphd.conf (directory is /home/user/nebula/build/install/etc ) respectively. Note: Configuration precedence: meta > console > environment variable > configuration files. If set --local_config to true, the configuration files take precedence. Restart the services after changing the configuration files to take effect. Configuration changes in console take effect in real time.","title":"Introduction to Configuration"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#gflag_parameters","text":"Nebula Graph uses gflags for run-time configurations. gflags parameters see the following table. Name Type Description max_edge_returned_per_vertex MUTABLE Control the max edges returned by a certain vertex. minloglevel MUTABLE Minimum log level. v MUTABLE Debug log level. heartbeat_interval_secs MUTABLE Heartbeat interval. meta_client_retry_times MUTABLE Meta client retry times. slow_op_threshhold_ms MUTABLE Default threshold for slow operation, set in ms wal_ttl MUTABLE Life time of write-ahead-log in disks. Default value is 14400 secondes rocksdb_db_options NESTED Parameter in json format, and the key and value of them are in string format. rocksdb_column_family_options NESTED Parameter in json format, and the key and value of them are in string format. rocksdb_block_based_table_options NESTED Parameter in json format, and the key and value of them are in string format. For example, you can set as follows in the conf file of storage: rocksdb_db_options = {\"stats_dump_period_sec\":\"200\", \"enable_write_thread_adaptive_yield\":\"false\", \"write_thread_max_yield_usec\":\"600\"} rocksdb_column_family_options = {\"max_write_buffer_number\":\"4\", \"min_write_buffer_number_to_merge\":\"2\", \"max_write_buffer_number_to_maintain\":\"1\"} rocksdb_block_based_table_options = {\"block_restart_interval\":\"2\"} \"max_edge_returned_per_vertex\":\"INT_MAX\" Nebula Graph supports changing some rocksdb parameters in storage service as follows: // rocksdb_column_family_options disable_auto_compactions write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier // rocksdb_db_options max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files For example nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = \\ { disable_auto_compactions = false, level0_file_num_compaction_trigger = 10 }","title":"gflag Parameters"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#reservoir_sampling_parameters","text":"Set the following parameters in the configuration file storaged-conf : enable_reservoir_sampling = true/false # Enable reservoir sampling with true. max_edge_returned_per_vertex = number # Set the sampling number. For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is effected compared to direct truncation due to the probability calculation.","title":"Reservoir Sampling Parameters"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#show_configs","text":"SHOW CONFIGS [graph|meta|storage] For example nebula> SHOW CONFIGS meta; ============================================================================================================================ | module | name | type | mode | value | ============================================================================================================================ | META | v | INT64 | IMMUTABLE | 4 | ---------------------------------------------------------------------------------------------------------------------------- | META | help | BOOL | IMMUTABLE | False | ---------------------------------------------------------------------------------------------------------------------------- | META | port | INT64 | IMMUTABLE | 45500 | ----------------------------------------------------------------------------------------------------------------------------","title":"SHOW CONFIGS"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#get_configs","text":"GET CONFIGS [graph|meta|storage :] var For example nebula> GET CONFIGS storage:local_ip; ======================================================= | module | name | type | mode | value | ======================================================= | STORAGE | local_ip | STRING | IMMUTABLE | 127.0.0.1 | ------------------------------------------------------- nebula> GET CONFIGS heartbeat_interval_secs; ================================================================= | module | name | type | mode | value | ================================================================= | GRAPH | heartbeat_interval_secs | INT64 | MUTABLE | 10 | ----------------------------------------------------------------- | STORAGE | heartbeat_interval_secs | INT64 | MUTABLE | 10 | -----------------------------------------------------------------","title":"Get CONFIGS"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/2.configs-syntax/#update_configs","text":"UPDATE CONFIGS [graph|meta|storage :] var = value The updated CONFIGS will be stored into meta-service permanently. If the configuration's mode is MUTABLE , the change will take effects immediately. Otherwise, if the mode is REBOOT , the change will not work until server restart. Expression is supported in UPDATE CONFIGS. For example nebula> UPDATE CONFIGS storage:heartbeat_interval_secs=1; nebula> GET CONFIGS storage:heartbeat_interval_secs; =============================================================== | module | name | type | mode | value | =============================================================== | STORAGE | heartbeat_interval_secs | INT64 | MUTABLE | 1 | ---------------------------------------------------------------","title":"Update CONFIGS"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/","text":"Meta Service Configurations \u00b6 This document introduces the metad configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production Basic Configurations \u00b6 Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID. Logging Configurations \u00b6 Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to metad log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. logbufsecs 0 (in seconds) Seconds to buffer the log messages Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Networking Configurations \u00b6 Name Default Value Descriptions meta_server_addrs \"127.0.0.1:45500\" A list of meta server IPs. The format is ip1:port1, ip2:port2, ip3:port3. Configure 3 machines to form a RAFT group in production. port 45500 RPC daemon listening port. reuse_port true Enable Kernel(>3.9) SO_REUSEPORT item ws_http_port 11000 HTTP Protocol daemon port. (For internal use) ws_h2_port 11002 HTTP/2 Protocol daemon port. (For internal use) ws_ip \"127.0.0.1\" web service to bind to Storage Configurations \u00b6 Name Default Value Descriptions data_path data/meta (i.e. /usr/local/nebula/data/meta/) Directory for cluster metadata persistence","title":"Metad Configuration"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/#meta_service_configurations","text":"This document introduces the metad configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production","title":"Meta Service Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/#basic_configurations","text":"Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID.","title":"Basic Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/#logging_configurations","text":"Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to metad log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. logbufsecs 0 (in seconds) Seconds to buffer the log messages Modified with UPDATE CONFIGS syntax. The modification takes effect immediately.","title":"Logging Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/#networking_configurations","text":"Name Default Value Descriptions meta_server_addrs \"127.0.0.1:45500\" A list of meta server IPs. The format is ip1:port1, ip2:port2, ip3:port3. Configure 3 machines to form a RAFT group in production. port 45500 RPC daemon listening port. reuse_port true Enable Kernel(>3.9) SO_REUSEPORT item ws_http_port 11000 HTTP Protocol daemon port. (For internal use) ws_h2_port 11002 HTTP/2 Protocol daemon port. (For internal use) ws_ip \"127.0.0.1\" web service to bind to","title":"Networking Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/3.meta-config/#storage_configurations","text":"Name Default Value Descriptions data_path data/meta (i.e. /usr/local/nebula/data/meta/) Directory for cluster metadata persistence","title":"Storage Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/","text":"Graph Configurations \u00b6 This document introduces the graphd configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for daily debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production Basic Configurations \u00b6 Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID. Logging Configurations \u00b6 Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to graphd log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. logbufsecs 0 (in seconds) Seconds to buffer the log messages Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. redirect_stdout true Whether to redirect stdout and stderr to separate files. stdout_log_file \"stdout.log\" Destination filename of stdout. stderr_log_file \"stderr.log\" Destination filename of stderr. slow_op_threshhold_ms 50 (ms) default threshold for slow operation Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. For example, change the graphd log level to v=1 with the following command. nebula> UPDATE CONFIGS graph:v=1; Networking Configurations \u00b6 Name Default Value Descriptions Dynamic Modification meta_server_addrs \"127.0.0.1:45500\" List of meta server addresses, the format looks like ip1:port1, ip2:port2, ip3:port3. port 3699 RPC daemon's listen port. meta_client_retry_times 3 meta client retry times Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. heartbeat_interval_secs 3 (seconds) Seconds between each heartbeat in graph service. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. client_idle_timeout_secs 0 Seconds before we close the idle connections, 0 for infinite. session_idle_timeout_secs 0 Seconds before we expire the idle sessions, 0 for infinite. num_netio_threads 0 Number of networking threads, 0 for number of physical CPU cores. num_accept_threads 1 Number of threads to accept incoming connections. num_worker_threads 0 Number of threads to execute user queries. reuse_port true Whether to turn on the SO_REUSEPORT option. listen_backlog 1024 Backlog of the listen socket. listen_netdev \"any\" The network device to listen on. ws_http_port 13000 Port to listen on Graph with HTTP protocol is 13000. ws_h2_port 13002 Port to listen on Graph with HTTP/2 protocol is 13002. ws_ip \"127.0.0.1\" IP/Hostname to bind to. Authorization Configurations \u00b6 Name Default Value Default Value enable_authorize false Enable authorize auth_type password password: account password; ldap: LDAP; cloud","title":"Graphd Configuration"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/#graph_configurations","text":"This document introduces the graphd configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for daily debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production","title":"Graph Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/#basic_configurations","text":"Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID.","title":"Basic Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/#logging_configurations","text":"Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to graphd log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. logbufsecs 0 (in seconds) Seconds to buffer the log messages Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. redirect_stdout true Whether to redirect stdout and stderr to separate files. stdout_log_file \"stdout.log\" Destination filename of stdout. stderr_log_file \"stderr.log\" Destination filename of stderr. slow_op_threshhold_ms 50 (ms) default threshold for slow operation Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. For example, change the graphd log level to v=1 with the following command. nebula> UPDATE CONFIGS graph:v=1;","title":"Logging Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/#networking_configurations","text":"Name Default Value Descriptions Dynamic Modification meta_server_addrs \"127.0.0.1:45500\" List of meta server addresses, the format looks like ip1:port1, ip2:port2, ip3:port3. port 3699 RPC daemon's listen port. meta_client_retry_times 3 meta client retry times Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. heartbeat_interval_secs 3 (seconds) Seconds between each heartbeat in graph service. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. client_idle_timeout_secs 0 Seconds before we close the idle connections, 0 for infinite. session_idle_timeout_secs 0 Seconds before we expire the idle sessions, 0 for infinite. num_netio_threads 0 Number of networking threads, 0 for number of physical CPU cores. num_accept_threads 1 Number of threads to accept incoming connections. num_worker_threads 0 Number of threads to execute user queries. reuse_port true Whether to turn on the SO_REUSEPORT option. listen_backlog 1024 Backlog of the listen socket. listen_netdev \"any\" The network device to listen on. ws_http_port 13000 Port to listen on Graph with HTTP protocol is 13000. ws_h2_port 13002 Port to listen on Graph with HTTP/2 protocol is 13002. ws_ip \"127.0.0.1\" IP/Hostname to bind to.","title":"Networking Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/4.graph-config/#authorization_configurations","text":"Name Default Value Default Value enable_authorize false Enable authorize auth_type password password: account password; ldap: LDAP; cloud","title":"Authorization Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/","text":"Storage Configurations \u00b6 This document introduces the storaged configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for daily debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production Basic Configurations \u00b6 Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID. Logging Configurations \u00b6 Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to storaged log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. slow_op_threshhold_ms 50 (ms) default threshhold for slow operation Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. For example, change the storage log level to v=1 with the following command. nebula> UPDATE CONFIGS storage:v=1; Networking Configurations \u00b6 Name Default Value Descriptions Dynamic Modification meta_server_addrs \"127.0.0.1:45500\" List of meta server addresses, the format looks like ip1:port1, ip2:port2, ip3:port3. port 3699 RPC daemon's listen port. reuse_port true Whether to turn on the SO_REUSEPORT option. ws_http_port 12000 HTTP Protocol daemon port. (For internal use)\uff09 ws_h2_port 12002 HTTP/2 Protocol daemon port. (For internal use) ws_ip \"127.0.0.1\" web service to bind to heartbeat_interval_secs 10 (seconds) Seconds between each heartbeat. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. raft_heartbeat_interval_secs 5 (seconds) RAFT seconds between each heartbeat. Modify the configuration file and restart service. raft_rpc_timeout_ms 500 (ms) RPC timeout for raft client. Modify the configuration file and restart service. Consistency Seedings for storage \u00b6 Name Default Value Default Value data_path data/storage (i.e. /usr/local/nebula/data/storage/) Root consistent data path , multi paths should be split by comma. For RocksDB engine, one path one instance. Seperate directories When using multiple hard disks. Each directory corresponds to a RocksDB instance for better concurrency. For example: --data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/ RocksDB Options \u00b6 Name Default Value Descriptions Dynamic Modification rocksdb_batch_size 4096 (B) Batch Write rocksdb_block_cache 1024 (MB) block cache siez. Suggest set to 1/3 of the machine memory rocksdb_disable_wal true Whether to disable the WAL in RocksDB. wal_ttl 14400 (seconds) RAFT wal time Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. rocksdb_db_options {} jJson string of DBOptions, all keys and values are string. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json rocksdb_column_family_options {} Json string of ColumnFamilyOptions, all keys and values are string. Details see below. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json rocksdb_block_based_table_options {} Json string of BlockBasedTableOptions, all keys and values are string. Details see below. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json rocksdb_db_options \u00b6 max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files enable_write_thread_adaptive_yield max_subcompactions -- The compact from level0 to level1 enables multi-thread. The default value is 1. Dynamic modification takes effect after restarting. max_background_jobs -- Use multi-thread for compacting. The default value is 1. Dynamic modification takes effect after restarting. The above parameters can either be dynamically modified by UPDATE CONFIGS syntax, or written in the local configuration file. Please refer to the RocksDB manual for specific functions and whether restarting is needed. rocksdb_column_family_options \u00b6 write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions -- Compact automatically when writing data is stopped, default value is false. Dynamic modification takes effect immediately. The above parameters can either be dynamically modified by UPDATE CONFIGS syntax, or written in the local configuration file. Please refer to the RocksDB manual for specific functions and whether restarting is needed. The Above Parameters can be set via the Command Line as Follows \u00b6 nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = \\ { disable_auto_compactions = false, level0_file_num_compaction_trigger = 10 }; -- The command overwrites rocksdb_column_family_options. Please note whether other sub-items will be overwritten nebula> UPDATE CONFIGS storage:rocksdb_db_options = \\ { max_subcompactions = 10, max_background_jobs = 10}; nebula> UPDATE CONFIGS storage:max_edge_returned_per_vertex = 10; -- The parameter is explained below Configurations Done With the Configuration File \u00b6 rocksdb_db_options = {\"stats_dump_period_sec\":\"200\", \"\":\"false\", \"write_thread_max_yield_usec\":\"600\"} rocksdb_column_family_options = {\"max_write_buffer_number\":\"4\", \"min_write_buffer_number_to_merge\":\"2\", \"max_write_buffer_number_to_maintain\":\"1\"} rocksdb_block_based_table_options = {\"block_restart_interval\":\"2\"} Description on Super-Large Vertices \u00b6 For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is effected compared to direct truncation due to the probability calculation. Truncating Directly \u00b6 Name Default Value Descriptions Dynamic Modification max_edge_returned_per_vertex 2147483647 The max returned edges of each super-large vertex. The excess edges are truncated and not returned. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Reservoir Sampling Truncation \u00b6 Name Default Value Descriptions Dynamic Modification enable_reservoir_sampling false Truncated with equal probability from the total n edges. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately.","title":"Storaged Configuration"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#storage_configurations","text":"This document introduces the storaged configuration file in the /usr/local/nebula/etc/ directory. The *.default file is used for daily debugging and the default configuration file when starting the services *.production file is the file used for recommended production , please remove .production suffix during production","title":"Storage Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#basic_configurations","text":"Name Default Value Default Value daemonize true Run as daemon thread pid_file \"pids/nebula-metad.pid\" File to hold the process ID.","title":"Basic Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#logging_configurations","text":"Name Default Value Descriptions Dynamic Modification log_dir logs (i.e. /usr/local/nebula/logs ) Directory to storaged log. It is recommended to put it on a different hard disk from data_path. minloglevel 0 The corresponding log levels are INFO(DEBUG), WARNING, ERROR and FATAL. Usually specified as 0 in debug, 1 in production. The minloglevel to 4 prints no logs. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. v 0 0-4: when minloglevel is set to 0, you can further set the severity level of the debug log. The larger the value, the more detailed the log. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. slow_op_threshhold_ms 50 (ms) default threshhold for slow operation Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. For example, change the storage log level to v=1 with the following command. nebula> UPDATE CONFIGS storage:v=1;","title":"Logging Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#networking_configurations","text":"Name Default Value Descriptions Dynamic Modification meta_server_addrs \"127.0.0.1:45500\" List of meta server addresses, the format looks like ip1:port1, ip2:port2, ip3:port3. port 3699 RPC daemon's listen port. reuse_port true Whether to turn on the SO_REUSEPORT option. ws_http_port 12000 HTTP Protocol daemon port. (For internal use)\uff09 ws_h2_port 12002 HTTP/2 Protocol daemon port. (For internal use) ws_ip \"127.0.0.1\" web service to bind to heartbeat_interval_secs 10 (seconds) Seconds between each heartbeat. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. raft_heartbeat_interval_secs 5 (seconds) RAFT seconds between each heartbeat. Modify the configuration file and restart service. raft_rpc_timeout_ms 500 (ms) RPC timeout for raft client. Modify the configuration file and restart service.","title":"Networking Configurations"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#consistency_seedings_for_storage","text":"Name Default Value Default Value data_path data/storage (i.e. /usr/local/nebula/data/storage/) Root consistent data path , multi paths should be split by comma. For RocksDB engine, one path one instance. Seperate directories When using multiple hard disks. Each directory corresponds to a RocksDB instance for better concurrency. For example: --data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/","title":"Consistency Seedings for storage"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#rocksdb_options","text":"Name Default Value Descriptions Dynamic Modification rocksdb_batch_size 4096 (B) Batch Write rocksdb_block_cache 1024 (MB) block cache siez. Suggest set to 1/3 of the machine memory rocksdb_disable_wal true Whether to disable the WAL in RocksDB. wal_ttl 14400 (seconds) RAFT wal time Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. rocksdb_db_options {} jJson string of DBOptions, all keys and values are string. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json rocksdb_column_family_options {} Json string of ColumnFamilyOptions, all keys and values are string. Details see below. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json rocksdb_block_based_table_options {} Json string of BlockBasedTableOptions, all keys and values are string. Details see below. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately. Overwrite all json","title":"RocksDB Options"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#rocksdb_db_options","text":"max_total_wal_size delete_obsolete_files_period_micros max_background_jobs stats_dump_period_sec compaction_readahead_size writable_file_max_buffer_size bytes_per_sync wal_bytes_per_sync delayed_write_rate avoid_flush_during_shutdown max_open_files enable_write_thread_adaptive_yield max_subcompactions -- The compact from level0 to level1 enables multi-thread. The default value is 1. Dynamic modification takes effect after restarting. max_background_jobs -- Use multi-thread for compacting. The default value is 1. Dynamic modification takes effect after restarting. The above parameters can either be dynamically modified by UPDATE CONFIGS syntax, or written in the local configuration file. Please refer to the RocksDB manual for specific functions and whether restarting is needed.","title":"rocksdb_db_options"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#rocksdb_column_family_options","text":"write_buffer_size max_write_buffer_number level0_file_num_compaction_trigger level0_slowdown_writes_trigger level0_stop_writes_trigger target_file_size_base target_file_size_multiplier max_bytes_for_level_base max_bytes_for_level_multiplier disable_auto_compactions -- Compact automatically when writing data is stopped, default value is false. Dynamic modification takes effect immediately. The above parameters can either be dynamically modified by UPDATE CONFIGS syntax, or written in the local configuration file. Please refer to the RocksDB manual for specific functions and whether restarting is needed.","title":"rocksdb_column_family_options"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#the_above_parameters_can_be_set_via_the_command_line_as_follows","text":"nebula> UPDATE CONFIGS storage:rocksdb_column_family_options = \\ { disable_auto_compactions = false, level0_file_num_compaction_trigger = 10 }; -- The command overwrites rocksdb_column_family_options. Please note whether other sub-items will be overwritten nebula> UPDATE CONFIGS storage:rocksdb_db_options = \\ { max_subcompactions = 10, max_background_jobs = 10}; nebula> UPDATE CONFIGS storage:max_edge_returned_per_vertex = 10; -- The parameter is explained below","title":"The Above Parameters can be set via the Command Line as Follows"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#configurations_done_with_the_configuration_file","text":"rocksdb_db_options = {\"stats_dump_period_sec\":\"200\", \"\":\"false\", \"write_thread_max_yield_usec\":\"600\"} rocksdb_column_family_options = {\"max_write_buffer_number\":\"4\", \"min_write_buffer_number_to_merge\":\"2\", \"max_write_buffer_number_to_maintain\":\"1\"} rocksdb_block_based_table_options = {\"block_restart_interval\":\"2\"}","title":"Configurations Done With the Configuration File"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#description_on_super-large_vertices","text":"For super vertex with a large number of edges, currently there are two truncation strategies: Truncate directly. Set the enable_reservoir_sampling parameter to false . A certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated by default. Truncate with the reservoir sampling algorithm. Based on the algorithm, a certain number of edges specified in the Max_edge_returned_per_vertex parameter are truncated with equal probability from the total n edges. Equal probability sampling is useful in some business scenarios. However, the performance is effected compared to direct truncation due to the probability calculation.","title":"Description on Super-Large Vertices"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#truncating_directly","text":"Name Default Value Descriptions Dynamic Modification max_edge_returned_per_vertex 2147483647 The max returned edges of each super-large vertex. The excess edges are truncated and not returned. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately.","title":"Truncating Directly"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/5.storage-config/#reservoir_sampling_truncation","text":"Name Default Value Descriptions Dynamic Modification enable_reservoir_sampling false Truncated with equal probability from the total n edges. Modified with UPDATE CONFIGS syntax. The modification takes effect immediately.","title":"Reservoir Sampling Truncation"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/6.console-config/","text":"Console Configurations \u00b6 Name Default Value Description addr \"127.0.0.1\" Graph daemon IP address. port 0 Graph daemon listening port. u \"\" Username used to authenticate. p \"\" Password used to authenticate. enable_history false Whether to force saving the command history. server_conn_timeout_ms 1000 Connection timeout in milliseconds.","title":"Console Configuration"},{"location":"manual-EN/3.build-develop-and-administration/3.configurations/6.console-config/#console_configurations","text":"Name Default Value Description addr \"127.0.0.1\" Graph daemon IP address. port 0 Graph daemon listening port. u \"\" Username used to authenticate. p \"\" Password used to authenticate. enable_history false Whether to force saving the command history. server_conn_timeout_ms 1000 Connection timeout in milliseconds.","title":"Console Configurations"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/alter-user-syntax/","text":"Alter User Syntax \u00b6 ALTER USER <user_name> WITH PASSWORD <password> The ALTER USER statement modifies Nebula Graph user accounts. ALTER USER requires the global CREATE USER privilege. An error occurs if you try to modify a user that does not exist. ALTER does not require password verification.","title":"Alter User Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/alter-user-syntax/#alter_user_syntax","text":"ALTER USER <user_name> WITH PASSWORD <password> The ALTER USER statement modifies Nebula Graph user accounts. ALTER USER requires the global CREATE USER privilege. An error occurs if you try to modify a user that does not exist. ALTER does not require password verification.","title":"Alter User Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/built-in-roles/","text":"Built-in Roles \u00b6 Nebula Graph provides the following roles: God The initial root user (similar to the Root in Linux and Administrator in Windows). All the operation access. A cluster can only have one God. God manages all the spaces in the cluster. The God role is automatically initialized by meta and cannot be granted by users. Admin The administration user. Read/write access to both the schema and data limited to its authorized space. Authorization access to users limited to its authorized space. DBA Read/write access to both the schema and data limited to its authorized space. No authorization access to users. User Read/write access to data limited to its authorized space. Read-only access to the schema limited to its authorized space. Guest Read-only access to both the schema and data limited to its authorized space. If the authorization is enabled, the default user name and password are root and nebula respectively, and the user name is immutable. Set the enable_authorize parameter in the /usr/local/nebula/etc/nebula-graphd.conf file to true to enable the authorization. A user who has no assigned roles will not have any accesses to the space. A user can only have one assigned role in the same space. A user can have different roles in different spaces. The set of executor prescribed by each role are described below. Divided by operation permissions. OPERATION STATEMENTS Read space Use, DescribeSpace Write space CreateSpace, DropSpace, CreateSnapshot, DropSnapshot, Balance, Admin, Config, Ingest, Download Read schema DescribeTag, DescribeEdge, DescribeTagIndex, DescribeEdgeIndex Write schema CreateTag, AlterTag, CreateEdge, AlterEdge, DropTag, DropEdge, CreateTagIndex, CreateEdgeIndex, DropTagIndex, DropEdgeIndex Write user CreateUser, DropUser, AlterUser Write role Grant, Revoke Read data Go, Set, Pipe, Match, Assignment, Lookup, Yield, OrderBy, FetchVertices, Find, FetchEdges, FindPath, Limit, GroupBy, Return Write data BuildTagIndex, BuildEdgeIndex, InsertVertex, UpdateVertex, InsertEdge, UpdateEdge, DeleteVertex, DeleteEdges Special operation Show, ChangePassword Divided by operations. OPERATION GOD ADMIN DBA USER GUEST Read space Y Y Y Y Y Write space Y Read schema Y Y Y Y Y Write schema Y Y Y Write user Y Write role Y Y Read data Y Y Y Y Y Write data Y Y Y Y Special operation Y Y Y Y Y","title":"Built-in Roles"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/built-in-roles/#built-in_roles","text":"Nebula Graph provides the following roles: God The initial root user (similar to the Root in Linux and Administrator in Windows). All the operation access. A cluster can only have one God. God manages all the spaces in the cluster. The God role is automatically initialized by meta and cannot be granted by users. Admin The administration user. Read/write access to both the schema and data limited to its authorized space. Authorization access to users limited to its authorized space. DBA Read/write access to both the schema and data limited to its authorized space. No authorization access to users. User Read/write access to data limited to its authorized space. Read-only access to the schema limited to its authorized space. Guest Read-only access to both the schema and data limited to its authorized space. If the authorization is enabled, the default user name and password are root and nebula respectively, and the user name is immutable. Set the enable_authorize parameter in the /usr/local/nebula/etc/nebula-graphd.conf file to true to enable the authorization. A user who has no assigned roles will not have any accesses to the space. A user can only have one assigned role in the same space. A user can have different roles in different spaces. The set of executor prescribed by each role are described below. Divided by operation permissions. OPERATION STATEMENTS Read space Use, DescribeSpace Write space CreateSpace, DropSpace, CreateSnapshot, DropSnapshot, Balance, Admin, Config, Ingest, Download Read schema DescribeTag, DescribeEdge, DescribeTagIndex, DescribeEdgeIndex Write schema CreateTag, AlterTag, CreateEdge, AlterEdge, DropTag, DropEdge, CreateTagIndex, CreateEdgeIndex, DropTagIndex, DropEdgeIndex Write user CreateUser, DropUser, AlterUser Write role Grant, Revoke Read data Go, Set, Pipe, Match, Assignment, Lookup, Yield, OrderBy, FetchVertices, Find, FetchEdges, FindPath, Limit, GroupBy, Return Write data BuildTagIndex, BuildEdgeIndex, InsertVertex, UpdateVertex, InsertEdge, UpdateEdge, DeleteVertex, DeleteEdges Special operation Show, ChangePassword Divided by operations. OPERATION GOD ADMIN DBA USER GUEST Read space Y Y Y Y Y Write space Y Read schema Y Y Y Y Y Write schema Y Y Y Write user Y Write role Y Y Read data Y Y Y Y Y Write data Y Y Y Y Special operation Y Y Y Y Y","title":"Built-in Roles"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/change-password/","text":"CHANGE PASSWORD Syntax \u00b6 CHANGE PASSWORD <user_name> FROM <old_psw> TO <new-psw> The CHANGE PASSWORD statement changes a password to a Nebula Graph user account. The old password is required in addition to the new one.","title":"Change Password"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/change-password/#change_password_syntax","text":"CHANGE PASSWORD <user_name> FROM <old_psw> TO <new-psw> The CHANGE PASSWORD statement changes a password to a Nebula Graph user account. The old password is required in addition to the new one.","title":"CHANGE PASSWORD Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/create-user-syntax/","text":"Create User Syntax \u00b6 CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD <password>] The CREATE USER statement creates new Nebula Graph accounts. To use CREATE USER , you must have the global CREATE USER privilege. By default, an error occurs if you try to create a user that already exists. If the IF NOT EXISTS clause is given, the statement produces a warning for each named user that already exists, rather than an error.","title":"Create User"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/create-user-syntax/#create_user_syntax","text":"CREATE USER [IF NOT EXISTS] <user_name> [WITH PASSWORD <password>] The CREATE USER statement creates new Nebula Graph accounts. To use CREATE USER , you must have the global CREATE USER privilege. By default, an error occurs if you try to create a user that already exists. If the IF NOT EXISTS clause is given, the statement produces a warning for each named user that already exists, rather than an error.","title":"Create User Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/drop-user-syntax/","text":"Drop User Syntax \u00b6 DROP USER [IF EXISTS] <user_name> Only God and Admin users have the DROP privilege for the sentence. DROP USER does not automatically close any already opened client session.","title":"Drop User"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/drop-user-syntax/#drop_user_syntax","text":"DROP USER [IF EXISTS] <user_name> Only God and Admin users have the DROP privilege for the sentence. DROP USER does not automatically close any already opened client session.","title":"Drop User Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/grant-role-syntax/","text":"Grant Role Syntax \u00b6 GRANT ROLE <role_type> ON <space> TO <user> The GRANT statement assigns role to Nebula Graph user account. To use GRANT , you must have the GRANT privilege. Currently, there are five roles in Nebula Graph : GOD , ADMIN , DBA , USER and GUEST . Normally, first use CREATE USER to create an account then use GRANT to define its privileges (assume you have the CREATE and GRANT privilege). Each role and user account to be granted must exist, or errors will occur. The <space> clause must be specified as an existed graph space or an error will occur.","title":"Grant Role"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/grant-role-syntax/#grant_role_syntax","text":"GRANT ROLE <role_type> ON <space> TO <user> The GRANT statement assigns role to Nebula Graph user account. To use GRANT , you must have the GRANT privilege. Currently, there are five roles in Nebula Graph : GOD , ADMIN , DBA , USER and GUEST . Normally, first use CREATE USER to create an account then use GRANT to define its privileges (assume you have the CREATE and GRANT privilege). Each role and user account to be granted must exist, or errors will occur. The <space> clause must be specified as an existed graph space or an error will occur.","title":"Grant Role Syntax"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/revoke-syntax/","text":"Revoke Syntax \u00b6 REVOKE ROLE <role_type> ON <space> FROM <user> The REVOKE statement removes access privileges from Nebula Graph user accounts. To use REVOKE , you must have the REVOKE privilege. Currently, there are five roles in Nebula Graph : GOD , ADMIN , DBA , USER and GUEST . User accounts and roles are to be revoked must exist, or errors will occur. The <space> clause must be specified as an existed graph space or an error will occur.","title":"Revoke"},{"location":"manual-EN/3.build-develop-and-administration/4.account-management-statements/revoke-syntax/#revoke_syntax","text":"REVOKE ROLE <role_type> ON <space> FROM <user> The REVOKE statement removes access privileges from Nebula Graph user accounts. To use REVOKE , you must have the REVOKE privilege. Currently, there are five roles in Nebula Graph : GOD , ADMIN , DBA , USER and GUEST . User accounts and roles are to be revoked must exist, or errors will occur. The <space> clause must be specified as an existed graph space or an error will occur.","title":"Revoke Syntax"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/","text":"Cluster Snapshot \u00b6 Create Snapshot \u00b6 The CREATE SNAPSHOT command creates a snapshot at the current point in time for the whole cluster. The snapshot name is composed of the timestamp of the meta server. If snapshot creation fails in the current version, you must use the DROP SNAPSHOT to clear the invalid snapshots. The current version does not support creating snapshot for the specified graph spaces, and executing CREATE SNAPSHOT creates a snapshot for all graph spaces in the cluster. For example: nebula> CREATE SNAPSHOT; Execution succeeded (Time spent: 22892/23923 us) Show Snapshots \u00b6 The command SHOW SNAPSHOT looks at the states (VALID or INVALID), names and the IP addresses of all storage servers when the snapshots are created in the cluster. For example: nebula> SHOW SNAPSHOTS; =========================================================== | Name | Status | Hosts | =========================================================== | SNAPSHOT_2019_12_04_10_54_36 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_42 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_44 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- Delete Snapshot \u00b6 The DROP SNAPSHOT command deletes a snapshot with the specified name, the syntax is: DROP SNAPSHOT <snapshot-name> You can get the snapshot names with the command SHOW SNAPSHOTS . DROP SNAPSHOT can delete both valid snapshots and invalid snapshots that failed during creation. For example: nebula> DROP SNAPSHOT SNAPSHOT_2019_12_04_10_54_36; nebula> SHOW SNAPSHOTS; =========================================================== | Name | Status | Hosts | =========================================================== | SNAPSHOT_2019_12_04_10_54_42 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_44 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- Now the deletes snapshot is not in the show snapshots list. Tips \u00b6 When the system structure changes, it is better to create a snapshot immediately. For example, when you add host, drop host, create space, drop space or balance. The current version does not support automatic garbage collection for the failed snapshots in creation. We will develop cluster checker in meta server to check the cluster state via asynchronous threads and automatically collect the garbage files in failure snapshot creation. The current version does not support customized snapshot directory. The snapshots are created in the data_path/nebula directory by default. The current version does not support snapshot restore. Users need to write a shell script based on their actual productions to restore snapshots. The implementation logic is rather simple, you copy the snapshots of the engine servers to the specified folder, set this folder to data_path/ , then start the cluster.","title":"Cluster Snapshot"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/#cluster_snapshot","text":"","title":"Cluster Snapshot"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/#create_snapshot","text":"The CREATE SNAPSHOT command creates a snapshot at the current point in time for the whole cluster. The snapshot name is composed of the timestamp of the meta server. If snapshot creation fails in the current version, you must use the DROP SNAPSHOT to clear the invalid snapshots. The current version does not support creating snapshot for the specified graph spaces, and executing CREATE SNAPSHOT creates a snapshot for all graph spaces in the cluster. For example: nebula> CREATE SNAPSHOT; Execution succeeded (Time spent: 22892/23923 us)","title":"Create Snapshot"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/#show_snapshots","text":"The command SHOW SNAPSHOT looks at the states (VALID or INVALID), names and the IP addresses of all storage servers when the snapshots are created in the cluster. For example: nebula> SHOW SNAPSHOTS; =========================================================== | Name | Status | Hosts | =========================================================== | SNAPSHOT_2019_12_04_10_54_36 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_42 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_44 | VALID | 127.0.0.1:77833 | -----------------------------------------------------------","title":"Show Snapshots"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/#delete_snapshot","text":"The DROP SNAPSHOT command deletes a snapshot with the specified name, the syntax is: DROP SNAPSHOT <snapshot-name> You can get the snapshot names with the command SHOW SNAPSHOTS . DROP SNAPSHOT can delete both valid snapshots and invalid snapshots that failed during creation. For example: nebula> DROP SNAPSHOT SNAPSHOT_2019_12_04_10_54_36; nebula> SHOW SNAPSHOTS; =========================================================== | Name | Status | Hosts | =========================================================== | SNAPSHOT_2019_12_04_10_54_42 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- | SNAPSHOT_2019_12_04_10_54_44 | VALID | 127.0.0.1:77833 | ----------------------------------------------------------- Now the deletes snapshot is not in the show snapshots list.","title":"Delete Snapshot"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/cluster-snapshot/#tips","text":"When the system structure changes, it is better to create a snapshot immediately. For example, when you add host, drop host, create space, drop space or balance. The current version does not support automatic garbage collection for the failed snapshots in creation. We will develop cluster checker in meta server to check the cluster state via asynchronous threads and automatically collect the garbage files in failure snapshot creation. The current version does not support customized snapshot directory. The snapshots are created in the data_path/nebula directory by default. The current version does not support snapshot restore. Users need to write a shell script based on their actual productions to restore snapshots. The implementation logic is rather simple, you copy the snapshots of the engine servers to the specified folder, set this folder to data_path/ , then start the cluster.","title":"Tips"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/","text":"Job Manager \u00b6 The job here refers to the long tasks running at the storage layer. For example, compact and flush . The manager means to manage the jobs. For example, you can run, show, stop and recover jobs. Statements List \u00b6 SUBMIT JOB COMPACT \u00b6 The SUBMIT JOB COMPACT command triggers the long time RocksDB compact operation. The example returns the results as follows: nebula> SUBMIT JOB COMPACT; ============== | New Job Id | ============== | 40 | -------------- See here to modify the default compact thread number. SUBMIT JOB FLUSH \u00b6 The SUBMIT JOB FLUSH command writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; ============== | New Job Id | ============== | 2 | -------------- SHOW JOB \u00b6 List Single Job Information \u00b6 The SHOW JOB <job_id> statement shows a job with certain ID and all its tasks. After a job arrives to Meta, Meta will split the job to tasks, and send them to storage. nebula> SHOW JOB 40 ===================================================================================== | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | ===================================================================================== | 40 | flush nba | finished | 12/17/19 17:21:30 | 12/17/19 17:21:30 | ------------------------------------------------------------------------------------- | 40-0 | 192.168.8.5 | finished | 12/17/19 17:21:30 | 12/17/19 17:21:30 | ------------------------------------------------------------------------------------- The above statement returns one to multiple rows, which is determined by the storaged number where the space is located. What's in the returned results: 40 is the job ID flush nba indicates that a flush operation is performed on space nba finished is the job status, which indicates that the job execution is finished and successful. Other job status are Queue, running, failed and stopped 12/17/19 17:21:30 is the start time, which is initially empty(Queue). The value is set if and only if the job status is running. 12/17/19 17:21:30 is the stop time, which is empty when the job status is Queue or running. The value is set when the job status is finished, failed and stopped 40-0 indicated that the job ID is 40, the task ID is 0 192.168.8.5 shows which machine the job is running on finished is the job status, which indicates that the job execution is finished and successful. Other job status are Queue, running, failed and stopped 12/17/19 17:21:30 is the start time, which can never be empty because the initial status is running 12/17/19 17:21:30 is the end time, which is empty when the job status is running. The value is set when the job status is finished, failed and stopped Note: There are five job status, i.e. QUEUE, RUNNING, FINISHED, FAILED, STOPPED. Status switching is described below: Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/ List All Jobs \u00b6 The SHOW JOBS statement lists all the jobs that are not expired. The default job expiration time is one week. You can change it with meta flag job_expired_secs . nebula> SHOW JOBS ============================================================================= | Job Id | Command | Status | Start Time | Stop Time | ============================================================================= | 22 | flush test2 | failed | 12/06/19 14:46:22 | 12/06/19 14:46:22 | ----------------------------------------------------------------------------- | 23 | compact test2 | stopped | 12/06/19 15:07:09 | 12/06/19 15:07:33 | ----------------------------------------------------------------------------- | 24 | compact test2 | stopped | 12/06/19 15:07:11 | 12/06/19 15:07:20 | ----------------------------------------------------------------------------- | 25 | compact test2 | stopped | 12/06/19 15:07:13 | 12/06/19 15:07:24 | ----------------------------------------------------------------------------- For details on the returned results, please refer to the previous section List Single Job Information . STOP JOB \u00b6 The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22 ========================= | STOP Result | ========================= | stop 1 jobs 2 tasks | ------------------------- RECOVER JOB \u00b6 The RECOVER JOB statement re-executes the failed jobs and returns the number of the recovered jobs. nebula> RECOVER JOB ===================== | Recovered job num | ===================== | 5 job recovered | --------------------- FAQ \u00b6 SUBMIT JOB uses HTTP port. Please check if the HTTP ports among the storages are normal. You can use the following command to debug. curl \"http://{storaged-ip}:12000/admin?space={test}&op=compact\"","title":"Long Time-Consuming Task Management"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#job_manager","text":"The job here refers to the long tasks running at the storage layer. For example, compact and flush . The manager means to manage the jobs. For example, you can run, show, stop and recover jobs.","title":"Job Manager"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#statements_list","text":"","title":"Statements List"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#submit_job_compact","text":"The SUBMIT JOB COMPACT command triggers the long time RocksDB compact operation. The example returns the results as follows: nebula> SUBMIT JOB COMPACT; ============== | New Job Id | ============== | 40 | -------------- See here to modify the default compact thread number.","title":"SUBMIT JOB COMPACT"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#submit_job_flush","text":"The SUBMIT JOB FLUSH command writes the RocksDB memfile in memory to the hard disk. nebula> SUBMIT JOB FLUSH; ============== | New Job Id | ============== | 2 | --------------","title":"SUBMIT JOB FLUSH"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#show_job","text":"","title":"SHOW JOB"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#list_single_job_information","text":"The SHOW JOB <job_id> statement shows a job with certain ID and all its tasks. After a job arrives to Meta, Meta will split the job to tasks, and send them to storage. nebula> SHOW JOB 40 ===================================================================================== | Job Id(TaskId) | Command(Dest) | Status | Start Time | Stop Time | ===================================================================================== | 40 | flush nba | finished | 12/17/19 17:21:30 | 12/17/19 17:21:30 | ------------------------------------------------------------------------------------- | 40-0 | 192.168.8.5 | finished | 12/17/19 17:21:30 | 12/17/19 17:21:30 | ------------------------------------------------------------------------------------- The above statement returns one to multiple rows, which is determined by the storaged number where the space is located. What's in the returned results: 40 is the job ID flush nba indicates that a flush operation is performed on space nba finished is the job status, which indicates that the job execution is finished and successful. Other job status are Queue, running, failed and stopped 12/17/19 17:21:30 is the start time, which is initially empty(Queue). The value is set if and only if the job status is running. 12/17/19 17:21:30 is the stop time, which is empty when the job status is Queue or running. The value is set when the job status is finished, failed and stopped 40-0 indicated that the job ID is 40, the task ID is 0 192.168.8.5 shows which machine the job is running on finished is the job status, which indicates that the job execution is finished and successful. Other job status are Queue, running, failed and stopped 12/17/19 17:21:30 is the start time, which can never be empty because the initial status is running 12/17/19 17:21:30 is the end time, which is empty when the job status is running. The value is set when the job status is finished, failed and stopped Note: There are five job status, i.e. QUEUE, RUNNING, FINISHED, FAILED, STOPPED. Status switching is described below: Queue -- running -- finished -- removed \\ \\ / \\ \\ -- failed -- / \\ \\ / \\ ---------- stopped -/","title":"List Single Job Information"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#list_all_jobs","text":"The SHOW JOBS statement lists all the jobs that are not expired. The default job expiration time is one week. You can change it with meta flag job_expired_secs . nebula> SHOW JOBS ============================================================================= | Job Id | Command | Status | Start Time | Stop Time | ============================================================================= | 22 | flush test2 | failed | 12/06/19 14:46:22 | 12/06/19 14:46:22 | ----------------------------------------------------------------------------- | 23 | compact test2 | stopped | 12/06/19 15:07:09 | 12/06/19 15:07:33 | ----------------------------------------------------------------------------- | 24 | compact test2 | stopped | 12/06/19 15:07:11 | 12/06/19 15:07:20 | ----------------------------------------------------------------------------- | 25 | compact test2 | stopped | 12/06/19 15:07:13 | 12/06/19 15:07:24 | ----------------------------------------------------------------------------- For details on the returned results, please refer to the previous section List Single Job Information .","title":"List All Jobs"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#stop_job","text":"The STOP JOB statement stops jobs that are not finished. nebula> STOP JOB 22 ========================= | STOP Result | ========================= | stop 1 jobs 2 tasks | -------------------------","title":"STOP JOB"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#recover_job","text":"The RECOVER JOB statement re-executes the failed jobs and returns the number of the recovered jobs. nebula> RECOVER JOB ===================== | Recovered job num | ===================== | 5 job recovered | ---------------------","title":"RECOVER JOB"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/job-manager/#faq","text":"SUBMIT JOB uses HTTP port. Please check if the HTTP ports among the storages are normal. You can use the following command to debug. curl \"http://{storaged-ip}:12000/admin?space={test}&op=compact\"","title":"FAQ"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/","text":"Storage Balance Usage \u00b6 Nebula Graph 's services are composed of three parts: graphd, storaged and metad. The balance in this document focuses on the operation of storage. Currently, storage can be scaled horizontally by the command balance . There are two kinds of balance command, one is to move data, which is BALANCE DATA ; the other one only changes the distribution of leader partition to balance load without moving data, which is BALANCE LEADER . Balance data \u00b6 Let's use an example to expand the cluster from 3 instances to 8 to show how to BALANCE DATA. Step 1 Prerequisites \u00b6 Deploy a cluster with three replicas, one graphd, one metad and three storaged. Check cluster status using command SHOW HOSTS . Step 1.1 \u00b6 nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ Explanations on the returned results: IP and Port are the present storage instance, the cluster starts with three storaged instances (192.168.8.210:34600, 192.168.8.210:34700, 192.168.8.210:34500) without any data. Status shows the state of the present instance, currently there are two kind of states, i.e. online/offline. When a machine is out of service, metad will turn it to offline if no heart beat received for certain time threshold. The default threshold is 10 minutes and can be changed in parameter expired_threshold_sec when starting metad service. Leader count shows RAFT leader number of the present process. Leader distribution shows how the present leader is distributed in each graph space. No space is created for now. Partition distribution shows how many partitions are served by each host. Step 1.2 \u00b6 Create a graph space named test with 100 partition and 3 replicas. nebula> CREATE SPACE test(PARTITION_NUM=100, REPLICA_FACTOR=3) Get the new partition distribution by the command SHOW HOSTS . nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | test: 0 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 52 | test: 52 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 48 | test: 48 | test: 100 | ------------------------------------------------------------------------------------------------ Step 2 Add new hosts into the cluster \u00b6 Now, add some new hosts (storaged instances) into the cluster. Again, show the new status using command SHOW HOSTS . You can see there are already eight hosts in serving. But no partition is running on the new hosts. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | test: 0 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 52 | test: 52 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 48 | test: 48 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ Step 3 Data migration \u00b6 Check the current balance plan ID using command BALANCE DATA if the cluster is balanced. Otherwise, a new plan ID will be generated by the command. nebula> BALANCE DATA ============== | ID | ============== | 1570761786 | -------------- Check the progress of balance using command BALANCE DATA $id . nebula> BALANCE DATA 1570761786 =============================================================================== | balanceId, spaceId:partId, src->dst | status | =============================================================================== | [1570761786, 1:1, 192.168.8.210:34600->192.168.8.210:44920] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:1, 192.168.8.210:34700->192.168.8.210:34920] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:1, 192.168.8.210:34500->192.168.8.210:34800] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34600->192.168.8.210:44920] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34700->192.168.8.210:34920] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34500->192.168.8.210:34800] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:3, 192.168.8.210:34600->192.168.8.210:44920] | succeeded | ------------------------------------------------------------------------------- ... | Total:189, Succeeded:170, Failed:0, In Progress:19, Invalid:0 | 89.947090% | Explanations on the returned results: The first column is a specific balance task. Take 1570761786, 1:88, 192.168.8.210:34700->192.168.8.210:35940 for example 1570761786 is the balance ID 1:88 , 1 is the spaceId, 88 is the moved partId 192.168.8.210:34700->192.168.8.210:3594 , moving data from 192.168.8.210:34700 to 192.168.8.210:35940 The second column shows the state (result) of the task, there are four states: Succeeded Failed In progress Invalid The last row is the summary of the tasks. Some partitions are yet to be migrated. Step 4 Migration confirmation \u00b6 In most cases, data migration will take hours or even days. During the migration, Nebula Graph services are not affected. Once migration is done, the progress will show 100%. You can retry BALANCE DATA to fix a failed task. If it can't be fixed after several attempts, please contact us at GitHub . Now, you can check partition distribution using command SHOW HOSTS when balance completed. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 3 | test: 3 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 0 | test: 0 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 35 | test: 35 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 24 | test: 24 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 38 | test: 38 | test: 38 | ------------------------------------------------------------------------------------------------ Now partitions and data are evenly distributed on the machines. Balance stop \u00b6 BALANCE DATA STOP command stops the running balance data plan. If there is no running balance plan, an error is thrown. If there is a running plan, the related plan ID is returned. Since each balance plan includes several balance tasks, BALANCE DATA STOP doesn't stop the started tasks , but rather cancel the subsequent tasks. The started tasks will continue until the executions are completed. Input BALANCE DATA $id after BALANCE DATA STOP to check the status of the stopped balance plan. After all the tasks being executed are completed, rerun the BALANCE DATA command to restart balance. If there are failed tasks in the stopped plan, the plan will continue. Otherwise, if all the tasks are succeed, a new balance plan is created and executed. Batch Scale in \u00b6 Nebula supports specifying hosts that need to go offline to conduct batch scale in. The syntax is BALANCE DATA REMOVE $host_list . For example, statement BALANCE DATA REMOVE 192.168.0.1:50000,192.168.0.2:50000 removes two hosts, i.e. 192.168.0.1:50000\uff0c192.168.0.2:50000, during the balance process. If replica number cannot meet the requirement after removing (for example, the number of remaining hosts is less than the number of replicas or when one of the three replica is offline, one of the remaining two replicas is required to be removed), Nebula Graph will reject the balance request and return an error code. Balance leader \u00b6 Command BALANCE DATA only migrates partitions. But the leader distribution remains unbalanced, which means old hosts are overloaded, while the new ones are not fully used. Redistribute RAFT leader using the command BALANCE LEADER . nebula> BALANCE LEADER Seconds later, check the results using the command SHOW HOSTS . The RAFT leaders are distributed evenly over all the hosts in the cluster. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 12 | test: 12 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 13 | test: 13 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 12 | test: 12 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 13 | test: 13 | test: 38 | ------------------------------------------------------------------------------------------------","title":"Storage Balance"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#storage_balance_usage","text":"Nebula Graph 's services are composed of three parts: graphd, storaged and metad. The balance in this document focuses on the operation of storage. Currently, storage can be scaled horizontally by the command balance . There are two kinds of balance command, one is to move data, which is BALANCE DATA ; the other one only changes the distribution of leader partition to balance load without moving data, which is BALANCE LEADER .","title":"Storage Balance Usage"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#balance_data","text":"Let's use an example to expand the cluster from 3 instances to 8 to show how to BALANCE DATA.","title":"Balance data"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_1_prerequisites","text":"Deploy a cluster with three replicas, one graphd, one metad and three storaged. Check cluster status using command SHOW HOSTS .","title":"Step 1 Prerequisites"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_11","text":"nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ Explanations on the returned results: IP and Port are the present storage instance, the cluster starts with three storaged instances (192.168.8.210:34600, 192.168.8.210:34700, 192.168.8.210:34500) without any data. Status shows the state of the present instance, currently there are two kind of states, i.e. online/offline. When a machine is out of service, metad will turn it to offline if no heart beat received for certain time threshold. The default threshold is 10 minutes and can be changed in parameter expired_threshold_sec when starting metad service. Leader count shows RAFT leader number of the present process. Leader distribution shows how the present leader is distributed in each graph space. No space is created for now. Partition distribution shows how many partitions are served by each host.","title":"Step 1.1"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_12","text":"Create a graph space named test with 100 partition and 3 replicas. nebula> CREATE SPACE test(PARTITION_NUM=100, REPLICA_FACTOR=3) Get the new partition distribution by the command SHOW HOSTS . nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | test: 0 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 52 | test: 52 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 48 | test: 48 | test: 100 | ------------------------------------------------------------------------------------------------","title":"Step 1.2"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_2_add_new_hosts_into_the_cluster","text":"Now, add some new hosts (storaged instances) into the cluster. Again, show the new status using command SHOW HOSTS . You can see there are already eight hosts in serving. But no partition is running on the new hosts. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 0 | test: 0 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 52 | test: 52 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 48 | test: 48 | test: 100 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 0 | No valid partition | No valid partition | ------------------------------------------------------------------------------------------------","title":"Step 2 Add new hosts into the cluster"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_3_data_migration","text":"Check the current balance plan ID using command BALANCE DATA if the cluster is balanced. Otherwise, a new plan ID will be generated by the command. nebula> BALANCE DATA ============== | ID | ============== | 1570761786 | -------------- Check the progress of balance using command BALANCE DATA $id . nebula> BALANCE DATA 1570761786 =============================================================================== | balanceId, spaceId:partId, src->dst | status | =============================================================================== | [1570761786, 1:1, 192.168.8.210:34600->192.168.8.210:44920] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:1, 192.168.8.210:34700->192.168.8.210:34920] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:1, 192.168.8.210:34500->192.168.8.210:34800] | succeeded | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34600->192.168.8.210:44920] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34700->192.168.8.210:34920] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:2, 192.168.8.210:34500->192.168.8.210:34800] | in progress | ------------------------------------------------------------------------------- | [1570761786, 1:3, 192.168.8.210:34600->192.168.8.210:44920] | succeeded | ------------------------------------------------------------------------------- ... | Total:189, Succeeded:170, Failed:0, In Progress:19, Invalid:0 | 89.947090% | Explanations on the returned results: The first column is a specific balance task. Take 1570761786, 1:88, 192.168.8.210:34700->192.168.8.210:35940 for example 1570761786 is the balance ID 1:88 , 1 is the spaceId, 88 is the moved partId 192.168.8.210:34700->192.168.8.210:3594 , moving data from 192.168.8.210:34700 to 192.168.8.210:35940 The second column shows the state (result) of the task, there are four states: Succeeded Failed In progress Invalid The last row is the summary of the tasks. Some partitions are yet to be migrated.","title":"Step 3 Data migration"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#step_4_migration_confirmation","text":"In most cases, data migration will take hours or even days. During the migration, Nebula Graph services are not affected. Once migration is done, the progress will show 100%. You can retry BALANCE DATA to fix a failed task. If it can't be fixed after several attempts, please contact us at GitHub . Now, you can check partition distribution using command SHOW HOSTS when balance completed. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 3 | test: 3 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 0 | test: 0 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 0 | test: 0 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 35 | test: 35 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 24 | test: 24 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 38 | test: 38 | test: 38 | ------------------------------------------------------------------------------------------------ Now partitions and data are evenly distributed on the machines.","title":"Step 4 Migration confirmation"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#balance_stop","text":"BALANCE DATA STOP command stops the running balance data plan. If there is no running balance plan, an error is thrown. If there is a running plan, the related plan ID is returned. Since each balance plan includes several balance tasks, BALANCE DATA STOP doesn't stop the started tasks , but rather cancel the subsequent tasks. The started tasks will continue until the executions are completed. Input BALANCE DATA $id after BALANCE DATA STOP to check the status of the stopped balance plan. After all the tasks being executed are completed, rerun the BALANCE DATA command to restart balance. If there are failed tasks in the stopped plan, the plan will continue. Otherwise, if all the tasks are succeed, a new balance plan is created and executed.","title":"Balance stop"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#batch_scale_in","text":"Nebula supports specifying hosts that need to go offline to conduct batch scale in. The syntax is BALANCE DATA REMOVE $host_list . For example, statement BALANCE DATA REMOVE 192.168.0.1:50000,192.168.0.2:50000 removes two hosts, i.e. 192.168.0.1:50000\uff0c192.168.0.2:50000, during the balance process. If replica number cannot meet the requirement after removing (for example, the number of remaining hosts is less than the number of replicas or when one of the three replica is offline, one of the remaining two replicas is required to be removed), Nebula Graph will reject the balance request and return an error code.","title":"Batch Scale in"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/storage-balance/#balance_leader","text":"Command BALANCE DATA only migrates partitions. But the leader distribution remains unbalanced, which means old hosts are overloaded, while the new ones are not fully used. Redistribute RAFT leader using the command BALANCE LEADER . nebula> BALANCE LEADER Seconds later, check the results using the command SHOW HOSTS . The RAFT leaders are distributed evenly over all the hosts in the cluster. nebula> SHOW HOSTS ================================================================================================ | Ip | Port | Status | Leader count | Leader distribution | Partition distribution | ================================================================================================ | 192.168.8.210 | 34600 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34900 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 35940 | online | 12 | test: 12 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34920 | online | 12 | test: 12 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 44920 | online | 13 | test: 13 | test: 38 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34700 | online | 12 | test: 12 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34500 | online | 13 | test: 13 | test: 37 | ------------------------------------------------------------------------------------------------ | 192.168.8.210 | 34800 | online | 13 | test: 13 | test: 38 | ------------------------------------------------------------------------------------------------","title":"Balance leader"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-export/dump-tool/","text":"Dump Tool \u00b6 Dump Tool is a single-machine off-line data dumping tool that can be used to dump or count data with specified conditions. How to Get \u00b6 The source code of the dump tool is under nebula/src/tools/db_dump directory. You can use command make db_dump to compile it. Before using this tool, you can use the SHOW HOSTS statement in the Nebula Graph CLI to check the distribution of the partitions. Also, you can use the vertex_id % partition_num statement to calculate which partition the vertex's corresponding key is located. Note: The Dump Tool is located in the rpm package and its directory is nebula/bin/ . Since the tool dumps data by opening the RockDB, you need to use it on the machines that have the storage service deployed and make sure the meta_server is started. Please refer to the following section on detailed usage. How to Use \u00b6 The db_dump command displays information about how to use the dump tool. Parameter space is required. Parameters db_path and meta_server both have default values and you can configure them based on your actual situation. You can combine parameters vids , parts , tags and edges arbitrarily to dump the data you want. ./db_dump --space = <space name> required: --space = <space name> # A space name must be given. optional: --db_path = <path to rocksdb> # Path to the RocksDB data directory. If nebula was installed in `/usr/local/nebula`, # the db_path would be /usr/local/nebula/data/storage/nebula/ # Default: ./ --meta_server = <ip:port,...> # A list of meta severs' ip:port separated by comma. # Default: 127.0.0.1:45500 --mode = scan | stat # scan: print to screen when records meet the condition, and also print statistics to screen in final. # stat: print statistics to screen. # Default: scan --vids = <list of vid> # A list of vids separated by comma. This parameter means vertex_id/edge_src_id # Would scan the whole space's records if it is not given. --parts = <list of partition id> # A list of partition ids separated by comma. # Would output all partitions if it is not given. --tags = <list of tag name> # A list of tag name separated by comma. --edges = <list of edge name> # A list of edge name separated by comma. --limit = <N> # A positive number that limits the output. # Would output all if set to 0 or negative. # Default: 1000 Following is an example: // Specify a space to dump data ./db_dump --space = space_name // Specify space, db_path, meta_server ./db_dump --space = space_name --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 // Set mode to stat, only stats are returned, no data is printed ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 // Specify vid to dump the vertex and the edges sourcing from it ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 --vids = 123 ,456 // Specify tag and dump vertices with the tag ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 --tags = tag1,tag2 The returned data format: // vertices, key: part_id, vertex_id, tag_name, value: <prop_list> [ vertex ] key: 1 , 0 , poi value:mid:0,8191765721868409651,8025713627522363385,1993089399535188613,3926276052777355165,5123607763506443893,2990089379644866415,poi_name_0,\u4e0a\u6d77,\u534e\u4e1c,30.2824,120.016,poi_stat_0,poi_fc_0,poi_sc_0,0,poi_star_0, // edges, key: part_id, src_id, edge_name, ranking, dst_id, value: <prop_list> [ edge ] key: 1 , 0 , consume_poi_reverse, 0 , 656384 value:mid:656384,mid:0,7.19312,mid:656384,3897457441682646732,mun:656384,4038264117233984707,dun:656384,empe:656384,mobile:656384,gender:656384,age:656384,rs:656384,fpd:656384,0.75313,1.34433,fpd:656384,0.03567,7.56212, // stats ======================================================= COUNT: 10 # total number of data dumped VERTEX COUNT: 1 # number of vertices dumped EDGE COUNT: 9 # number of edges dumped TAG STATISTICS: # number of tags dumped poi : 1 EDGE STATISTICS: # number of edge types dumped consume_poi_reverse : 9 =======================================================","title":"Dump Tool"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-export/dump-tool/#dump_tool","text":"Dump Tool is a single-machine off-line data dumping tool that can be used to dump or count data with specified conditions.","title":"Dump Tool"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-export/dump-tool/#how_to_get","text":"The source code of the dump tool is under nebula/src/tools/db_dump directory. You can use command make db_dump to compile it. Before using this tool, you can use the SHOW HOSTS statement in the Nebula Graph CLI to check the distribution of the partitions. Also, you can use the vertex_id % partition_num statement to calculate which partition the vertex's corresponding key is located. Note: The Dump Tool is located in the rpm package and its directory is nebula/bin/ . Since the tool dumps data by opening the RockDB, you need to use it on the machines that have the storage service deployed and make sure the meta_server is started. Please refer to the following section on detailed usage.","title":"How to Get"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-export/dump-tool/#how_to_use","text":"The db_dump command displays information about how to use the dump tool. Parameter space is required. Parameters db_path and meta_server both have default values and you can configure them based on your actual situation. You can combine parameters vids , parts , tags and edges arbitrarily to dump the data you want. ./db_dump --space = <space name> required: --space = <space name> # A space name must be given. optional: --db_path = <path to rocksdb> # Path to the RocksDB data directory. If nebula was installed in `/usr/local/nebula`, # the db_path would be /usr/local/nebula/data/storage/nebula/ # Default: ./ --meta_server = <ip:port,...> # A list of meta severs' ip:port separated by comma. # Default: 127.0.0.1:45500 --mode = scan | stat # scan: print to screen when records meet the condition, and also print statistics to screen in final. # stat: print statistics to screen. # Default: scan --vids = <list of vid> # A list of vids separated by comma. This parameter means vertex_id/edge_src_id # Would scan the whole space's records if it is not given. --parts = <list of partition id> # A list of partition ids separated by comma. # Would output all partitions if it is not given. --tags = <list of tag name> # A list of tag name separated by comma. --edges = <list of edge name> # A list of edge name separated by comma. --limit = <N> # A positive number that limits the output. # Would output all if set to 0 or negative. # Default: 1000 Following is an example: // Specify a space to dump data ./db_dump --space = space_name // Specify space, db_path, meta_server ./db_dump --space = space_name --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 // Set mode to stat, only stats are returned, no data is printed ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 // Specify vid to dump the vertex and the edges sourcing from it ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 --vids = 123 ,456 // Specify tag and dump vertices with the tag ./db_dump --space = space_name --mode = stat --db_path = /usr/local/nebula/data/storage/nebula/ --meta_server = 127 .0.0.1:45513 --tags = tag1,tag2 The returned data format: // vertices, key: part_id, vertex_id, tag_name, value: <prop_list> [ vertex ] key: 1 , 0 , poi value:mid:0,8191765721868409651,8025713627522363385,1993089399535188613,3926276052777355165,5123607763506443893,2990089379644866415,poi_name_0,\u4e0a\u6d77,\u534e\u4e1c,30.2824,120.016,poi_stat_0,poi_fc_0,poi_sc_0,0,poi_star_0, // edges, key: part_id, src_id, edge_name, ranking, dst_id, value: <prop_list> [ edge ] key: 1 , 0 , consume_poi_reverse, 0 , 656384 value:mid:656384,mid:0,7.19312,mid:656384,3897457441682646732,mun:656384,4038264117233984707,dun:656384,empe:656384,mobile:656384,gender:656384,age:656384,rs:656384,fpd:656384,0.75313,1.34433,fpd:656384,0.03567,7.56212, // stats ======================================================= COUNT: 10 # total number of data dumped VERTEX COUNT: 1 # number of vertices dumped EDGE COUNT: 9 # number of edges dumped TAG STATISTICS: # number of tags dumped poi : 1 EDGE STATISTICS: # number of edge types dumped consume_poi_reverse : 9 =======================================================","title":"How to Use"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/download-and-ingest-sst-file/","text":"Download and Ingest \u00b6 Nebula Graph uses RocksDB as the default key-value storage engine. Therefore, when you are trying to load a large amount of data, the SST files of RocksDB can be generated offline by running a map-reduce job and distributed directly to the servers. Nebula Graph provides Spark-SSTFile-Generator tool. Spark-SSTFile-Generator generates SST files from the hive table via the mapping files. For details on how to use it, please refer Spark application command line reference . The execution will generate SST files on HDFS . The directory structure is as follows: |---1 (this is partition number) | | ---- vertex-${FIRST_KEY_IN_THIS_FILE}.sst | | ---- edge-${FIRST_KEY_IN_THIS_FILE}.sst |---2 | ---- vertex-${FIRST_KEY_IN_THIS_FILE}.sst | ---- edge-${FIRST_KEY_IN_THIS_FILE}.sst .... Each directory is a partition number. SST file name format is {TYPE}-${FIRST_KEY_IN_THIS_FILE}.sst , where TYPE is data type, FIRST_KEY_IN_THIS_FILE is the start Key of the file. (If you want to write your own tools to generate SST files, you need to ensure that the keys in each SST file are ordered.) Please confirm that all servers have Hadoop installed and HADOOP_HOME set. Run Nebula Graph console and execute the download command: nebula > DOWNLOAD HDFS \"hdfs://${HADOOP_HOST}:${HADOOP_PORT}/${HADOOP_PATH}\" Download the SST files of each server into directory data/download respectively via the DOWNLOAD command and meta in the storage servers. Explanation of the above command: HADOOP_HOST specifies Hadoop NameNode address HADOOP_PORT specifies Hadoop NameNode port number HADOOP_PATH specifies Hadoop data storage directory If error occurs when downloading, delete the corresponding data files in data/download directory and try to download again. If error occurs again, please raise us an issue on GitHub . When data download is done, re-execute the command leads to no actions. When the offline SST data download is done, it can be ingested into the storage service via INGEST command. INGEST command is as follows: nebula > INGEST The command will ingest the SST files in data/download directory. Note: ingest will block RocksDB when the data amount is large, please avoid running the command at requirement peak.","title":"Download and Ingest"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/download-and-ingest-sst-file/#download_and_ingest","text":"Nebula Graph uses RocksDB as the default key-value storage engine. Therefore, when you are trying to load a large amount of data, the SST files of RocksDB can be generated offline by running a map-reduce job and distributed directly to the servers. Nebula Graph provides Spark-SSTFile-Generator tool. Spark-SSTFile-Generator generates SST files from the hive table via the mapping files. For details on how to use it, please refer Spark application command line reference . The execution will generate SST files on HDFS . The directory structure is as follows: |---1 (this is partition number) | | ---- vertex-${FIRST_KEY_IN_THIS_FILE}.sst | | ---- edge-${FIRST_KEY_IN_THIS_FILE}.sst |---2 | ---- vertex-${FIRST_KEY_IN_THIS_FILE}.sst | ---- edge-${FIRST_KEY_IN_THIS_FILE}.sst .... Each directory is a partition number. SST file name format is {TYPE}-${FIRST_KEY_IN_THIS_FILE}.sst , where TYPE is data type, FIRST_KEY_IN_THIS_FILE is the start Key of the file. (If you want to write your own tools to generate SST files, you need to ensure that the keys in each SST file are ordered.) Please confirm that all servers have Hadoop installed and HADOOP_HOME set. Run Nebula Graph console and execute the download command: nebula > DOWNLOAD HDFS \"hdfs://${HADOOP_HOST}:${HADOOP_PORT}/${HADOOP_PATH}\" Download the SST files of each server into directory data/download respectively via the DOWNLOAD command and meta in the storage servers. Explanation of the above command: HADOOP_HOST specifies Hadoop NameNode address HADOOP_PORT specifies Hadoop NameNode port number HADOOP_PATH specifies Hadoop data storage directory If error occurs when downloading, delete the corresponding data files in data/download directory and try to download again. If error occurs again, please raise us an issue on GitHub . When data download is done, re-execute the command leads to no actions. When the offline SST data download is done, it can be ingested into the storage service via INGEST command. INGEST command is as follows: nebula > INGEST The command will ingest the SST files in data/download directory. Note: ingest will block RocksDB when the data amount is large, please avoid running the command at requirement peak.","title":"Download and Ingest"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/import-csv-file/","text":"Import csv File \u00b6 See vesoft-inc/nebula-importer .","title":"Import .csv File"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/import-csv-file/#import_csv_file","text":"See vesoft-inc/nebula-importer .","title":"Import csv File"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/","text":"Spark Writer \u00b6 Overview \u00b6 Spark Writer is Nebula Graph's Spark-based distributed data import tool that converts data from multiple data repositories into vertices and edges of graphs and batch imports data into the graph database. Currently supported data repositories are: HDFS, including Parquet, JSON, ORC and CSV HIVE Spark Writer supports concurrent importing multiple tags and edges, and configuring different data repositories on different tags and edges. Prerequisites \u00b6 Note: To use Nebula Graph Spark Writer , please make sure you have: Spark 2.0 or above Hive 2.3 or above Hadoop 2.0 or above Get Spark Writer \u00b6 From Source Code \u00b6 git clone https://github.com/vesoft-inc/nebula.git cd nebula/src/tools/spark-sstfile-generator mvn compile package Or you can download from OSS. Download From Cloud Storage OSS \u00b6 wget https://oss-cdn.nebula-graph.io/jar-packages/sst.generator-1.0.0-beta.jar User Guide \u00b6 This section includes the following steps: Create a graph space and its schema in Nebula Graph Write data files Write input source mapping file Import data Create Graph Space \u00b6 Please refer to the example graph in Quick Start . Note: Please create a space and define the schema in Nebula Graph first, then use this tool to import data to Nebula Graph. Example Data \u00b6 Vertices \u00b6 A vertex data file consists of multiple rows, with each line in the file representing a point and its properties. In general, the first column is the ID of the vertex. This ID column is specified in the mapping file. Other columns are the properties of the vertex. Consider the following example in JSON format. Player data {\"id\":100,\"name\":\"Tim Duncan\",\"age\":42} {\"id\":101,\"name\":\"Tony Parker\",\"age\":36} {\"id\":102,\"name\":\"LaMarcus Aldridge\",\"age\":33} Edges \u00b6 An edge data file consists of multiple rows, with each line in the file representing a point and its properties. In general, the first column is the ID of the source vertex, the second column is the ID of the dest vertex. These ID columns are specified in the mapping file. Other columns are the properties of the edge. Consider the following example in JSON format. Take edge follow as example: Edge without rank {\"source\":100,\"target\":101,\"likeness\":95} {\"source\":101,\"target\":100,\"likeness\":95} {\"source\":101,\"target\":102,\"likeness\":90} Edge with rank {\"source\":100,\"target\":101,\"likeness\":95,\"ranking\":2} {\"source\":101,\"target\":100,\"likeness\":95,\"ranking\":1} {\"source\":101,\"target\":102,\"likeness\":90,\"ranking\":3} Spatial Data Geo \u00b6 Spark Writer supports importing Geo data. Geo data contains latitude and longitude , and the data type is double. {\"latitude\":30.2822095,\"longitude\":120.0298785,\"target\":0,\"dp_poi_name\":\"0\"} {\"latitude\":30.2813834,\"longitude\":120.0208692,\"target\":1,\"dp_poi_name\":\"1\"} {\"latitude\":30.2807347,\"longitude\":120.0181162,\"target\":2,\"dp_poi_name\":\"2\"} {\"latitude\":30.2812694,\"longitude\":120.0164896,\"target\":3,\"dp_poi_name\":\"3\"} Data Source Files \u00b6 The currently supported data sources by Spark Writer are: HDFS HIVE HDFS Files \u00b6 HDFS supports the following file formats: Parquet JSON CSV ORC Player data in Parquet format: +-------+---+---------+ |age| id| name| +-------+---+---------+ | 42|100| Tim Duncan | | 36|101| Tony Parker| +-------+---+---------+ In JSON: { \"id\" : 100 , \"name\" : \"Tim Duncan\" , \"age\" : 42 } { \"id\" : 101 , \"name\" : \"Tony Parker\" , \"age\" : 36 } In CSV: age,id,name 42,100,Tim Duncan 36,101,Tony Parker Database \u00b6 Spark Writer supports database as the data source, and only HIVE is available now. Player format as follows: col_name data_type comment id int name string age int Write Configuration Files \u00b6 The configuration files consist of Spark related information, Nebula Graph related information, and tags mapping and edges mapping blocks. Spark information is configured with the associated parameters running Spark. Nebula Graph information is configured with information such as user name and password to connect Nebula Graph. Tags mapping and edges mapping correspond to the input source mapping of multiple tag/edges respectively, describing the basic information like each tag/edge's data source. It's possible that different tag/edge come from different data sources. Example of a mapping file for the input source: { # Spark related configurations. # See also: http://spark.apache.org/docs/latest/configuration.html spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } cores { max: 16 } } # Nebula Graph related configurations. nebula: { # Query engine address list. addresses: [\"127.0.0.1:3699\"] # Nebula Graph access user name and password. user: user pswd: password # Nebula Graph space's name. space: test # The thrift connection timeout and retry times. # If no configurations are set, the default values are 3000 and 3 respectively. connection { timeout: 3000 retry: 3 } # The nGQL execution retry times. # If no configuration is set, the default value is 3. execution { retry: 3 } } # Processing tags tags: [ # Loading tag from HDFS and the data type is parquet. # The tag's name is tag_name_0. # field_0, field_1 and field_2 from HDFS's Parquet file are written into tag_name_0 # and the vertex column is vertex_key_field. { name: tag_name_0 type: parquet path: hdfs_path fields: { field_0: nebula_field_0, field_1: nebula_field_1, field_2: nebula_field_2 } vertex: vertex_key_field batch : 16 } # Similar to the above. # Loading from Hive will execute command ${exec} as data set. { name: tag_name_1 type: hive exec: \"select hive_field_0, hive_field_1, hive_field_2 from database.table\" fields: { hive_field_0: nebula_field_0, hive_field_1: nebula_field_1, hive_field_2: nebula_field_2 } vertex: vertex_id_field } ] # Processing edges edges: [ # Loading edge from HDFS and data type is JSON. # The edge's name is edge_name_0. # field_0, field_1 and field_2 from HDFS's JSON file are written into edge_name_0 # The source column is source_field, target column is target_field and ranking column is ranking_field. { name: edge_name_0 type: json path: hdfs_path fields: { field_0: nebula_field_0, field_1: nebula_field_1, field_2: nebula_field_2 } source: source_field target: target_field ranking: ranking_field } # Loading from Hive will execute command ${exec} as data set. # Ranking is optional. { name: edge_name_1 type: hive exec: \"select hive_field_0, hive_field_1, hive_field_2 from database.table\" fields: { hive_field_0: nebula_field_0, hive_field_1: nebula_field_1, hive_field_2: nebula_field_2 } source: source_id_field target: target_id_field } ] } Spark Properties \u00b6 The following table gives some example properties, all of which can be found in Spark Available Properties . Field Default Required Description spark.app.name Spark Writer No The name of your application spark.driver.cores 1 No Number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G No Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. It must be at least 1M, or 0 for unlimited. spark.cores.max (not set) No When running on a standalone deploy cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos. Nebula Graph Configuration \u00b6 Field Default Value Required Description nebula.addresses / yes query engine IP list, separated with comma nebula.user / yes user name, the default value is user nebula.pswd / yes password, the default user password is password nebula.space / yes space to import data, the space name is test in this document nebula.connection.timeout 3000 no Thrift timeout nebula.connection.retry 3 no Thrift retry times nebula.execution.retry 3 no nGQL execution retry times Mapping of Tags and Edges \u00b6 The options for tag and edge mapping are very similar. The following describes the same options first, and then introduces the unique options of tag mapping and edge mapping . Same Options type is a case insensitive required field that specifies data type in the context, and currently supports Parquet, JSON, ORC and CSV path is applied to HDFS data source and specifies the absolute path of HDFS file or directory. It is a required field when the type is HDFS exec is applied to Hive data source. It is a required filed when the query type is HIVE fields is a required filed that maps the columns of the data source to properties of tag / edge unique options for tag mapping vertex is a required field that specifies a column as the vertex ID column unique options for edge mapping source is a required field that specifies a column in the input source as the source vertex ID column target is a required field that specifies a column as the dest vertex ID column ranking is an optional field that specifies a column as the edge ranking column when the inserted edge has a ranking value Data Source Mapping \u00b6 HDFS Parquet Files type specifies the input source type. When it is parquet, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HDFS JSON Files type specifies the type of the input source. When it is JSON, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be absolute directory HIVE ORC Files type specifies the input source type. When it is ORC, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HIVE CSV Files type specifies the input source type. When it is CSV, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HIVE type specifies the input source type. When it is HIVE, it is a case insensitive required field exec is a required field that specifies the HIVE executed query Import Data \u00b6 Input data with the following command: bin/spark-submit \\ --class com.vesoft.nebula.tools.generator.v2.SparkClientGenerator \\ --master ${ MASTER -URL } \\ ${ SPARK_WRITER_JAR_PACKAGE } -c conf/test.conf -h -d Parameter descriptions: Abbreviation Required Default Description Example --class yes / Specify the program's main class --master yes / Specify spark cluster master url. Refer to master urls for detail e.g. spark://23.195.26.187:7077 -c / --config yes / The configuration file path in the context -h / --hive no false Used to specify whether to support Hive -d / --directly no false True for console insertion; false for sst import (TODO) -D / --dry no false Check if the configuration file is correct Performance \u00b6 It takes about four minutes (i.e. 400k QPS) to input 100 million rows (each row contains three fields, each batch contains 64 rows) into three nodes (56 core, 250G memory, 10G network, SSD).","title":"Spark Writer"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#spark_writer","text":"","title":"Spark Writer"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#overview","text":"Spark Writer is Nebula Graph's Spark-based distributed data import tool that converts data from multiple data repositories into vertices and edges of graphs and batch imports data into the graph database. Currently supported data repositories are: HDFS, including Parquet, JSON, ORC and CSV HIVE Spark Writer supports concurrent importing multiple tags and edges, and configuring different data repositories on different tags and edges.","title":"Overview"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#prerequisites","text":"Note: To use Nebula Graph Spark Writer , please make sure you have: Spark 2.0 or above Hive 2.3 or above Hadoop 2.0 or above","title":"Prerequisites"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#get_spark_writer","text":"","title":"Get Spark Writer"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#from_source_code","text":"git clone https://github.com/vesoft-inc/nebula.git cd nebula/src/tools/spark-sstfile-generator mvn compile package Or you can download from OSS.","title":"From Source Code"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#download_from_cloud_storage_oss","text":"wget https://oss-cdn.nebula-graph.io/jar-packages/sst.generator-1.0.0-beta.jar","title":"Download From Cloud Storage OSS"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#user_guide","text":"This section includes the following steps: Create a graph space and its schema in Nebula Graph Write data files Write input source mapping file Import data","title":"User Guide"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#create_graph_space","text":"Please refer to the example graph in Quick Start . Note: Please create a space and define the schema in Nebula Graph first, then use this tool to import data to Nebula Graph.","title":"Create Graph Space"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#example_data","text":"","title":"Example Data"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#vertices","text":"A vertex data file consists of multiple rows, with each line in the file representing a point and its properties. In general, the first column is the ID of the vertex. This ID column is specified in the mapping file. Other columns are the properties of the vertex. Consider the following example in JSON format. Player data {\"id\":100,\"name\":\"Tim Duncan\",\"age\":42} {\"id\":101,\"name\":\"Tony Parker\",\"age\":36} {\"id\":102,\"name\":\"LaMarcus Aldridge\",\"age\":33}","title":"Vertices"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#edges","text":"An edge data file consists of multiple rows, with each line in the file representing a point and its properties. In general, the first column is the ID of the source vertex, the second column is the ID of the dest vertex. These ID columns are specified in the mapping file. Other columns are the properties of the edge. Consider the following example in JSON format. Take edge follow as example: Edge without rank {\"source\":100,\"target\":101,\"likeness\":95} {\"source\":101,\"target\":100,\"likeness\":95} {\"source\":101,\"target\":102,\"likeness\":90} Edge with rank {\"source\":100,\"target\":101,\"likeness\":95,\"ranking\":2} {\"source\":101,\"target\":100,\"likeness\":95,\"ranking\":1} {\"source\":101,\"target\":102,\"likeness\":90,\"ranking\":3}","title":"Edges"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#spatial_data_geo","text":"Spark Writer supports importing Geo data. Geo data contains latitude and longitude , and the data type is double. {\"latitude\":30.2822095,\"longitude\":120.0298785,\"target\":0,\"dp_poi_name\":\"0\"} {\"latitude\":30.2813834,\"longitude\":120.0208692,\"target\":1,\"dp_poi_name\":\"1\"} {\"latitude\":30.2807347,\"longitude\":120.0181162,\"target\":2,\"dp_poi_name\":\"2\"} {\"latitude\":30.2812694,\"longitude\":120.0164896,\"target\":3,\"dp_poi_name\":\"3\"}","title":"Spatial Data Geo"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#data_source_files","text":"The currently supported data sources by Spark Writer are: HDFS HIVE","title":"Data Source Files"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#hdfs_files","text":"HDFS supports the following file formats: Parquet JSON CSV ORC Player data in Parquet format: +-------+---+---------+ |age| id| name| +-------+---+---------+ | 42|100| Tim Duncan | | 36|101| Tony Parker| +-------+---+---------+ In JSON: { \"id\" : 100 , \"name\" : \"Tim Duncan\" , \"age\" : 42 } { \"id\" : 101 , \"name\" : \"Tony Parker\" , \"age\" : 36 } In CSV: age,id,name 42,100,Tim Duncan 36,101,Tony Parker","title":"HDFS Files"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#database","text":"Spark Writer supports database as the data source, and only HIVE is available now. Player format as follows: col_name data_type comment id int name string age int","title":"Database"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#write_configuration_files","text":"The configuration files consist of Spark related information, Nebula Graph related information, and tags mapping and edges mapping blocks. Spark information is configured with the associated parameters running Spark. Nebula Graph information is configured with information such as user name and password to connect Nebula Graph. Tags mapping and edges mapping correspond to the input source mapping of multiple tag/edges respectively, describing the basic information like each tag/edge's data source. It's possible that different tag/edge come from different data sources. Example of a mapping file for the input source: { # Spark related configurations. # See also: http://spark.apache.org/docs/latest/configuration.html spark: { app: { name: Spark Writer } driver: { cores: 1 maxResultSize: 1G } cores { max: 16 } } # Nebula Graph related configurations. nebula: { # Query engine address list. addresses: [\"127.0.0.1:3699\"] # Nebula Graph access user name and password. user: user pswd: password # Nebula Graph space's name. space: test # The thrift connection timeout and retry times. # If no configurations are set, the default values are 3000 and 3 respectively. connection { timeout: 3000 retry: 3 } # The nGQL execution retry times. # If no configuration is set, the default value is 3. execution { retry: 3 } } # Processing tags tags: [ # Loading tag from HDFS and the data type is parquet. # The tag's name is tag_name_0. # field_0, field_1 and field_2 from HDFS's Parquet file are written into tag_name_0 # and the vertex column is vertex_key_field. { name: tag_name_0 type: parquet path: hdfs_path fields: { field_0: nebula_field_0, field_1: nebula_field_1, field_2: nebula_field_2 } vertex: vertex_key_field batch : 16 } # Similar to the above. # Loading from Hive will execute command ${exec} as data set. { name: tag_name_1 type: hive exec: \"select hive_field_0, hive_field_1, hive_field_2 from database.table\" fields: { hive_field_0: nebula_field_0, hive_field_1: nebula_field_1, hive_field_2: nebula_field_2 } vertex: vertex_id_field } ] # Processing edges edges: [ # Loading edge from HDFS and data type is JSON. # The edge's name is edge_name_0. # field_0, field_1 and field_2 from HDFS's JSON file are written into edge_name_0 # The source column is source_field, target column is target_field and ranking column is ranking_field. { name: edge_name_0 type: json path: hdfs_path fields: { field_0: nebula_field_0, field_1: nebula_field_1, field_2: nebula_field_2 } source: source_field target: target_field ranking: ranking_field } # Loading from Hive will execute command ${exec} as data set. # Ranking is optional. { name: edge_name_1 type: hive exec: \"select hive_field_0, hive_field_1, hive_field_2 from database.table\" fields: { hive_field_0: nebula_field_0, hive_field_1: nebula_field_1, hive_field_2: nebula_field_2 } source: source_id_field target: target_id_field } ] }","title":"Write Configuration Files"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#spark_properties","text":"The following table gives some example properties, all of which can be found in Spark Available Properties . Field Default Required Description spark.app.name Spark Writer No The name of your application spark.driver.cores 1 No Number of cores to use for the driver process, only in cluster mode. spark.driver.maxResultSize 1G No Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. It must be at least 1M, or 0 for unlimited. spark.cores.max (not set) No When running on a standalone deploy cluster or a Mesos cluster in \"coarse-grained\" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos.","title":"Spark Properties"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#nebula_graph_configuration","text":"Field Default Value Required Description nebula.addresses / yes query engine IP list, separated with comma nebula.user / yes user name, the default value is user nebula.pswd / yes password, the default user password is password nebula.space / yes space to import data, the space name is test in this document nebula.connection.timeout 3000 no Thrift timeout nebula.connection.retry 3 no Thrift retry times nebula.execution.retry 3 no nGQL execution retry times","title":"Nebula Graph Configuration"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#mapping_of_tags_and_edges","text":"The options for tag and edge mapping are very similar. The following describes the same options first, and then introduces the unique options of tag mapping and edge mapping . Same Options type is a case insensitive required field that specifies data type in the context, and currently supports Parquet, JSON, ORC and CSV path is applied to HDFS data source and specifies the absolute path of HDFS file or directory. It is a required field when the type is HDFS exec is applied to Hive data source. It is a required filed when the query type is HIVE fields is a required filed that maps the columns of the data source to properties of tag / edge unique options for tag mapping vertex is a required field that specifies a column as the vertex ID column unique options for edge mapping source is a required field that specifies a column in the input source as the source vertex ID column target is a required field that specifies a column as the dest vertex ID column ranking is an optional field that specifies a column as the edge ranking column when the inserted edge has a ranking value","title":"Mapping of Tags and Edges"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#data_source_mapping","text":"HDFS Parquet Files type specifies the input source type. When it is parquet, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HDFS JSON Files type specifies the type of the input source. When it is JSON, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be absolute directory HIVE ORC Files type specifies the input source type. When it is ORC, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HIVE CSV Files type specifies the input source type. When it is CSV, it is a case insensitive required field path specifies the HDFS file directory. It is a required field that must be the absolute directory HIVE type specifies the input source type. When it is HIVE, it is a case insensitive required field exec is a required field that specifies the HIVE executed query","title":"Data Source Mapping"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#import_data","text":"Input data with the following command: bin/spark-submit \\ --class com.vesoft.nebula.tools.generator.v2.SparkClientGenerator \\ --master ${ MASTER -URL } \\ ${ SPARK_WRITER_JAR_PACKAGE } -c conf/test.conf -h -d Parameter descriptions: Abbreviation Required Default Description Example --class yes / Specify the program's main class --master yes / Specify spark cluster master url. Refer to master urls for detail e.g. spark://23.195.26.187:7077 -c / --config yes / The configuration file path in the context -h / --hive no false Used to specify whether to support Hive -d / --directly no false True for console insertion; false for sst import (TODO) -D / --dry no false Check if the configuration file is correct","title":"Import Data"},{"location":"manual-EN/3.build-develop-and-administration/5.storage-service-administration/data-import/spark-writer/#performance","text":"It takes about four minutes (i.e. 400k QPS) to input 100 million rows (each row contains three fields, each batch contains 64 rows) into three nodes (56 core, 250G memory, 10G network, SSD).","title":"Performance"},{"location":"manual-EN/3.build-develop-and-administration/6.develop-and-interface/kv-interfaces/","text":"KV interfaces \u00b6 Interface Demo \u00b6 Nebula Graph storage provides key-value interfaces. Users can perform kv operations through the StorageClient. Please note that users still need to create space through console. Currently supported interfaces are Get and Put. The interfaces are as follows. folly :: SemiFuture < StorageRpcResponse < storage :: cpp2 :: ExecResponse >> put ( GraphSpaceID space , std :: vector < nebula :: cpp2 :: Pair > values , folly :: EventBase * evb = nullptr ); folly :: SemiFuture < StorageRpcResponse < storage :: cpp2 :: GeneralResponse >> get ( GraphSpaceID space , const std :: vector < std :: string >& keys , folly :: EventBase * evb = nullptr ); Methods like remove, removeRange and scan will be provided later. Interfaces usage are demonstrated as follows: // Put interface std :: vector < nebula :: cpp2 :: Pair > pairs ; for ( int32_t i = 0 ; i < 1000 ; i ++ ) { auto key = std :: to_string ( folly :: Random :: rand32 ( 1000000000 )); auto value = std :: to_string ( folly :: Random :: rand32 ( 1000000000 )); pairs . emplace_back ( apache :: thrift :: FragileConstructor :: FRAGILE , std :: move ( key ), std :: move ( value )); } // Send requirements through StorageClient, the corresponding parameter is spaceId, the key-value pairs to put auto future = storageClient -> put ( spaceId , std :: move ( pairs )); // Get results auto resp = std :: move ( future ). get (); // Get interface std :: vector < std :: string > keys ; for ( auto & pair : pairs ) { keys . emplace_back ( pair . first ); } // Send requirements through StorageClient, the corresponding parameter is spaceId, the keys to get auto future = storageClient -> get ( spaceId , std :: move ( keys )); // Get results auto resp = std :: move ( future ). get () Processing Returned Results \u00b6 Check the returned results of the rpc to examine if the corresponding operation runs successfully. In addition, since Nebula Graph storage shards data, if one partition fails, the error code is also returned. If any of the partition fails, the entire requirement fails (resp.succeeded() is false). But those succeed are still read/written. Users can retry until all the requirements run successfully. Currently, auto retry is not supported by StorageClient. Users can decide whether to retry based on the error code. // Check if the call is successful if ( ! resp . succeeded ()) { LOG ( ERROR ) << \"Operation Failed\" ; return ; } // Failed partitions and the corresponding error code if ( ! resp . failedParts (). empty ()) { for ( const auto & partEntry : resp . failedParts ()) { LOG ( ERROR ) << \"Operation Failed in \" << partEntry . first << \", Code: \" << static_cast < int32_t > ( partEntry . second ); } return ; } Read Values \u00b6 For the Get interface, we need some more operations to get the corresponding values. Nebula Graph storage is a multi-copy based on Raft, and all read/written operations can only be sent to the leader of the corresponding partition. When a get request contains multiple keys across partitions, the Storage Client requests the keys from the Partition leader. Each rpc return is stored separately in an unordered_map, and the user is currently required to traverse these unordered_maps to check if the key exists. An example is as follows: // Examine whether the value corresponding to the key is in the returned result. If it exists, it is saved in the value. bool found = false ; std :: string value ; // resp.responses() it the results returned by the storage servers for ( const auto & result : resp . responses ()) { // result.values is the returned key-value pairs of a certain storage server auto iter = result . values . find ( key ); if ( iter != result . values . end ()) { value = iter -> second ; found = true ; break ; } }","title":"Key Value API"},{"location":"manual-EN/3.build-develop-and-administration/6.develop-and-interface/kv-interfaces/#kv_interfaces","text":"","title":"KV interfaces"},{"location":"manual-EN/3.build-develop-and-administration/6.develop-and-interface/kv-interfaces/#interface_demo","text":"Nebula Graph storage provides key-value interfaces. Users can perform kv operations through the StorageClient. Please note that users still need to create space through console. Currently supported interfaces are Get and Put. The interfaces are as follows. folly :: SemiFuture < StorageRpcResponse < storage :: cpp2 :: ExecResponse >> put ( GraphSpaceID space , std :: vector < nebula :: cpp2 :: Pair > values , folly :: EventBase * evb = nullptr ); folly :: SemiFuture < StorageRpcResponse < storage :: cpp2 :: GeneralResponse >> get ( GraphSpaceID space , const std :: vector < std :: string >& keys , folly :: EventBase * evb = nullptr ); Methods like remove, removeRange and scan will be provided later. Interfaces usage are demonstrated as follows: // Put interface std :: vector < nebula :: cpp2 :: Pair > pairs ; for ( int32_t i = 0 ; i < 1000 ; i ++ ) { auto key = std :: to_string ( folly :: Random :: rand32 ( 1000000000 )); auto value = std :: to_string ( folly :: Random :: rand32 ( 1000000000 )); pairs . emplace_back ( apache :: thrift :: FragileConstructor :: FRAGILE , std :: move ( key ), std :: move ( value )); } // Send requirements through StorageClient, the corresponding parameter is spaceId, the key-value pairs to put auto future = storageClient -> put ( spaceId , std :: move ( pairs )); // Get results auto resp = std :: move ( future ). get (); // Get interface std :: vector < std :: string > keys ; for ( auto & pair : pairs ) { keys . emplace_back ( pair . first ); } // Send requirements through StorageClient, the corresponding parameter is spaceId, the keys to get auto future = storageClient -> get ( spaceId , std :: move ( keys )); // Get results auto resp = std :: move ( future ). get ()","title":"Interface Demo"},{"location":"manual-EN/3.build-develop-and-administration/6.develop-and-interface/kv-interfaces/#processing_returned_results","text":"Check the returned results of the rpc to examine if the corresponding operation runs successfully. In addition, since Nebula Graph storage shards data, if one partition fails, the error code is also returned. If any of the partition fails, the entire requirement fails (resp.succeeded() is false). But those succeed are still read/written. Users can retry until all the requirements run successfully. Currently, auto retry is not supported by StorageClient. Users can decide whether to retry based on the error code. // Check if the call is successful if ( ! resp . succeeded ()) { LOG ( ERROR ) << \"Operation Failed\" ; return ; } // Failed partitions and the corresponding error code if ( ! resp . failedParts (). empty ()) { for ( const auto & partEntry : resp . failedParts ()) { LOG ( ERROR ) << \"Operation Failed in \" << partEntry . first << \", Code: \" << static_cast < int32_t > ( partEntry . second ); } return ; }","title":"Processing Returned Results"},{"location":"manual-EN/3.build-develop-and-administration/6.develop-and-interface/kv-interfaces/#read_values","text":"For the Get interface, we need some more operations to get the corresponding values. Nebula Graph storage is a multi-copy based on Raft, and all read/written operations can only be sent to the leader of the corresponding partition. When a get request contains multiple keys across partitions, the Storage Client requests the keys from the Partition leader. Each rpc return is stored separately in an unordered_map, and the user is currently required to traverse these unordered_maps to check if the key exists. An example is as follows: // Examine whether the value corresponding to the key is in the returned result. If it exists, it is saved in the value. bool found = false ; std :: string value ; // resp.responses() it the results returned by the storage servers for ( const auto & result : resp . responses ()) { // result.values is the returned key-value pairs of a certain storage server auto iter = result . values . find ( key ); if ( iter != result . values . end ()) { value = iter -> second ; found = true ; break ; } }","title":"Read Values"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/0.connect-prometheus/","text":"Connect to Prometheus \u00b6 Connection Method \u00b6 Prometheus supports fetching metrics with push/pull . Nebula Graph only supports pull . When you pull, Prometheus pulls metrics data periodically from certain endpoints via HTTP requests. Configuring Prometheus \u00b6 This section introduces Prometheus configurations. Prometheus fetches metrics data from the configured endpoints. For details on Prometheus installation and configuration, please refer to Prometheus Official Documentation . In this section, we only modify the endpoints that pull the metrics data. An example configuration file prometheus.yml is as follows. # my global config global : scrape_interval : 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval : 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting : alertmanagers : - static_configs : - targets : # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files : # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs : # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name : 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs : - targets : [ 'localhost:11000' , 'localhost:12000' , 'localhost:13000' ] As shown above, for default configuration Nebula Graph , i.e. the single node, we only need to pull data of endpoints 11000, 12000, and 13000. For non-default configuration or clusters, you need to expose all the HTTP endpoints of all services to Prometheus. Check Metrics via Prometheus \u00b6 After executing the above three steps successfully, all the configurations are completed and Nebula Graph and Prometheus are connected. Now you can access the graphical operation interface provided by Prometheus through a browser, enter http: // localhost: 9090 in the browser, then input add_edges_latency_bucket in the query box of Prometheus, click the execute button to check the corresponding metrics value. Consider the following example:","title":"Connect Prometheus"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/0.connect-prometheus/#connect_to_prometheus","text":"","title":"Connect to Prometheus"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/0.connect-prometheus/#connection_method","text":"Prometheus supports fetching metrics with push/pull . Nebula Graph only supports pull . When you pull, Prometheus pulls metrics data periodically from certain endpoints via HTTP requests.","title":"Connection Method"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/0.connect-prometheus/#configuring_prometheus","text":"This section introduces Prometheus configurations. Prometheus fetches metrics data from the configured endpoints. For details on Prometheus installation and configuration, please refer to Prometheus Official Documentation . In this section, we only modify the endpoints that pull the metrics data. An example configuration file prometheus.yml is as follows. # my global config global : scrape_interval : 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval : 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting : alertmanagers : - static_configs : - targets : # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files : # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs : # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name : 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs : - targets : [ 'localhost:11000' , 'localhost:12000' , 'localhost:13000' ] As shown above, for default configuration Nebula Graph , i.e. the single node, we only need to pull data of endpoints 11000, 12000, and 13000. For non-default configuration or clusters, you need to expose all the HTTP endpoints of all services to Prometheus.","title":"Configuring Prometheus"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/0.connect-prometheus/#check_metrics_via_prometheus","text":"After executing the above three steps successfully, all the configurations are completed and Nebula Graph and Prometheus are connected. Now you can access the graphical operation interface provided by Prometheus through a browser, enter http: // localhost: 9090 in the browser, then input add_edges_latency_bucket in the query box of Prometheus, click the execute button to check the corresponding metrics value. Consider the following example:","title":"Check Metrics via Prometheus"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/1.metrics-exposer/","text":"Metrics Exposer \u00b6 See here .","title":"Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/1.metrics-exposer/#metrics_exposer","text":"See here .","title":"Metrics Exposer"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/","text":"Meta Metrics \u00b6 Introduction \u00b6 Currently, Nebula Graph supports obtaining the basic performance metrics for the meta service via HTTP. Each performance metric consists of three parts, namely <counter_name>.<statistic_type>.<time_range> . Counter Names \u00b6 Each counter name is composed of the interface name and the counter name. Meta service only counts the heartbeat. Currently, the supported interfaces are: meta_heartbeat_qps meta_heartbeat_error_qps meta_heartbeat_latency Statistics Type \u00b6 Currently supported types are SUM, COUNT, AVG, RATE and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles. Time Range \u00b6 Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now. Obtain the Corresponding Metrics via HTTP Interface \u00b6 Here are some examples: meta_heartbeat_qps . avg .60 // the average QPS of the heart beat in the last minute meta_heartbeat_error_qps . count .60 // the total errors occurred of the heart beat in the last minute meta_heartbeat_latency . avg .60 // the average latency of the heart beat in the last minute Assume that a Nebula Graph meta service is started locally, and the ws_http_port port number is set to 11000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60\" # meta_heartbeat_qps.avg.60=580 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60,meta_heartbeat_error_qps.avg.60\" # meta_heartbeat_qps.avg.60=537 # meta_heartbeat_error_qps.avg.60=579 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60,meta_heartbeat_error_qps.avg.60&returnjson\" # [{\"value\":533,\"name\":\"meta_heartbeat_qps.avg.60\"},{\"value\":574,\"name\":\"meta_heartbeat_error_qps.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:11000/get_stats?stats\" # or curl -G \"http://127.0.0.1:11000/get_stats\"","title":"Meta Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#meta_metrics","text":"","title":"Meta Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#introduction","text":"Currently, Nebula Graph supports obtaining the basic performance metrics for the meta service via HTTP. Each performance metric consists of three parts, namely <counter_name>.<statistic_type>.<time_range> .","title":"Introduction"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#counter_names","text":"Each counter name is composed of the interface name and the counter name. Meta service only counts the heartbeat. Currently, the supported interfaces are: meta_heartbeat_qps meta_heartbeat_error_qps meta_heartbeat_latency","title":"Counter Names"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#statistics_type","text":"Currently supported types are SUM, COUNT, AVG, RATE and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles.","title":"Statistics Type"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#time_range","text":"Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now.","title":"Time Range"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/2.meta-metrics/#obtain_the_corresponding_metrics_via_http_interface","text":"Here are some examples: meta_heartbeat_qps . avg .60 // the average QPS of the heart beat in the last minute meta_heartbeat_error_qps . count .60 // the total errors occurred of the heart beat in the last minute meta_heartbeat_latency . avg .60 // the average latency of the heart beat in the last minute Assume that a Nebula Graph meta service is started locally, and the ws_http_port port number is set to 11000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60\" # meta_heartbeat_qps.avg.60=580 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60,meta_heartbeat_error_qps.avg.60\" # meta_heartbeat_qps.avg.60=537 # meta_heartbeat_error_qps.avg.60=579 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:11000/get_stats?stats=meta_heartbeat_qps.avg.60,meta_heartbeat_error_qps.avg.60&returnjson\" # [{\"value\":533,\"name\":\"meta_heartbeat_qps.avg.60\"},{\"value\":574,\"name\":\"meta_heartbeat_error_qps.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:11000/get_stats?stats\" # or curl -G \"http://127.0.0.1:11000/get_stats\"","title":"Obtain the Corresponding Metrics via HTTP Interface"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/","text":"Storage Metrics \u00b6 Introduction \u00b6 Currently, Nebula Graph supports obtaining the basic performance metrics for the storage service via HTTP. Each performance metric consists of three parts, namely <counter_name>.<statistic_type>.<time_range> , details are introduced as the follows. Counter Names \u00b6 Each counter name is composed of the interface name and the counter name. Currently, the supported interfaces are: storage_vertex_props // obtain properties of a vertex storage_edge_props // obtain properties of an edge storage_add_vertex // insert a vertex storage_add_edge // insert an edge storage_del_vertex // delete a vertex storage_update_vertex // update properties of a vertex storage_update_edge // update properties of an edge storage_get_kv // read kv pair storage_put_kv // put kv pair storage_get_bound // internal use only Each interface has three metrics, namely latency (in the units of us), QPS and QPS with errors. The suffixes are as follows: _latency _qps _error_qps The complete metric concatenates the interface name with the corresponding metric, such as storage_add_vertex_latency , storage_add_vertex_qps and storage_add_vertex_error_qps , representing the latency, QPS, and the QPS with errors of inserting a vertex, respectively. Statistics Type \u00b6 Currently supported types are SUM, COUNT, AVG, RATE, and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles. Time Range \u00b6 Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now. Obtain the Corresponding Metrics via HTTP Interface \u00b6 According to the above introduction, you can make a complete metrics name, here are some examples: storage_add_vertex_latency . avg .60 // the average latency of inserting a vertex in the last minute storage_get_bound_qps . rate .600 // obtain neighbor's QPS in the last ten minutes storage_update_edge_error_qps . count .3600 // errors occurred in updating an edge in the last hour Assume that a Nebula Graph storage service is started locally, and the ws_http_port port number is set to 12000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60\" # storage_vertex_props_qps.rate.60=2674 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60,storage_vertex_props_latency.avg.60\" # storage_vertex_props_qps.rate.60=2638 # storage_vertex_props_latency.avg.60=812 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60,storage_vertex_props_latency.avg.60&returnjson\" # [{\"value\":2723,\"name\":\"storage_vertex_props_qps.rate.60\"},{\"value\":804,\"name\":\"storage_vertex_props_latency.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:12000/get_stats?stats\" # or curl -G \"http://127.0.0.1:12000/get_stats\"","title":"Storage Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#storage_metrics","text":"","title":"Storage Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#introduction","text":"Currently, Nebula Graph supports obtaining the basic performance metrics for the storage service via HTTP. Each performance metric consists of three parts, namely <counter_name>.<statistic_type>.<time_range> , details are introduced as the follows.","title":"Introduction"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#counter_names","text":"Each counter name is composed of the interface name and the counter name. Currently, the supported interfaces are: storage_vertex_props // obtain properties of a vertex storage_edge_props // obtain properties of an edge storage_add_vertex // insert a vertex storage_add_edge // insert an edge storage_del_vertex // delete a vertex storage_update_vertex // update properties of a vertex storage_update_edge // update properties of an edge storage_get_kv // read kv pair storage_put_kv // put kv pair storage_get_bound // internal use only Each interface has three metrics, namely latency (in the units of us), QPS and QPS with errors. The suffixes are as follows: _latency _qps _error_qps The complete metric concatenates the interface name with the corresponding metric, such as storage_add_vertex_latency , storage_add_vertex_qps and storage_add_vertex_error_qps , representing the latency, QPS, and the QPS with errors of inserting a vertex, respectively.","title":"Counter Names"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#statistics_type","text":"Currently supported types are SUM, COUNT, AVG, RATE, and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles.","title":"Statistics Type"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#time_range","text":"Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now.","title":"Time Range"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/3.storage-metrics/#obtain_the_corresponding_metrics_via_http_interface","text":"According to the above introduction, you can make a complete metrics name, here are some examples: storage_add_vertex_latency . avg .60 // the average latency of inserting a vertex in the last minute storage_get_bound_qps . rate .600 // obtain neighbor's QPS in the last ten minutes storage_update_edge_error_qps . count .3600 // errors occurred in updating an edge in the last hour Assume that a Nebula Graph storage service is started locally, and the ws_http_port port number is set to 12000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60\" # storage_vertex_props_qps.rate.60=2674 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60,storage_vertex_props_latency.avg.60\" # storage_vertex_props_qps.rate.60=2638 # storage_vertex_props_latency.avg.60=812 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:12000/get_stats?stats=storage_vertex_props_qps.rate.60,storage_vertex_props_latency.avg.60&returnjson\" # [{\"value\":2723,\"name\":\"storage_vertex_props_qps.rate.60\"},{\"value\":804,\"name\":\"storage_vertex_props_latency.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:12000/get_stats?stats\" # or curl -G \"http://127.0.0.1:12000/get_stats\"","title":"Obtain the Corresponding Metrics via HTTP Interface"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/","text":"Graph Metrics \u00b6 Introduction \u00b6 Currently, Nebula Graph supports obtaining the basic performance metric for the graph service via HTTP. Each performance metrics consists of three parts, namely <counter_name>.<statistic_type>.<time_range> . Counter Names \u00b6 Each counter name is composed of the interface name and the counter name. Currently, the supported interfaces are: graph_storageClient // Requests sent via storageClient, when sending requests to multiple storages concurrently, counted as one graph_metaClient // Requests sent via metaClient graph_graph_all // Requests sent by the client to the graph, when a request contains multiple queries, counted as one graph_insertVertex // Insert a vertex graph_insertEdge // Insert an edge graph_deleteVertex // Delete a vertex graph_deleteEdge // Delete an edge // Not supported yet graph_updateVertex // Update properties of a vertex graph_updateEdge // Update properties of an edge graph_go // Execute the go command graph_findPath // Find the shortest path or the full path graph_fetchVertex // Fetch the vertex's properties. Only count the commands executed rather than the total number of fetched vertices. graph_fetchEdge // Fetch the edge's properties. Only count the commands executed rather than the total number of fetched edges. Each interface has three metrics, namely latency (in the units of us), QPS and QPS with errors. The suffixes are as follows: _latency _qps _error_qps The complete metric concatenates the interface name with the corresponding metric, such as graph_insertVertex_latency , graph_insertVertex_qps and graph_insertVertex_error_qps , representing the latency of inserting a vertex, QPS and the QPS with errors, respectively. Statistics Type \u00b6 Currently supported types are SUM, COUNT, AVG, RATE and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles. Time Range \u00b6 Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now. Obtain the Corresponding Metrics via HTTP Interface \u00b6 According to the above introduction, you can make a complete metrics name. Here are some examples: graph_insertVertex_latency . avg .60 // the average latency of successfully inserting a vertex in the last minute graph_updateEdge_error_qps . count .3600 // total number of failures in updating an edge in the last hour Assume that a graph service is started locally, and the ws_http_port port number is set to 13000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60\" # graph_insertVertex_qps.rate.60=3069 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60, graph_deleteVertex_latency.avg.60\" # graph_insertVertex_qps.rate.60=3069 # graph_deleteVertex_latency.avg.60=837 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60, graph_deleteVertex_latency.avg.60&returnjson\" # [{\"value\":2373,\"name\":\"graph_insertVertex_qps.rate.60\"},{\"value\":760,\"name\":\"graph_deleteVertex_latency.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:13000/get_stats?stats\" # or curl -G \"http://127.0.0.1:13000/get_stats\"","title":"Graph Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#graph_metrics","text":"","title":"Graph Metrics"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#introduction","text":"Currently, Nebula Graph supports obtaining the basic performance metric for the graph service via HTTP. Each performance metrics consists of three parts, namely <counter_name>.<statistic_type>.<time_range> .","title":"Introduction"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#counter_names","text":"Each counter name is composed of the interface name and the counter name. Currently, the supported interfaces are: graph_storageClient // Requests sent via storageClient, when sending requests to multiple storages concurrently, counted as one graph_metaClient // Requests sent via metaClient graph_graph_all // Requests sent by the client to the graph, when a request contains multiple queries, counted as one graph_insertVertex // Insert a vertex graph_insertEdge // Insert an edge graph_deleteVertex // Delete a vertex graph_deleteEdge // Delete an edge // Not supported yet graph_updateVertex // Update properties of a vertex graph_updateEdge // Update properties of an edge graph_go // Execute the go command graph_findPath // Find the shortest path or the full path graph_fetchVertex // Fetch the vertex's properties. Only count the commands executed rather than the total number of fetched vertices. graph_fetchEdge // Fetch the edge's properties. Only count the commands executed rather than the total number of fetched edges. Each interface has three metrics, namely latency (in the units of us), QPS and QPS with errors. The suffixes are as follows: _latency _qps _error_qps The complete metric concatenates the interface name with the corresponding metric, such as graph_insertVertex_latency , graph_insertVertex_qps and graph_insertVertex_error_qps , representing the latency of inserting a vertex, QPS and the QPS with errors, respectively.","title":"Counter Names"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#statistics_type","text":"Currently supported types are SUM, COUNT, AVG, RATE and P quantiles (P99, P999, ..., P999999). Among which: Metrics have suffixes _qps and _error_qps support SUM, COUNT, AVG, RATE but don't support P quantiles. Metrics have suffixes _latency support SUM, COUNT, AVG, RATE, and P quantiles.","title":"Statistics Type"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#time_range","text":"Currently, the supported time ranges are 60s, 600s, and 3600s, which correspond to the last minute, the last ten minutes, and the last hour till now.","title":"Time Range"},{"location":"manual-EN/3.build-develop-and-administration/7.monitor/4.graph-metrics/#obtain_the_corresponding_metrics_via_http_interface","text":"According to the above introduction, you can make a complete metrics name. Here are some examples: graph_insertVertex_latency . avg .60 // the average latency of successfully inserting a vertex in the last minute graph_updateEdge_error_qps . count .3600 // total number of failures in updating an edge in the last hour Assume that a graph service is started locally, and the ws_http_port port number is set to 13000 when starting. It is sent through the GET interface of HTTP. The method name is get_stats , and the parameter is stats plus the corresponding metrics name. Here's an example of getting metrics via the HTTP interface: # obtain a metrics curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60\" # graph_insertVertex_qps.rate.60=3069 # obtain multiple metrics at the same time curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60, graph_deleteVertex_latency.avg.60\" # graph_insertVertex_qps.rate.60=3069 # graph_deleteVertex_latency.avg.60=837 # obtain multiple metrics at the same time and return in json format curl -G \"http://127.0.0.1:13000/get_stats?stats=graph_insertVertex_qps.rate.60, graph_deleteVertex_latency.avg.60&returnjson\" # [{\"value\":2373,\"name\":\"graph_insertVertex_qps.rate.60\"},{\"value\":760,\"name\":\"graph_deleteVertex_latency.avg.60\"}] # obtain all the metrics curl -G \"http://127.0.0.1:13000/get_stats?stats\" # or curl -G \"http://127.0.0.1:13000/get_stats\"","title":"Obtain the Corresponding Metrics via HTTP Interface"},{"location":"manual-EN/4.contributions/","text":"Reader \u00b6 This chapter is a guideline for the contributors and developers.","title":"Reader"},{"location":"manual-EN/4.contributions/#reader","text":"This chapter is a guideline for the contributors and developers.","title":"Reader"},{"location":"manual-EN/4.contributions/contribute-to-documentation/","text":"Contribute to Documentation \u00b6 Contributing to the Nebula Graph documentation can be a rewarding experience. We welcome your participation to help make the documentation better! How to Contribute to the Docs \u00b6 There are many ways to contribute: Raise a documentation issue on GitHub . Fork the documentation, make changes or add new content on your local branch, and submit a pull request (PR) to the master branch for the docs.","title":"Contribute to Documentation"},{"location":"manual-EN/4.contributions/contribute-to-documentation/#contribute_to_documentation","text":"Contributing to the Nebula Graph documentation can be a rewarding experience. We welcome your participation to help make the documentation better!","title":"Contribute to Documentation"},{"location":"manual-EN/4.contributions/contribute-to-documentation/#how_to_contribute_to_the_docs","text":"There are many ways to contribute: Raise a documentation issue on GitHub . Fork the documentation, make changes or add new content on your local branch, and submit a pull request (PR) to the master branch for the docs.","title":"How to Contribute to the Docs"},{"location":"manual-EN/4.contributions/cpp-coding-style/","text":"Cpp Coding Style \u00b6 Please Refer to Google C++ Style Guide .","title":"cpp Coding Style"},{"location":"manual-EN/4.contributions/cpp-coding-style/#cpp_coding_style","text":"Please Refer to Google C++ Style Guide .","title":"Cpp Coding Style"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/","text":"Developer Documentation Style Guide \u00b6 Key Point: Use this guide as a style reference for our developer documentation. Goals \u00b6 The guide can help you avoid making decisions about the same issue over and over, can provide editorial assistance on structuring and writing your documentation, and can help you keep your documentation consistent with our other documentation. Non-Goals \u00b6 This guide isn't intended to provide an industry documentation standard, nor to compete with other well-known style guides. It's a description of our house style, not a statement that our decisions are objectively correct. This guide is a living document; it changes over time, and when it changes, we generally don't change previously published documentation to match. We strive for consistency to the extent feasible, but at any given time there are certain to be parts of our documentation that don't match this style guide. When in doubt, follow this guide rather than imitating existing documents. Breaking the \"Rules\" \u00b6 In most contexts, Nebula Graph has no ability nor desire to enforce these guidelines if they're not appropriate to the context. But we hope that you'll join us in striving for high-quality documentation. Like most style guides, our style guide aims to improve our documentation, especially by improving consistency; therefore, there may be contexts where it makes sense to diverge from our guidelines in order to make your documentation better. Style and Authorial Tone \u00b6 Aim, in your documents, for a voice and tone that's conversational, friendly, and respectful without being overly colloquial or frivolous; a voice that's casual and natural and approachable, not pedantic or pushy. Try to sound like a knowledgeable friend who understands what the developer wants to do. Don't try to write exactly the way you speak; you probably speak more colloquially and verbosely than you should write, at least for developer documentation. But aim for a conversational tone rather than a formal one. Some Techniques and Approaches to Consider \u00b6 If you're having trouble expressing something, step back and ask yourself, \"What am I trying to say?\" Often, the answer you give yourself reveals what you should be saying in the document. If you're uncertain about your phrasing or tone, ask a colleague to take a look. Try reading parts of your document out loud, or at least mouthing the words. Does it sound natural? Not every sentence has to sound natural when spoken; these are written documents. But if you come across a sentence that's awkward or confusing when spoken, consider whether you can make it more conversational. Use transitions between sentences. Phrases like \"Though\" or \"This way\" can make paragraphs less stilted. (Then again, sometimes transitions like \"However\" or \"Nonetheless\" can make paragraphs more stilted.) Even if you're having trouble hitting the right tone, make sure you're communicating useful information in a clear and direct way; that's the most important part. Tense \u00b6 In general, use present tense rather than future tense; in particular, try to avoid using will where possible. The fact that the reader will be writing and running code in the future isn't a good reason to use future tense. Stick with present tense where. Also avoid the hypothetical future would . Links \u00b6 When you're writing link text, use a phrase that describes what the reader will see after following the link. That can take either of two forms: The exact title of the linked-to page, capitalized the same way the title is capitalized. A description of the linked-to page, capitalized like ordinary text instead of like a title. A couple of specific things to not do in link text: Don't use the phrase \"click here.\" (It's bad for accessibility and bad for scannability.) Similarly, don't use phrases like \"this document.\" (It's easy to read \"this\" as meaning \"the one you're reading now\" rather than \"the one I'm pointing to.\") Don't use a URL as link text. Instead, use the page title or a description of the page. Punctuation With Links \u00b6 If you have punctuation immediately before or after a link, put the punctuation outside of the link tags where possible. In particular, put quotation marks outside of link tags. Accessible Content \u00b6 General dos and Don'ts \u00b6 Ensure that readers can reach all parts of the document (including tabs, form-submission buttons, and interactive elements) using only a keyboard, without a mouse or trackpad. Don't use color, size, location, or other visual cues as the primary way of communicating information. If you're using color, icon, or outline thickness to convey state, then also provide a secondary cue, such as a change in the text label. Refer to buttons and other elements by their label (or aria-label , if they\u2019re visual elements), not by location or shape. Avoid unnecessary font formatting. (Screen readers explicitly describe text modifications.) If you're documenting a product that includes specialized accessibility features, then explicitly document those features. For example, the gcloud command-line tool includes togglable accessibility features such as percentage progress bars and ASCII box rendering. Images \u00b6 For every image, provide alt text that adequately summarizes the intent of each image. Don't present new information in images; always provide an equivalent text explanation with the image. Use SVG files or crushed PNG images. Provide high-resolution images when practical. Tables \u00b6 If your tables include both row and column headings, then mark heading cells with the scope attribute. If your tables have more than one row containing column headings, then use the headers attribute. Forms \u00b6 Label every input field, using a <label> element. Place labels outside of fields. When you're creating an error message for form validation, clearly state what went wrong and how to fix it. For example: \"Name is a required field.\" Videos \u00b6 Provide captions. Ensure that captions can be translated into major languages. Language and Grammar \u00b6 Use second person: \"you\" rather than \"we.\" Use active voice; make clear who's performing the action. Use standard American spelling and punctuation. Put conditional clauses before instructions, not after. For usage and spelling of specific words, see the word list. Formatting, Punctuation, and Organization \u00b6 Use sentence case for document titles and section headings. Use numbered lists for sequences. Use bulleted lists for most other lists. Use description lists for pairs of related pieces of data. Use serial commas . Put code-related text in code font . Put UI elements in bold . Use unambiguous date formatting .","title":"Developer Documentation Style Guide"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#developer_documentation_style_guide","text":"Key Point: Use this guide as a style reference for our developer documentation.","title":"Developer Documentation Style Guide"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#goals","text":"The guide can help you avoid making decisions about the same issue over and over, can provide editorial assistance on structuring and writing your documentation, and can help you keep your documentation consistent with our other documentation.","title":"Goals"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#non-goals","text":"This guide isn't intended to provide an industry documentation standard, nor to compete with other well-known style guides. It's a description of our house style, not a statement that our decisions are objectively correct. This guide is a living document; it changes over time, and when it changes, we generally don't change previously published documentation to match. We strive for consistency to the extent feasible, but at any given time there are certain to be parts of our documentation that don't match this style guide. When in doubt, follow this guide rather than imitating existing documents.","title":"Non-Goals"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#breaking_the_rules","text":"In most contexts, Nebula Graph has no ability nor desire to enforce these guidelines if they're not appropriate to the context. But we hope that you'll join us in striving for high-quality documentation. Like most style guides, our style guide aims to improve our documentation, especially by improving consistency; therefore, there may be contexts where it makes sense to diverge from our guidelines in order to make your documentation better.","title":"Breaking the \"Rules\""},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#style_and_authorial_tone","text":"Aim, in your documents, for a voice and tone that's conversational, friendly, and respectful without being overly colloquial or frivolous; a voice that's casual and natural and approachable, not pedantic or pushy. Try to sound like a knowledgeable friend who understands what the developer wants to do. Don't try to write exactly the way you speak; you probably speak more colloquially and verbosely than you should write, at least for developer documentation. But aim for a conversational tone rather than a formal one.","title":"Style and Authorial Tone"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#some_techniques_and_approaches_to_consider","text":"If you're having trouble expressing something, step back and ask yourself, \"What am I trying to say?\" Often, the answer you give yourself reveals what you should be saying in the document. If you're uncertain about your phrasing or tone, ask a colleague to take a look. Try reading parts of your document out loud, or at least mouthing the words. Does it sound natural? Not every sentence has to sound natural when spoken; these are written documents. But if you come across a sentence that's awkward or confusing when spoken, consider whether you can make it more conversational. Use transitions between sentences. Phrases like \"Though\" or \"This way\" can make paragraphs less stilted. (Then again, sometimes transitions like \"However\" or \"Nonetheless\" can make paragraphs more stilted.) Even if you're having trouble hitting the right tone, make sure you're communicating useful information in a clear and direct way; that's the most important part.","title":"Some Techniques and Approaches to Consider"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#tense","text":"In general, use present tense rather than future tense; in particular, try to avoid using will where possible. The fact that the reader will be writing and running code in the future isn't a good reason to use future tense. Stick with present tense where. Also avoid the hypothetical future would .","title":"Tense"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#links","text":"When you're writing link text, use a phrase that describes what the reader will see after following the link. That can take either of two forms: The exact title of the linked-to page, capitalized the same way the title is capitalized. A description of the linked-to page, capitalized like ordinary text instead of like a title. A couple of specific things to not do in link text: Don't use the phrase \"click here.\" (It's bad for accessibility and bad for scannability.) Similarly, don't use phrases like \"this document.\" (It's easy to read \"this\" as meaning \"the one you're reading now\" rather than \"the one I'm pointing to.\") Don't use a URL as link text. Instead, use the page title or a description of the page.","title":"Links"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#punctuation_with_links","text":"If you have punctuation immediately before or after a link, put the punctuation outside of the link tags where possible. In particular, put quotation marks outside of link tags.","title":"Punctuation With Links"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#accessible_content","text":"","title":"Accessible Content"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#general_dos_and_donts","text":"Ensure that readers can reach all parts of the document (including tabs, form-submission buttons, and interactive elements) using only a keyboard, without a mouse or trackpad. Don't use color, size, location, or other visual cues as the primary way of communicating information. If you're using color, icon, or outline thickness to convey state, then also provide a secondary cue, such as a change in the text label. Refer to buttons and other elements by their label (or aria-label , if they\u2019re visual elements), not by location or shape. Avoid unnecessary font formatting. (Screen readers explicitly describe text modifications.) If you're documenting a product that includes specialized accessibility features, then explicitly document those features. For example, the gcloud command-line tool includes togglable accessibility features such as percentage progress bars and ASCII box rendering.","title":"General dos and Don'ts"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#images","text":"For every image, provide alt text that adequately summarizes the intent of each image. Don't present new information in images; always provide an equivalent text explanation with the image. Use SVG files or crushed PNG images. Provide high-resolution images when practical.","title":"Images"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#tables","text":"If your tables include both row and column headings, then mark heading cells with the scope attribute. If your tables have more than one row containing column headings, then use the headers attribute.","title":"Tables"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#forms","text":"Label every input field, using a <label> element. Place labels outside of fields. When you're creating an error message for form validation, clearly state what went wrong and how to fix it. For example: \"Name is a required field.\"","title":"Forms"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#videos","text":"Provide captions. Ensure that captions can be translated into major languages.","title":"Videos"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#language_and_grammar","text":"Use second person: \"you\" rather than \"we.\" Use active voice; make clear who's performing the action. Use standard American spelling and punctuation. Put conditional clauses before instructions, not after. For usage and spelling of specific words, see the word list.","title":"Language and Grammar"},{"location":"manual-EN/4.contributions/developer-documentation-style-guide/#formatting_punctuation_and_organization","text":"Use sentence case for document titles and section headings. Use numbered lists for sequences. Use bulleted lists for most other lists. Use description lists for pairs of related pieces of data. Use serial commas . Put code-related text in code font . Put UI elements in bold . Use unambiguous date formatting .","title":"Formatting, Punctuation, and Organization"},{"location":"manual-EN/4.contributions/how-to-contribute/","text":"How to Contribute \u00b6 Step 1: Fork in the Cloud \u00b6 Visit https://github.com/vesoft-inc/nebula Click Fork button (top right) to establish a cloud-based fork. Step 2: Clone Fork to Local Storage \u00b6 Define a local working directory: # Define your working directory working_dir = $HOME /Workspace Set user to match your Github profile name: user ={ your Github profile name } Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula.git # the following is recommended # or: git clone git@github.com:$user/nebula.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula.git # or: git remote add upstream git@github.com:vesoft-inc/nebula.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that your remotes make sense: # It should look like: # origin git@github.com:$(user)/nebula.git (fetch) # origin git@github.com:$(user)/nebula.git (push) # upstream https://github.com/vesoft-inc/nebula (fetch) # upstream no_push (push) git remote -v Define a Pre-Commit Hook \u00b6 Please link the Nebula Graph pre-commit hook into your .git directory. This hook checks your commits for formatting, building, doc generation, etc. cd $working_dir /nebula/.git/hooks ln -s ../../.linters/cpp/hooks/pre-commit.sh . Sometimes, pre-commit hook can not be executable. In such case, you have to make it executable manually. cd $working_dir /nebula/.git/hooks chmod +x pre-commit Step 3: Branch \u00b6 Get your local master up to date: cd $working_dir /nebula git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master: git checkout -b myfeature NOTE : Because your PR often consists of several commits, which might be squashed while being merged into upstream, we strongly suggest you open a separate topic branch to make your changes on. After merged, this topic branch could be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, maybe you must use a hard reset on the master branch, like: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master Step 4: Develop \u00b6 Edit the Code \u00b6 You can now edit the code on myfeature branch. Please be noted that we are following Google C++ Style Guide . Verifying Your Code \u00b6 Compiling the Source Code \u00b6 Please refer to the build source code documentation to compile. Code Verification \u00b6 Replace the binary files The compiled binary files of the three services are in nebula/build/src/daemon/_build/ directory. The compiled console is in nebula/build/src/console/_build directory. You can replace the binary files in the bin directory, restart the services and verify. - Add unit tests There is a test directory in the modified code module. You can add unit tests in it, then compile and run the unit tests. Please make sure your submitted codes pass all the unit tests. - Run all the unit tests cd nebula/build ctest -j $( nproc ) Step 5: Keep Your Branch in Sync \u00b6 # While on your myfeature branch. git fetch upstream git rebase upstream/master Step 6: Commit \u00b6 Commit your changes. git commit Likely you'll go back and edit/build/test some more than commit --amend in a few cycles. Step 7: Push \u00b6 When ready to review (or just to establish an offsite backup or your work), push your branch to your fork on github.com : git push -f origin myfeature Step 8: Create a Pull Request \u00b6 Visit your fork at https://github.com/$user/nebula (replace $user obviously). Click the Compare & pull request button next to your myfeature branch. Step 9: Get a Code Review \u00b6 Once your pull request has been opened, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to ensure the changes meet the repository's contributing guidelines and other quality standards.","title":"How to Contribute"},{"location":"manual-EN/4.contributions/how-to-contribute/#how_to_contribute","text":"","title":"How to Contribute"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_1_fork_in_the_cloud","text":"Visit https://github.com/vesoft-inc/nebula Click Fork button (top right) to establish a cloud-based fork.","title":"Step 1: Fork in the Cloud"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_2_clone_fork_to_local_storage","text":"Define a local working directory: # Define your working directory working_dir = $HOME /Workspace Set user to match your Github profile name: user ={ your Github profile name } Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/ $user /nebula.git # the following is recommended # or: git clone git@github.com:$user/nebula.git cd $working_dir /nebula git remote add upstream https://github.com/vesoft-inc/nebula.git # or: git remote add upstream git@github.com:vesoft-inc/nebula.git # Never push to upstream master since you do not have write access. git remote set-url --push upstream no_push # Confirm that your remotes make sense: # It should look like: # origin git@github.com:$(user)/nebula.git (fetch) # origin git@github.com:$(user)/nebula.git (push) # upstream https://github.com/vesoft-inc/nebula (fetch) # upstream no_push (push) git remote -v","title":"Step 2: Clone Fork to Local Storage"},{"location":"manual-EN/4.contributions/how-to-contribute/#define_a_pre-commit_hook","text":"Please link the Nebula Graph pre-commit hook into your .git directory. This hook checks your commits for formatting, building, doc generation, etc. cd $working_dir /nebula/.git/hooks ln -s ../../.linters/cpp/hooks/pre-commit.sh . Sometimes, pre-commit hook can not be executable. In such case, you have to make it executable manually. cd $working_dir /nebula/.git/hooks chmod +x pre-commit","title":"Define a Pre-Commit Hook"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_3_branch","text":"Get your local master up to date: cd $working_dir /nebula git fetch upstream git checkout master git rebase upstream/master Checkout a new branch from master: git checkout -b myfeature NOTE : Because your PR often consists of several commits, which might be squashed while being merged into upstream, we strongly suggest you open a separate topic branch to make your changes on. After merged, this topic branch could be just abandoned, thus you could synchronize your master branch with upstream easily with a rebase like above. Otherwise, if you commit your changes directly into master, maybe you must use a hard reset on the master branch, like: git fetch upstream git checkout master git reset --hard upstream/master git push --force origin master","title":"Step 3: Branch"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_4_develop","text":"","title":"Step 4: Develop"},{"location":"manual-EN/4.contributions/how-to-contribute/#edit_the_code","text":"You can now edit the code on myfeature branch. Please be noted that we are following Google C++ Style Guide .","title":"Edit the Code"},{"location":"manual-EN/4.contributions/how-to-contribute/#verifying_your_code","text":"","title":"Verifying Your Code"},{"location":"manual-EN/4.contributions/how-to-contribute/#compiling_the_source_code","text":"Please refer to the build source code documentation to compile.","title":"Compiling the Source Code"},{"location":"manual-EN/4.contributions/how-to-contribute/#code_verification","text":"Replace the binary files The compiled binary files of the three services are in nebula/build/src/daemon/_build/ directory. The compiled console is in nebula/build/src/console/_build directory. You can replace the binary files in the bin directory, restart the services and verify. - Add unit tests There is a test directory in the modified code module. You can add unit tests in it, then compile and run the unit tests. Please make sure your submitted codes pass all the unit tests. - Run all the unit tests cd nebula/build ctest -j $( nproc )","title":"Code Verification"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_5_keep_your_branch_in_sync","text":"# While on your myfeature branch. git fetch upstream git rebase upstream/master","title":"Step 5: Keep Your Branch in Sync"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_6_commit","text":"Commit your changes. git commit Likely you'll go back and edit/build/test some more than commit --amend in a few cycles.","title":"Step 6: Commit"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_7_push","text":"When ready to review (or just to establish an offsite backup or your work), push your branch to your fork on github.com : git push -f origin myfeature","title":"Step 7: Push"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_8_create_a_pull_request","text":"Visit your fork at https://github.com/$user/nebula (replace $user obviously). Click the Compare & pull request button next to your myfeature branch.","title":"Step 8: Create a Pull Request"},{"location":"manual-EN/4.contributions/how-to-contribute/#step_9_get_a_code_review","text":"Once your pull request has been opened, it will be assigned to at least two reviewers. Those reviewers will do a thorough code review to ensure the changes meet the repository's contributing guidelines and other quality standards.","title":"Step 9: Get a Code Review"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/","text":"Pull Request and Commit Message Guidelines \u00b6 This document describes the commit message and Pull Request style applied to all Nebula Graph repositories. Every commit made directly to the master branch must follow the below guidelines. Commit Message \u00b6 <type> ( <scope> ) : <subject> // scope is optional, subject is must <body> // optional <footer> // optional These rules are adopted from the AngularJS commit convention . <Type> describes the kind of change that this commit is providing. <subject> is a short description of the change. If additional details are required, add a blank line, and then provide explanation and context in paragraph format. Commit Types \u00b6 Type Description Feature New features Fix Bug fix Doc Documentation changes Style Formatting, missing semi colons, ... Refactor Code cleanup Test New tests Chore Maintain Pull Request \u00b6 When you submit a Pull Request, please include enough details about all changes in the title but keep it concise. The title of a pull request must briefly describe the changes made. For very simple changes, you can leave the description blank as there\u2019s no need to describe what will be obvious from looking at the diff. For more complex changes, give an overview of the changes. If the PR fixes an issue, make sure to include the GitHub issue-number in the description. Pull Request Template \u00b6 What changes were proposed in this pull request? Why are the changes needed? Does this PR introduce any user-facing change? How was this patch tested?","title":"Pull Request and Commit Message Guidelines"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/#pull_request_and_commit_message_guidelines","text":"This document describes the commit message and Pull Request style applied to all Nebula Graph repositories. Every commit made directly to the master branch must follow the below guidelines.","title":"Pull Request and Commit Message Guidelines"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/#commit_message","text":"<type> ( <scope> ) : <subject> // scope is optional, subject is must <body> // optional <footer> // optional These rules are adopted from the AngularJS commit convention . <Type> describes the kind of change that this commit is providing. <subject> is a short description of the change. If additional details are required, add a blank line, and then provide explanation and context in paragraph format.","title":"Commit Message"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/#commit_types","text":"Type Description Feature New features Fix Bug fix Doc Documentation changes Style Formatting, missing semi colons, ... Refactor Code cleanup Test New tests Chore Maintain","title":"Commit Types"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/#pull_request","text":"When you submit a Pull Request, please include enough details about all changes in the title but keep it concise. The title of a pull request must briefly describe the changes made. For very simple changes, you can leave the description blank as there\u2019s no need to describe what will be obvious from looking at the diff. For more complex changes, give an overview of the changes. If the PR fixes an issue, make sure to include the GitHub issue-number in the description.","title":"Pull Request"},{"location":"manual-EN/4.contributions/pull-request-commit-message-guidelines/#pull_request_template","text":"What changes were proposed in this pull request? Why are the changes needed? Does this PR introduce any user-facing change? How was this patch tested?","title":"Pull Request Template"},{"location":"manual-EN/5.appendix/cypher-ngql/","text":"Comparison Between Cypher and nGQL \u00b6 Conceptual Comparisons \u00b6 Name Cypher nGQL vertex, node node vertex edge, relationship relationship edge vertex type label tag edge type relationship type edge type vertex identifier node id generated by default vid edge identifier edge id generated by default Basic Graph Operations \u00b6 Operations Cypher nGQL List all labels/tags * MATCH (n) RETURN distinct labels(n); * call db.labels(); SHOW TAGS Insert a vertex with a specified type CREATE (:Person {age: 16}) INSERT VERTEX (prop_name_list) VALUES \\ :(prop_value_list) Insert an edge with specified edge type CREATE (src)-[rel:LIKES]->(dst) SET rel.prop = V INSERT EDGE ( ) VALUES -> [@ ]: ( ) Delete a vertex MATCH (n) WHERE ID(n) = vid DETACH DELETE n DELETE VERTEX \\ Delete an edge MATCH ()-[r]->() WHERE ID(r)=edgeID DELETE r DELETE EDGE \\ -> \\ [@ ] Update a vertex property SET n.name = V UPDATE VERTEX \\ SET Fetch vertice prop MATCH (n) WHERE ID(n) = vid RETURN properties(n) FETCH PROP ON \\ Fetch edges prop MATCH (n)-[r]->() WHERE ID(r)=edgeID return properties(r) FETCH PROP ON -> [@ ] Query a vertex along specified edge type MATCH (n)-[r:edge_type]->() WHERE ID(n) = vid GO FROM \\ OVER \\ Query a vertex along specified edge type reversely MATCH (n)<-[r:edge_type]-() WHERE ID(n) = vid GO FROM \\ OVER \\ REVERSELY Get the N-Hop along a specified edge type MATCH (n)-[r:edge_type*N]->() WHERE ID(n) = vid return r GO N STEPS FROM \\ OVER \\ Find path between two vertices MATCH p =(a)-[]->(b) WHERE ID(a) = a_vid AND ID(b) = b_vid RETURN p FIND ALL PATH FROM \\ TO \\ OVER * Example Queries \u00b6 The example queries are based on the graph below: Insert data # insert vertex nebula> INSERT VERTEX character(name, age, type) VALUES hash(\"saturn\"):(\"saturn\", 10000, \"titan\"), hash(\"jupiter\"):(\"jupiter\", 5000, \"god\"); # insert edge nebula> INSERT EDGE father() VALUES hash(\"jupiter\")->hash(\"saturn\"):(); // cypher cypher> CREATE (src:character {name:\"saturn\", age: 10000, type:\"titan\"}) > CREATE (dst:character {name:\"jupiter\", age: 5000, type:\"god\"}) > CREATE (src)-[rel:father]->(dst) ``` - Delete vertex ```ngql nebula> DELETE VERTEX hash(\"prometheus\"); cypher> MATCH (n:character {name:\"prometheus\"}) > DETACH DELETE n Update vertex nebula> UPDATE VERTEX hash(\"jesus\") SET character.type = 'titan'; cypher> MATCH (n:character {name:\"jesus\"}) > SET n.type = 'titan' Fetch vertices properties nebula> FETCH PROP ON character hash(\"saturn\"); =================================================== | character.name | character.age | character.type | =================================================== | saturn | 10000 | titan | --------------------------------------------------- cypher> MATCH (n:character {name:\"saturn\"}) > RETURN properties(n) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"properties(n)\" \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502{\"name\":\"saturn\",\"type\":\"titan\",\"age\":10000}\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the name of hercules's grandfather nebula> GO 2 STEPS FROM hash(\"hercules\") OVER father YIELD $$.character.name; ===================== | $$.character.name | ===================== | saturn | --------------------- cypher> MATCH (src:character{name:\"hercules\"})-[r:father*2]->(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"satun\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the name of hercules's father nebula> GO FROM hash(\"hercules\") OVER father YIELD $$.character.name; ===================== | $$.character.name | ===================== | jupiter | --------------------- cypher> MATCH (src:character{name:\"hercules\"})-[r:father]->(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"jupiter\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the the centenarians' name. nebula> # coming soon cypher> MATCH (src:character) > WHERE src.age > 100 > RETURN src.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"src.name\" \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \"saturn\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"jupiter\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"neptune\" \u2502 \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 \u2502 \"pluto\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find who live with pluto nebula> GO FROM hash(\"pluto\") OVER lives YIELD lives._dst AS place | GO FROM $-.place OVER lives REVERSELY WHERE \\ > $$.character.name != \"pluto\" YIELD $$.character.name AS cohabitants; =============== | cohabitants | =============== | cerberus | --------------- cypher> MATCH (src:character{name:\"pluto\"})-[r1:lives]->()<-[r2:lives]-(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"cerberus\"\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find pluto's brother and their habitations? nebula> GO FROM hash(\"pluto\") OVER brother YIELD brother._dst AS god | \\ > GO FROM $-.god OVER lives YIELD $^.character.name AS Brother, $$.location.name AS Habitations; ========================= | Brother | Habitations | ========================= | jupiter | sky | ------------------------- | neptune | sea | ------------------------- cypher> MATCH (src:Character{name:\"pluto\"})-[r1:brother]->(bro:Character)-[r2:lives]->(dst) > RETURN bro.name, dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"bro.name\" \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \"jupiter\" \u2502 \"sky\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"neptune\" \u2502 \"sea\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Cypher & nGQL"},{"location":"manual-EN/5.appendix/cypher-ngql/#comparison_between_cypher_and_ngql","text":"","title":"Comparison Between Cypher and nGQL"},{"location":"manual-EN/5.appendix/cypher-ngql/#conceptual_comparisons","text":"Name Cypher nGQL vertex, node node vertex edge, relationship relationship edge vertex type label tag edge type relationship type edge type vertex identifier node id generated by default vid edge identifier edge id generated by default","title":"Conceptual Comparisons"},{"location":"manual-EN/5.appendix/cypher-ngql/#basic_graph_operations","text":"Operations Cypher nGQL List all labels/tags * MATCH (n) RETURN distinct labels(n); * call db.labels(); SHOW TAGS Insert a vertex with a specified type CREATE (:Person {age: 16}) INSERT VERTEX (prop_name_list) VALUES \\ :(prop_value_list) Insert an edge with specified edge type CREATE (src)-[rel:LIKES]->(dst) SET rel.prop = V INSERT EDGE ( ) VALUES -> [@ ]: ( ) Delete a vertex MATCH (n) WHERE ID(n) = vid DETACH DELETE n DELETE VERTEX \\ Delete an edge MATCH ()-[r]->() WHERE ID(r)=edgeID DELETE r DELETE EDGE \\ -> \\ [@ ] Update a vertex property SET n.name = V UPDATE VERTEX \\ SET Fetch vertice prop MATCH (n) WHERE ID(n) = vid RETURN properties(n) FETCH PROP ON \\ Fetch edges prop MATCH (n)-[r]->() WHERE ID(r)=edgeID return properties(r) FETCH PROP ON -> [@ ] Query a vertex along specified edge type MATCH (n)-[r:edge_type]->() WHERE ID(n) = vid GO FROM \\ OVER \\ Query a vertex along specified edge type reversely MATCH (n)<-[r:edge_type]-() WHERE ID(n) = vid GO FROM \\ OVER \\ REVERSELY Get the N-Hop along a specified edge type MATCH (n)-[r:edge_type*N]->() WHERE ID(n) = vid return r GO N STEPS FROM \\ OVER \\ Find path between two vertices MATCH p =(a)-[]->(b) WHERE ID(a) = a_vid AND ID(b) = b_vid RETURN p FIND ALL PATH FROM \\ TO \\ OVER *","title":"Basic Graph Operations"},{"location":"manual-EN/5.appendix/cypher-ngql/#example_queries","text":"The example queries are based on the graph below: Insert data # insert vertex nebula> INSERT VERTEX character(name, age, type) VALUES hash(\"saturn\"):(\"saturn\", 10000, \"titan\"), hash(\"jupiter\"):(\"jupiter\", 5000, \"god\"); # insert edge nebula> INSERT EDGE father() VALUES hash(\"jupiter\")->hash(\"saturn\"):(); // cypher cypher> CREATE (src:character {name:\"saturn\", age: 10000, type:\"titan\"}) > CREATE (dst:character {name:\"jupiter\", age: 5000, type:\"god\"}) > CREATE (src)-[rel:father]->(dst) ``` - Delete vertex ```ngql nebula> DELETE VERTEX hash(\"prometheus\"); cypher> MATCH (n:character {name:\"prometheus\"}) > DETACH DELETE n Update vertex nebula> UPDATE VERTEX hash(\"jesus\") SET character.type = 'titan'; cypher> MATCH (n:character {name:\"jesus\"}) > SET n.type = 'titan' Fetch vertices properties nebula> FETCH PROP ON character hash(\"saturn\"); =================================================== | character.name | character.age | character.type | =================================================== | saturn | 10000 | titan | --------------------------------------------------- cypher> MATCH (n:character {name:\"saturn\"}) > RETURN properties(n) \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"properties(n)\" \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502{\"name\":\"saturn\",\"type\":\"titan\",\"age\":10000}\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the name of hercules's grandfather nebula> GO 2 STEPS FROM hash(\"hercules\") OVER father YIELD $$.character.name; ===================== | $$.character.name | ===================== | saturn | --------------------- cypher> MATCH (src:character{name:\"hercules\"})-[r:father*2]->(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"satun\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the name of hercules's father nebula> GO FROM hash(\"hercules\") OVER father YIELD $$.character.name; ===================== | $$.character.name | ===================== | jupiter | --------------------- cypher> MATCH (src:character{name:\"hercules\"})-[r:father]->(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"jupiter\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find the the centenarians' name. nebula> # coming soon cypher> MATCH (src:character) > WHERE src.age > 100 > RETURN src.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"src.name\" \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \"saturn\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"jupiter\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"neptune\" \u2502 \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 \u2502 \"pluto\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find who live with pluto nebula> GO FROM hash(\"pluto\") OVER lives YIELD lives._dst AS place | GO FROM $-.place OVER lives REVERSELY WHERE \\ > $$.character.name != \"pluto\" YIELD $$.character.name AS cohabitants; =============== | cohabitants | =============== | cerberus | --------------- cypher> MATCH (src:character{name:\"pluto\"})-[r1:lives]->()<-[r2:lives]-(dst:character) > RETURN dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502\"cerberus\"\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Find pluto's brother and their habitations? nebula> GO FROM hash(\"pluto\") OVER brother YIELD brother._dst AS god | \\ > GO FROM $-.god OVER lives YIELD $^.character.name AS Brother, $$.location.name AS Habitations; ========================= | Brother | Habitations | ========================= | jupiter | sky | ------------------------- | neptune | sea | ------------------------- cypher> MATCH (src:Character{name:\"pluto\"})-[r1:brother]->(bro:Character)-[r2:lives]->(dst) > RETURN bro.name, dst.name \u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555 \u2502\"bro.name\" \u2502\"dst.name\"\u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \"jupiter\" \u2502 \"sky\" \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \"neptune\" \u2502 \"sea\" \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Example Queries"},{"location":"manual-EN/5.appendix/gremlin-ngql/","text":"Comparison Between Gremlin and nGQL \u00b6 Introduction to Gremlin \u00b6 Gremlin is a graph traversal language developed by Apache TinkerPop. It can be either declarative or imperative. Gremlin is Groovy-based, but has many language variants that allow developers to write Gremlin queries natively in many modern programming languages such as Java, JavaScript, Python, Scala, Clojure and Groovy. Introduction to nGQL \u00b6 Nebula Graph introduces its own query language, nGQL , which is a declarative, textual query language like SQL, but for graphs. Unlike SQL, nGQL is all about expressing graph patterns. The features of nGQL are as follows: Syntax is close to SQL, but not exactly the same (Easy to learn) Expandable Keyword is case insensitive Support basic graph traverse Support pattern matching Support aggregation Support graph mutation Support distributed transaction (future release) Statement composition, but NO statement embedding (Easy to read) Conceptual Comparisons \u00b6 Name Gremlin nGQL vertex, node vertex vertex edge, relationship edge edge vertex type label tag edge type label edge type vertex id vid vid edge id eid not support In Gremlin and nGQL, vertices and edges are identified with unique identifiers. In Nebula Graph , you can either specify identifiers or generate automatically with the hash or uuid function. Basic Graph Operations \u00b6 Name Gremlin nGQL Create a new graph g = TinkerGraph.open().traversal() CREATE SPACE gods Show vertices' types g.V().label() SHOW TAGS Insert a vertex with a specified type g.addV(String vertexLabel).property() INSERT VERTEX (prop_name_list) VALUES \\ :(prop_value_list) Insert an edge with specified edge type g.addE(String edgeLabel).from(v1).to(v2).property() INSERT EDGE ( ) VALUES -> : ( ) Delete a vertex g.V(\\ ).drop() DELETE VERTEX \\ Delete an edge g.E(\\ ).outE(\\ ).where(otherV().is(\\ ))drop() DELETE EDGE \\ -> \\ Update a vertex property g.V(\\ ).property() UPDATE VERTEX \\ SET Fetch vertices with ID g.V(\\ ) FETCH PROP ON \\ Fetch edges with ID g.E( >> ) FETCH PROP ON -> Query a vertex along specified edge type g.V(\\ ).outE( \\ ) GO FROM \\ OVER \\ Query a vertex along specified edge type reversely g.V(\\ ).in( \\ ) GO FROM \\ OVER \\ REVERSELY Query N hops along a specified edge g.V(\\ ).repeat(out(\\ )).times(N) GO N STEPS FROM \\ OVER \\ Find path between two vertices g.V(\\ ).repeat(out()).until(\\ ).path() FIND ALL PATH FROM \\ TO \\ OVER * Example Queries \u00b6 The examples in this section make extensive use of the toy graph distributed with Janus Graph called The Graphs of Gods . This graph is diagrammed below. The abstract data model is known as a Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Insert data # insert vertex nebula> INSERT VERTEX character ( name,age, type ) VALUES hash ( \"saturn\" ) : ( \"saturn\" , 10000 , \"titan\" ) , hash ( \"jupiter\" ) : ( \"jupiter\" , 5000 , \"god\" ) ; gremlin> saturn = g.addV ( \"character\" ) .property ( T.id, 1 ) .property ( 'name' , 'saturn' ) .property ( 'age' , 10000 ) .property ( 'type' , 'titan' ) .next () ; == >v [ 1 ] gremlin> jupiter = g.addV ( \"character\" ) .property ( T.id, 2 ) .property ( 'name' , 'jupiter' ) .property ( 'age' , 5000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 2 ] gremlin> prometheus = g.addV ( \"character\" ) .property ( T.id, 31 ) .property ( 'name' , 'prometheus' ) .property ( 'age' , 1000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 31 ] gremlin> jesus = g.addV ( \"character\" ) property ( T.id, 32 ) .property ( 'name' , 'jesus' ) .property ( 'age' , 5000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 32 ] # insert edge nebula> INSERT EDGE father () VALUES hash ( \"jupiter\" ) ->hash ( \"saturn\" ) : () ; gremlin> g.addE ( \"father\" ) .from ( jupiter ) .to ( saturn ) .property ( T.id, 13 ) ; == >e [ 13 ][ 2 -father->1 ] Delete vertex nebula> DELETE VERTEX hash ( \"prometheus\" ) ; gremlin> g.V ( prometheus ) .drop () ; Update vertex nebula> UPDATE VERTEX hash ( \"jesus\" ) SET character.type = 'titan' ; gremlin> g.V ( jesus ) .property ( 'age' , 6000 ) ; Fetch data nebula> FETCH PROP ON character hash ( \"saturn\" ) ; ================================================== | character.name | character.age | character.type | ================================================== | saturn | 10000 | titan | -------------------------------------------------- gremlin> g.V ( saturn ) .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] Find the name of hercules's grandfather nebula> LOOKUP ON character WHERE character.name == 'hercules' | \\ -> GO 2 STEPS FROM $- .VertexID OVER father YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | saturn | --------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .out ( 'father' ) .out ( 'father' ) .values ( 'name' ) ; == >saturn Find the name of hercules's father nebula> LOOKUP ON character WHERE character.name == 'hercules' | \\ -> GO FROM $- .VertexID OVER father YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | jupiter | --------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .out ( 'father' ) .values ( 'name' ) ; == >jupiter Find the characters with age > 100 nebula> LOOKUP ON character WHERE character.age > 100 YIELD character.name ; ========================================= | VertexID | character.name | ========================================= | 6761447489613431910 | pluto | ----------------------------------------- | -5860788569139907963 | neptune | ----------------------------------------- | 4863977009196259577 | jupiter | ----------------------------------------- | -4316810810681305233 | saturn | ----------------------------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'age' ,gt ( 100 )) .values ( 'name' ) ; == >saturn == >jupiter == >neptune == >pluto Find who are pluto's cohabitants nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | \\ GO FROM $- .place OVER lives REVERSELY YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | pluto | --------------- | cerberus | --------------- gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .values ( 'name' ) ; == >pluto == >cerberus pluto can't be his own cohabitant nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | GO FROM $- .place OVER lives REVERSELY WHERE \\ $$ .character.name ! = \"pluto\" YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | cerberus | --------------- gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .where ( is ( neq ( pluto ))) .values ( 'name' ) ; == >cerberus Pluto's Brothers # where do pluto's brothers live? nebula> GO FROM hash ( \"pluto\" ) OVER brother YIELD brother._dst AS brother | \\ GO FROM $- .brother OVER lives YIELD $$ .location.name ; ==================== | $$ .location.name | ==================== | sky | -------------------- | sea | -------------------- gremlin> g.V ( pluto ) .out ( 'brother' ) .out ( 'lives' ) .values ( 'name' ) ; == >sky == >sea # which brother lives in which place? nebula> GO FROM hash ( \"pluto\" ) OVER brother YIELD brother._dst AS god | \\ GO FROM $- .god OVER lives YIELD $^.character.name AS Brother, $$ .location.name AS Habitations ; ========================= | Brother | Habitations | ========================= | jupiter | sky | ------------------------- | neptune | sea | ------------------------- gremlin> g.V ( pluto ) .out ( 'brother' ) .as ( 'god' ) .out ( 'lives' ) .as ( 'place' ) .select ( 'god' , 'place' ) .by ( 'name' ) ; == > [ god:jupiter, place:sky ] == > [ god:neptune, place:sea ] Advance Queries \u00b6 Graph Exploration \u00b6 # Gremlin version gremlin> Gremlin.version () ; == >3.3.5 # Return all the vertices gremlin> g.V () ; == >v [ 1 ] == >v [ 2 ] ... nebula> # Coming soon # Count all the vertices gremlin> g.V () .count () ; == >12 nebula> # Coming soon # Count the vertices and edges by label gremlin> g.V () .groupCount () .by ( label ) ; == > [ character:9,location:3 ] gremlin> g.E () .groupCount () .by ( label ) ; == > [ mother:1,lives:5,father:2,brother:6,battled:3,pet:1 ] nebula> # Coming soon # Return all edges gremlin> g.E () ; == >e [ 13 ][ 2 -father->1 ] == >e [ 14 ][ 2 -lives->3 ] ... nebula> # Coming soon # Return vertices labels gremlin> g.V () .label () .dedup () ; == >character == >location nebula> SHOW TAGS ; ================== | ID | Name | ================== | 15 | character | ------------------ | 16 | location | ------------------ # Return edge types gremlin> g.E () .label () .dedup () ; == >father == >lives ...nebula> SHOW EDGES ; ================ | ID | Name | ================ | 17 | father | ---------------- | 18 | brother | ---------------- ... # Return all vertices properties gremlin> g.V () .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] ... nebula> # Coming soon # Return properties of vertices labeled character gremlin> g.V () .hasLabel ( 'character' ) .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] ... Traversing Edges \u00b6 Name Gremlin nGQL Out adjacent vertices to the vertex out(\\ ) GO FROM \\ OVER \\ In adjacent vertices to the vertex in(\\ ) GO FROM \\ OVER \\ REVERSELY Both adjacent vertices of the vertex both(\\ ) GO FROM \\ OVER \\ BIDIRECT # Find the out adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .out ( 'brother' ) ; == >v [ 8 ] == >v [ 5 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother ; ======================== | brother._dst | ======================== | 6761447489613431910 | ------------------------ | -5860788569139907963 | ------------------------ # Find the in adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .in ( 'brother' ) ; == >v [ 5 ] == >v [ 8 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother REVERSELY ; ======================= | brother._dst | ======================= | 4863977009196259577 | ----------------------- | 4863977009196259577 | ----------------------- # Find the both adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .both ( 'brother' ) ; == >v [ 8 ] == >v [ 5 ] == >v [ 5 ] == >v [ 8 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother BIDIRECT ; ======================= | brother._dst | ======================= | 6761447489613431910 | ------------------------ | -5860788569139907963 | | 4863977009196259577 | ----------------------- | 4863977009196259577 | ----------------------- # Two hops out traverse gremlin> g.V ( hercules ) .out ( 'father' ) .out ( 'lives' ) ; == >v [ 3 ] nebula> GO FROM hash ( \"hercules\" ) OVER father YIELD father._dst AS id | \\ GO FROM $- .id OVER lives ; ======================== | lives._dst | ======================== | -1121386748834253737 | ------------------------ Has Filter Condition \u00b6 Name Gremlin nGQL Filter vertex via identifier hasId(\\ ) FETCH PROP ON \\ Filter vertex or edge via label, key and value has(\\ , \\ , \\ ) LOOKUP \\ | \\ WHERE \\ # Filter vertex with ID saturn gremlin> g.V () .hasId ( saturn ) ; == >v [ 1 ] nebula> FETCH PROP ON * hash ( \"saturn\" ) ; ========================================================================== | VertexID | character.name | character.age | character.type | ========================================================================== | -4316810810681305233 | saturn | 10000 | titan | -------------------------------------------------------------------------- # Find for vertices with tag \"character\" and \"name\" attribute value \"hercules\" gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .valueMap () ; == > [ name: [ hercules ] ,type: [ demigod ] ,age: [ 30 ]] nebula> LOOKUP ON character WHERE character.name == 'hercules' YIELD character.name, character.age, character.type ; ========================================================================= | VertexID | character.name | character.age | character.type | ========================================================================= | 5976696804486077889 | hercules | 30 | demigod | ------------------------------------------------------------------------- Limiting Returned Results \u00b6 Name Gremlin nGQL Constrain the number of rows to return limit() LIMIT Emit the last n-objects tail() ORDER BY \\ DESC LIMIT Skip n-objects skip() LIMIT \\ # Find the first two records gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .limit ( 2 ) ; == >v [ 9 ] == >v [ 10 ] nebula> GO FROM hash ( 'hercules' ) OVER battled | LIMIT 2 ; ======================= | battled._dst | ======================= | 530133512982221454 | ----------------------- | -695163537569412701 | ----------------------- # Find the last record gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .values ( 'name' ) .tail ( 1 ) ; == >cerberus nebula> GO FROM hash ( 'hercules' ) OVER battled YIELD $$ .character.name AS name | ORDER BY name | LIMIT 1 ; ============ | name | ============ | cerberus | ------------ # Skip the first record and return one record gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .values ( 'name' ) .skip ( 1 ) .limit ( 1 ) ; == >hydra nebula> GO FROM hash ( 'hercules' ) OVER battled YIELD $$ .character.name AS name | ORDER BY name | LIMIT 1 ,1 ; ========= | name | ========= | hydra | --------- Finding Path \u00b6 Name Gremlin nGQL All path path() FIND ALL PATH Exclude cycles path simplePath() \\ Only cycles path cyclicPath() \\ Shortest path \\ FIND SHORTEST PATH NOTE: Nebula Graph requires the source vertex and the dest vertex to find path while Gremlin only needs the source vertex. # Find path from vertex pluto to the out adjacent vertices gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'pluto' ) .out () .path () ; == > [ v [ 8 ] ,v [ 12 ]] == > [ v [ 8 ] ,v [ 2 ]] == > [ v [ 8 ] ,v [ 5 ]] == > [ v [ 8 ] ,v [ 11 ]] # Find the shortest path from vertex pluto to vertex jupiter nebula> LOOKUP ON character WHERE character.name == \"pluto\" YIELD character.name AS name | \\ FIND SHORTEST PATH FROM $- .VertexID TO hash ( \"jupiter\" ) OVER * ; ============================================================ | _path_ | ============================================================ | 6761447489613431910 <brother,0> 4863977009196259577 ------------------------------------------------------------ Traversing N Hops \u00b6 Name Gremlin nGQL Loop over a traversal repeat() N STEPS Times the traverser has gone through a loop times() N STEPS Specify when to end the loop until() \\ Specify when to collect data emit() \\ # Find vertex pluto's out adjacent neighbors gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'pluto' ) .repeat ( out ()) .times ( 1 ) ; == >v [ 12 ] == >v [ 2 ] == >v [ 5 ] == >v [ 11 ] nebula> LOOKUP ON character WHERE character.name == \"pluto\" YIELD character.name AS name | \\ GO FROM $- .VertexID OVER * ; ================================================================================================================ | father._dst | brother._dst | lives._dst | mother._dst | pet._dst | battled._dst | ================================================================================================================ | 0 | -5860788569139907963 | 0 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 4863977009196259577 | 0 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 0 | -4331657707562925133 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 0 | 0 | 0 | 4594048193862126013 | 0 | ---------------------------------------------------------------------------------------------------------------- # Find path between vertex hercules and vertex cerberus # Stop traversing when the dest vertex is cerberus gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .repeat ( out ()) .until ( has ( 'name' , 'cerberus' )) .path () ; == > [ v [ 6 ] ,v [ 11 ]] == > [ v [ 6 ] ,v [ 2 ] ,v [ 8 ] ,v [ 11 ]] == > [ v [ 6 ] ,v [ 2 ] ,v [ 5 ] ,v [ 8 ] ,v [ 11 ]] ... nebula> # Coming soon # Find path sourcing from vertex hercules # And the dest vertex type is character gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .repeat ( out ()) .emit ( hasLabel ( 'character' )) .path () ; == > [ v [ 6 ] ,v [ 7 ]] == > [ v [ 6 ] ,v [ 2 ]] == > [ v [ 6 ] ,v [ 9 ]] == > [ v [ 6 ] ,v [ 10 ]] ... nebula> # Coming soon # Find shortest path between pluto and saturn over any edge # And the deepest loop is 3 gremlin> g.V ( 'pluto' ) .repeat ( out () .simplePath ()) .until ( hasId ( 'saturn' ) .and () .loops () .is ( lte ( 3 ))) .hasId ( 'saturn' ) .path () ; nebula> FIND SHORTEST PATH FROM hash ( 'pluto' ) TO hash ( 'saturn' ) OVER * UPTO 3 STEPS ; ================================================================================================= | _path_ | ================================================================================================= | 6761447489613431910 <brother,0> 4863977009196259577 <father,0> -4316810810681305233 ------------------------------------------------------------------------------------------------- Ordering Results \u00b6 Name Gremlin nGQL Order the items increasingly order().by() ORDER BY Order the items decreasingly order().by(decr) ORDER BY DESC Randomize the records order order().by(shuffle) \\ # Find pluto's brother and order by age decreasingly. gremlin> g.V ( pluto ) .out ( 'brother' ) .order () .by ( 'age' , decr ) .valueMap () ; == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] == > [ name: [ neptune ] ,type: [ god ] ,age: [ 4500 ]] nebula> GO FROM hash ( 'pluto' ) OVER brother YIELD $$ .character.name AS Name, $$ .character.age as Age | ORDER BY Age DESC ; ================== | Name | Age | ================== | jupiter | 5000 | ------------------ | neptune | 4500 | ------------------ Group By \u00b6 Name Gremlin nGQL Group by items group().by() GROUP BY Remove repeated items dedup() DISTINCT Group by items and count groupCount() GROUP BY COUNT Note: The GROUP BY function can only be applied in the YIELD clause. # Group vertices by label then count gremlin> g.V () .group () .by ( label ) .by ( count ()) ; == > [ character:9,location:3 ] nebula> # Coming soon # Find vertex jupiter's out adjacency vertices, group by name, then count gremlin> g.V ( jupiter ) .out () .group () .by ( 'name' ) .by ( count ()) ; == > [ sky:1,saturn:1,neptune:1,pluto:1 ] nebula> GO FROM hash ( 'jupiter' ) OVER * YIELD $$ .character.name AS Name, $$ .character.age as Age, $$ .location.name | \\ GROUP BY $- .Name YIELD $- .Name, COUNT ( * ) ; ====================== | $- .Name | COUNT ( * ) | ====================== | | 1 | ---------------------- | pluto | 1 | ---------------------- | saturn | 1 | ---------------------- | neptune | 1 | ---------------------- # Find the distinct dest vertices sourcing from vertex jupiter gremlin> g.V ( jupiter ) .out () .hasLabel ( 'character' ) .dedup () ; == >v [ 1 ] == >v [ 8 ] == >v [ 5 ] nebula> GO FROM hash ( 'jupiter' ) OVER * YIELD DISTINCT $$ .character.name, $$ .character.age, $$ .location.name ; =========================================================== | $$ .character.name | $$ .character.age | $$ .location.name | =========================================================== | pluto | 4000 | | ----------------------------------------------------------- | neptune | 4500 | | ----------------------------------------------------------- | saturn | 10000 | | ----------------------------------------------------------- | | 0 | sky | ----------------------------------------------------------- Where Filter Condition \u00b6 Name Gremlin nGQL Where filter condition where() WHERE Predicates comparison: Name Gremlin nGQL Equal to eq(object) == Not equal to neq(object) != Less than lt(number) < Less than or equal to lte(number) <= Greater than gt(number) > Greater than or equal to gte(number) >= Whether a value is within the array within(objects\u2026\u200b) udf_is_in() gremlin> eq ( 2 ) .test ( 3 ) ; == >false nebula> YIELD 3 == 2 ; ========== | ( 3 == 2 ) | ========== | false | ---------- gremlin> within ( 'a' , 'b' , 'c' ) .test ( 'd' ) ; == >false nebula> YIELD udf_is_in ( 'd' , 'a' , 'b' , 'c' ) ; ====================== | udf_is_in ( d,a,b,c ) | ====================== | false | ---------------------- # Find pluto's co-habitants and exclude himself gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .where ( is ( neq ( pluto ))) .values ( 'name' ) ; == >cerberus nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | GO FROM $- .place OVER lives REVERSELY WHERE \\ $$ .character.name ! = \"pluto\" YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | cerberus | --------------- Logical Operators \u00b6 Name Gremlin nGQL Is is() == Not not() != And and() AND Or or() OR # Find age greater than or equal to 30 gremlin> g.V () .values ( 'age' ) .is ( gte ( 30 )) ; == >10000 == >5000 == >4500 == >30 == >45 == >4000 nebula> LOOKUP ON character WHERE character.age > = 30 YIELD character.age ; ======================================== | VertexID | character.age | ======================================== | -4316810810681305233 | 10000 | ---------------------------------------\u2013 | 4863977009196259577 | 5000 | ---------------------------------------\u2013 | -5860788569139907963 | 4500 | ---------------------------------------\u2013 | 5976696804486077889 | 30 | ---------------------------------------\u2013 | -6780323075177699500 | 45 | ---------------------------------------\u2013 | 6761447489613431910 | 4000 | ---------------------------------------\u2013 # Find character with name pluto and age 4000 gremlin> g.V () .has ( 'name' , 'pluto' ) .and () .has ( 'age' ,4000 ) ; == >v [ 8 ] nebula> LOOKUP ON character WHERE character.name == 'pluto' AND character.age == 4000 ; ======================= | VertexID | ======================= | 6761447489613431910 | ----------------------- # Logical not gremlin> g.V () .has ( 'name' , 'pluto' ) .out ( 'brother' ) .not ( values ( 'name' ) .is ( 'neptune' )) .values ( 'name' ) ; == >jupiter nebula> LOOKUP ON character WHERE character.name == 'pluto' YIELD character.name AS name | \\ GO FROM $- .VertexID OVER brother WHERE $$ .character.name ! = 'neptune' YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | jupiter | --------------------- Statistical Operations \u00b6 Name Gremlin nGQL Sum sum() SUM() Max max() MAX() Min min() MIN() Mean mean() AVG() Nebula Graph statistical operations must be applied with GROUP BY . # Calculate the sum of ages of all characters gremlin> g.V () .hasLabel ( 'character' ) .values ( 'age' ) .sum () ; == >23595 nebula> # Coming soon # Calculate the sum of the out brother edges of all characters gremlin> g.V () .hasLabel ( 'character' ) .map ( outE ( 'brother' ) .count ()) .sum () ; == >6 nebula> # Coming soon # Return the max age of all characters gremlin> g.V () .hasLabel ( 'character' ) .values ( 'age' ) .max () ; == >10000 nebula> # Coming soon Selecting and Filtering Paths \u00b6 # Select the results of steps 1 and 3 from the path as the final result gremlin> g.V ( pluto ) .as ( 'a' ) .out () .as ( 'b' ) .out () .as ( 'c' ) .select ( 'a' , 'c' ) ; == > [ a:v [ 8 ] ,c:v [ 3 ]] == > [ a:v [ 8 ] ,c:v [ 1 ]] ... nebula> # Coming soon # Specify dimensions via by() gremlin> g.V ( pluto ) .as ( 'a' ) .out () .as ( 'b' ) .out () .as ( 'c' ) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:pluto,c:sky ] == > [ a:pluto,c:saturn ] ... nebula> # Coming soon # Selects the specified key value from the map gremlin> g.V () .valueMap () .select ( 'name' ) .dedup () ; == > [ saturn ] == > [ jupiter ] ... nebula> # Coming soon Branches \u00b6 # Traverse all vertices with label 'character' # If name is 'jupiter', return the age property # Else return the name property gremlin> g.V () .hasLabel ( 'character' ) .choose ( values ( 'name' )) .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 == >neptune ... # Lambda gremlin> g.V () .branch { it.get () .value ( 'name' )} .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 ... # Traversal gremlin> g.V () .branch ( values ( 'name' )) .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 # Branch gremlin> g.V () .choose ( has ( 'name' , 'jupiter' ) ,values ( 'age' ) ,values ( 'name' )) ; == >saturn == >5000 # Group based on if then gremlin> g.V () .hasLabel ( \"character\" ) .groupCount () .by ( values ( \"age\" ) .choose ( is ( lt ( 40 )) ,constant ( \"young\" ) , choose ( is ( lt ( 4500 )) , constant ( \"old\" ) , constant ( \"very old\" )))) ; == > [ young:4,old:2,very old:3 ] Similar function is yet to be supported in Nebula Graph . Coalesce \u00b6 The coalesce() step evaluates the provided traversals in order and returns the first traversal that emits at least one element. The optional() step returns the result of the specified traversal if it yields a result else it returns the calling element, i.e. the identity(). The union() step supports the merging of the results of an arbitrary number of traversals. # If type is monster, return type. Else return 'Not a monster'. gremlin> g.V ( pluto ) .coalesce ( has ( 'type' , 'monster' ) .values ( 'type' ) ,constant ( \"Not a monster\" )) ; == >Not a monster # Find the following edges and adjacent vertices of jupiter in order, and stop when finding one # 1. Edge brother out adjacent vertices # 2. Edge father out adjacent vertices # 3. Edge father in adjacent vertices gremlin> g.V ( jupiter ) .coalesce ( outE ( 'brother' ) , outE ( 'father' ) , inE ( 'father' )) .inV () .path () .by ( 'name' ) .by ( label ) ; == > [ jupiter,brother,pluto ] == > [ jupiter,brother,neptune ] # Find pluto's father, if there is not any then return pluto himself gremlin> g.V ( pluto ) .optional ( out ( 'father' )) .valueMap () ; == > [ name: [ pluto ] ,type: [ god ] ,age: [ 4000 ]] # Find pluto's father and brother, union the results then return the paths gremlin> g.V ( pluto ) .union ( out ( 'father' ) ,both ( 'brother' )) .path () ; == > [ v [ 8 ] ,v [ 2 ]] == > [ v [ 8 ] ,v [ 5 ]] Similar function is yet to be supported in Nebula Graph . Aggregating and Unfolding Results \u00b6 # Collect results of the first step into set x # Note: This operation doesn't affect subsequent results gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) ; == >v [ 12 ] == >v [ 2 ] ... # Specify the aggregation dimensions via by () gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) .by ( 'name' ) .cap ( 'x' ) ; == > [ tartarus,jupiter,neptune,cerberus ] # Find pluto's 2 hop out adjacent neighbors # Collect the results in set x # Show the neighbors' name gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) .out () .aggregate ( 'x' ) .cap ( 'x' ) .unfold () .values ( 'name' ) ; == >tartarus == >tartarus ... Similar function is yet to be supported in Nebula Graph . Matching Patterns \u00b6 The match() step provides a more declarative form of graph querying based on the notion of pattern matching. With match(), the user provides a collection of \"traversal fragments,\" called patterns, that have variables defined that must hold true throughout the duration of the match(). # Matching each vertex with the following pattern. If pattern is met, return map<String, Object>, els filter it. # Pattern 1: a is jupiter's son # Pattern 2: b is jupiter # Pattern 3: c is jupiter's brother, whose age is 4000 gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) ; == > [ a:v [ 6 ] ,b:v [ 2 ] ,c:v [ 8 ]] # match() can be applied with select() to select partial results from Map <String, Object> gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:hercules,c:pluto ] # match () can be applied with where () to filter the results gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) .where ( 'a' , neq ( 'c' )) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:hercules,c:pluto ] Random filtering \u00b6 The sample() step accepts an integer value and samples the maximum number of the specified results randomly from the previous traverser. The coin() step can randomly filter out a traverser with the given probability. You give coin a value indicating how biased the toss should be. # Randomly select 2 out edges from all vertices gremlin> g.V () .outE () .sample ( 2 ) ; == >e [ 15 ][ 2 -brother->5 ] == >e [ 18 ][ 5 -brother->2 ] # Pick 3 names randomly from all vertices gremlin> g.V () .values ( 'name' ) .sample ( 3 ) ; == >hercules == >sea == >jupiter # Pick 3 randomly from all characters based on age gremlin> g.V () .hasLabel ( 'character' ) .sample ( 3 ) .by ( 'age' ) ; == >v [ 1 ] == >v [ 2 ] == >v [ 6 ] # Applied with local to do random walk # Starting from pluto, conduct random walk 3 times gremlin> g.V ( pluto ) .repeat ( local ( bothE () .sample ( 1 ) .otherV ())) .times ( 3 ) .path () ; == > [ v [ 8 ] ,e [ 26 ][ 8 -brother->5 ] ,v [ 5 ] ,e [ 18 ][ 5 -brother->2 ] ,v [ 2 ] ,e [ 13 ][ 2 -father->1 ] ,v [ 1 ]] # Filter each vertex with a probability of 0.5 gremlin> g.V () .coin ( 0 .5 ) ; == >v [ 1 ] == >v [ 2 ] ... # Return the name attribute of all vertices labeled location, otherwise return not a location gremlin> g.V () .choose ( hasLabel ( 'location' ) , values ( 'name' ) , constant ( 'not a location' )) ; == >not a location == >not a location == >sky ... Sack \u00b6 A traverser that contains a local data structure is called a \"sack\". The sack() step is used to read and write sacks. Each sack of each traverser is created with withSack() . # Defines a Gremlin sack with a value of one and return values in the sack gremlin> g.withSack ( 1 ) .V () .sack () ; == >1 == >1 ... Barrier \u00b6 The barrier() step turns the lazy traversal pipeline into a bulk-synchronous pipeline. It's useful when everything prior to barrier() needs to be executed before moving onto the steps after the barrier(). # Calculate the Eigenvector Centrality with barrier # Including groupCount and cap, sorted in descending order gremlin> g.V () .repeat ( both () .groupCount ( 'm' )) .times ( 5 ) .cap ( 'm' ) .order ( local ) .by ( values, decr ) ; Local \u00b6 A GraphTraversal operates on a continuous stream of objects. In many situations, it is important to operate on a single element within that stream. To do such object-local traversal computations, local() step exists. # Without local() gremlin> g.V () .hasLabel ( 'character' ) .as ( 'character' ) .properties ( 'age' ) .order () .by ( value,decr ) .limit ( 2 ) .value () .as ( 'age' ) .select ( 'character' , 'age' ) .by ( 'name' ) .by () ; == > [ character:saturn,age:10000 ] == > [ character:jupiter,age:5000 ] # With local() gremlin> g.V () .hasLabel ( 'character' ) .as ( 'character' ) .local ( properties ( 'age' ) .order () .by ( value ) .limit ( 2 )) .value () .as ( 'age' ) .select ( 'character' , 'age' ) .by ( 'name' ) .by () == > [ character:saturn,age:10000 ] == > [ character:jupiter,age:5000 ] == > [ character:neptune,age:4500 ] == > [ character:hercules,age:30 ] ... # Return the property map of monster gremlin> g.V () hasLabel ( 'character' ) .has ( 'type' , 'type' ) .propertyMap () ; == > [ name: [ vp [ name->nemean ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->20 ]]] == > [ name: [ vp [ name->hydra ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->0 ]]] == > [ name: [ vp [ name->cerberus ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->0 ]]] # Find number of monster gremlin> g.V () hasLabel ( 'character' ) .has ( 'type' , 'monster' ) .propertyMap () .count ( local ) ; == >3 == >3 == >3 # Find the max vertices number labeled tha same tag gremlin> g.V () .groupCount () .by ( label ) .select ( values ) .max ( local ) ; == >9 # List the first attribute of all vertices gremlin> g.V () .valueMap () .limit ( local, 1 ) ; == > [ name: [ saturn ]] == > [ name: [ jupiter ]] == > [ name: [ sky ]] ... # Without local gremlin> g.V () .valueMap () .limit ( 1 ) ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] # All vertices as a set, sample 2 from it gremlin> g.V () .fold () .sample ( local,2 ) ; == > [ v [ 8 ] ,v [ 1 ]] Statistics and Analysis \u00b6 Gremlin provides two steps for statistics and analysis of the executed query statements: The explain() step will return a TraversalExplanation. A traversal explanation details how the traversal (prior to explain()) will be compiled given the registered traversal strategies. The profile() step allows developers to profile their traversals to determine statistical information like step runtime, counts, etc.","title":"Gremlin & nGQL"},{"location":"manual-EN/5.appendix/gremlin-ngql/#comparison_between_gremlin_and_ngql","text":"","title":"Comparison Between Gremlin and nGQL"},{"location":"manual-EN/5.appendix/gremlin-ngql/#introduction_to_gremlin","text":"Gremlin is a graph traversal language developed by Apache TinkerPop. It can be either declarative or imperative. Gremlin is Groovy-based, but has many language variants that allow developers to write Gremlin queries natively in many modern programming languages such as Java, JavaScript, Python, Scala, Clojure and Groovy.","title":"Introduction to Gremlin"},{"location":"manual-EN/5.appendix/gremlin-ngql/#introduction_to_ngql","text":"Nebula Graph introduces its own query language, nGQL , which is a declarative, textual query language like SQL, but for graphs. Unlike SQL, nGQL is all about expressing graph patterns. The features of nGQL are as follows: Syntax is close to SQL, but not exactly the same (Easy to learn) Expandable Keyword is case insensitive Support basic graph traverse Support pattern matching Support aggregation Support graph mutation Support distributed transaction (future release) Statement composition, but NO statement embedding (Easy to read)","title":"Introduction to nGQL"},{"location":"manual-EN/5.appendix/gremlin-ngql/#conceptual_comparisons","text":"Name Gremlin nGQL vertex, node vertex vertex edge, relationship edge edge vertex type label tag edge type label edge type vertex id vid vid edge id eid not support In Gremlin and nGQL, vertices and edges are identified with unique identifiers. In Nebula Graph , you can either specify identifiers or generate automatically with the hash or uuid function.","title":"Conceptual Comparisons"},{"location":"manual-EN/5.appendix/gremlin-ngql/#basic_graph_operations","text":"Name Gremlin nGQL Create a new graph g = TinkerGraph.open().traversal() CREATE SPACE gods Show vertices' types g.V().label() SHOW TAGS Insert a vertex with a specified type g.addV(String vertexLabel).property() INSERT VERTEX (prop_name_list) VALUES \\ :(prop_value_list) Insert an edge with specified edge type g.addE(String edgeLabel).from(v1).to(v2).property() INSERT EDGE ( ) VALUES -> : ( ) Delete a vertex g.V(\\ ).drop() DELETE VERTEX \\ Delete an edge g.E(\\ ).outE(\\ ).where(otherV().is(\\ ))drop() DELETE EDGE \\ -> \\ Update a vertex property g.V(\\ ).property() UPDATE VERTEX \\ SET Fetch vertices with ID g.V(\\ ) FETCH PROP ON \\ Fetch edges with ID g.E( >> ) FETCH PROP ON -> Query a vertex along specified edge type g.V(\\ ).outE( \\ ) GO FROM \\ OVER \\ Query a vertex along specified edge type reversely g.V(\\ ).in( \\ ) GO FROM \\ OVER \\ REVERSELY Query N hops along a specified edge g.V(\\ ).repeat(out(\\ )).times(N) GO N STEPS FROM \\ OVER \\ Find path between two vertices g.V(\\ ).repeat(out()).until(\\ ).path() FIND ALL PATH FROM \\ TO \\ OVER *","title":"Basic Graph Operations"},{"location":"manual-EN/5.appendix/gremlin-ngql/#example_queries","text":"The examples in this section make extensive use of the toy graph distributed with Janus Graph called The Graphs of Gods . This graph is diagrammed below. The abstract data model is known as a Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Insert data # insert vertex nebula> INSERT VERTEX character ( name,age, type ) VALUES hash ( \"saturn\" ) : ( \"saturn\" , 10000 , \"titan\" ) , hash ( \"jupiter\" ) : ( \"jupiter\" , 5000 , \"god\" ) ; gremlin> saturn = g.addV ( \"character\" ) .property ( T.id, 1 ) .property ( 'name' , 'saturn' ) .property ( 'age' , 10000 ) .property ( 'type' , 'titan' ) .next () ; == >v [ 1 ] gremlin> jupiter = g.addV ( \"character\" ) .property ( T.id, 2 ) .property ( 'name' , 'jupiter' ) .property ( 'age' , 5000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 2 ] gremlin> prometheus = g.addV ( \"character\" ) .property ( T.id, 31 ) .property ( 'name' , 'prometheus' ) .property ( 'age' , 1000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 31 ] gremlin> jesus = g.addV ( \"character\" ) property ( T.id, 32 ) .property ( 'name' , 'jesus' ) .property ( 'age' , 5000 ) .property ( 'type' , 'god' ) .next () ; == >v [ 32 ] # insert edge nebula> INSERT EDGE father () VALUES hash ( \"jupiter\" ) ->hash ( \"saturn\" ) : () ; gremlin> g.addE ( \"father\" ) .from ( jupiter ) .to ( saturn ) .property ( T.id, 13 ) ; == >e [ 13 ][ 2 -father->1 ] Delete vertex nebula> DELETE VERTEX hash ( \"prometheus\" ) ; gremlin> g.V ( prometheus ) .drop () ; Update vertex nebula> UPDATE VERTEX hash ( \"jesus\" ) SET character.type = 'titan' ; gremlin> g.V ( jesus ) .property ( 'age' , 6000 ) ; Fetch data nebula> FETCH PROP ON character hash ( \"saturn\" ) ; ================================================== | character.name | character.age | character.type | ================================================== | saturn | 10000 | titan | -------------------------------------------------- gremlin> g.V ( saturn ) .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] Find the name of hercules's grandfather nebula> LOOKUP ON character WHERE character.name == 'hercules' | \\ -> GO 2 STEPS FROM $- .VertexID OVER father YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | saturn | --------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .out ( 'father' ) .out ( 'father' ) .values ( 'name' ) ; == >saturn Find the name of hercules's father nebula> LOOKUP ON character WHERE character.name == 'hercules' | \\ -> GO FROM $- .VertexID OVER father YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | jupiter | --------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .out ( 'father' ) .values ( 'name' ) ; == >jupiter Find the characters with age > 100 nebula> LOOKUP ON character WHERE character.age > 100 YIELD character.name ; ========================================= | VertexID | character.name | ========================================= | 6761447489613431910 | pluto | ----------------------------------------- | -5860788569139907963 | neptune | ----------------------------------------- | 4863977009196259577 | jupiter | ----------------------------------------- | -4316810810681305233 | saturn | ----------------------------------------- gremlin> g.V () .hasLabel ( 'character' ) .has ( 'age' ,gt ( 100 )) .values ( 'name' ) ; == >saturn == >jupiter == >neptune == >pluto Find who are pluto's cohabitants nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | \\ GO FROM $- .place OVER lives REVERSELY YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | pluto | --------------- | cerberus | --------------- gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .values ( 'name' ) ; == >pluto == >cerberus pluto can't be his own cohabitant nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | GO FROM $- .place OVER lives REVERSELY WHERE \\ $$ .character.name ! = \"pluto\" YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | cerberus | --------------- gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .where ( is ( neq ( pluto ))) .values ( 'name' ) ; == >cerberus Pluto's Brothers # where do pluto's brothers live? nebula> GO FROM hash ( \"pluto\" ) OVER brother YIELD brother._dst AS brother | \\ GO FROM $- .brother OVER lives YIELD $$ .location.name ; ==================== | $$ .location.name | ==================== | sky | -------------------- | sea | -------------------- gremlin> g.V ( pluto ) .out ( 'brother' ) .out ( 'lives' ) .values ( 'name' ) ; == >sky == >sea # which brother lives in which place? nebula> GO FROM hash ( \"pluto\" ) OVER brother YIELD brother._dst AS god | \\ GO FROM $- .god OVER lives YIELD $^.character.name AS Brother, $$ .location.name AS Habitations ; ========================= | Brother | Habitations | ========================= | jupiter | sky | ------------------------- | neptune | sea | ------------------------- gremlin> g.V ( pluto ) .out ( 'brother' ) .as ( 'god' ) .out ( 'lives' ) .as ( 'place' ) .select ( 'god' , 'place' ) .by ( 'name' ) ; == > [ god:jupiter, place:sky ] == > [ god:neptune, place:sea ]","title":"Example Queries"},{"location":"manual-EN/5.appendix/gremlin-ngql/#advance_queries","text":"","title":"Advance Queries"},{"location":"manual-EN/5.appendix/gremlin-ngql/#graph_exploration","text":"# Gremlin version gremlin> Gremlin.version () ; == >3.3.5 # Return all the vertices gremlin> g.V () ; == >v [ 1 ] == >v [ 2 ] ... nebula> # Coming soon # Count all the vertices gremlin> g.V () .count () ; == >12 nebula> # Coming soon # Count the vertices and edges by label gremlin> g.V () .groupCount () .by ( label ) ; == > [ character:9,location:3 ] gremlin> g.E () .groupCount () .by ( label ) ; == > [ mother:1,lives:5,father:2,brother:6,battled:3,pet:1 ] nebula> # Coming soon # Return all edges gremlin> g.E () ; == >e [ 13 ][ 2 -father->1 ] == >e [ 14 ][ 2 -lives->3 ] ... nebula> # Coming soon # Return vertices labels gremlin> g.V () .label () .dedup () ; == >character == >location nebula> SHOW TAGS ; ================== | ID | Name | ================== | 15 | character | ------------------ | 16 | location | ------------------ # Return edge types gremlin> g.E () .label () .dedup () ; == >father == >lives ...nebula> SHOW EDGES ; ================ | ID | Name | ================ | 17 | father | ---------------- | 18 | brother | ---------------- ... # Return all vertices properties gremlin> g.V () .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] ... nebula> # Coming soon # Return properties of vertices labeled character gremlin> g.V () .hasLabel ( 'character' ) .valueMap () ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] ...","title":"Graph Exploration"},{"location":"manual-EN/5.appendix/gremlin-ngql/#traversing_edges","text":"Name Gremlin nGQL Out adjacent vertices to the vertex out(\\ ) GO FROM \\ OVER \\ In adjacent vertices to the vertex in(\\ ) GO FROM \\ OVER \\ REVERSELY Both adjacent vertices of the vertex both(\\ ) GO FROM \\ OVER \\ BIDIRECT # Find the out adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .out ( 'brother' ) ; == >v [ 8 ] == >v [ 5 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother ; ======================== | brother._dst | ======================== | 6761447489613431910 | ------------------------ | -5860788569139907963 | ------------------------ # Find the in adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .in ( 'brother' ) ; == >v [ 5 ] == >v [ 8 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother REVERSELY ; ======================= | brother._dst | ======================= | 4863977009196259577 | ----------------------- | 4863977009196259577 | ----------------------- # Find the both adjacent vertices of a vertex along an edge gremlin> g.V ( jupiter ) .both ( 'brother' ) ; == >v [ 8 ] == >v [ 5 ] == >v [ 5 ] == >v [ 8 ] nebula> GO FROM hash ( \"jupiter\" ) OVER brother BIDIRECT ; ======================= | brother._dst | ======================= | 6761447489613431910 | ------------------------ | -5860788569139907963 | | 4863977009196259577 | ----------------------- | 4863977009196259577 | ----------------------- # Two hops out traverse gremlin> g.V ( hercules ) .out ( 'father' ) .out ( 'lives' ) ; == >v [ 3 ] nebula> GO FROM hash ( \"hercules\" ) OVER father YIELD father._dst AS id | \\ GO FROM $- .id OVER lives ; ======================== | lives._dst | ======================== | -1121386748834253737 | ------------------------","title":"Traversing Edges"},{"location":"manual-EN/5.appendix/gremlin-ngql/#has_filter_condition","text":"Name Gremlin nGQL Filter vertex via identifier hasId(\\ ) FETCH PROP ON \\ Filter vertex or edge via label, key and value has(\\ , \\ , \\ ) LOOKUP \\ | \\ WHERE \\ # Filter vertex with ID saturn gremlin> g.V () .hasId ( saturn ) ; == >v [ 1 ] nebula> FETCH PROP ON * hash ( \"saturn\" ) ; ========================================================================== | VertexID | character.name | character.age | character.type | ========================================================================== | -4316810810681305233 | saturn | 10000 | titan | -------------------------------------------------------------------------- # Find for vertices with tag \"character\" and \"name\" attribute value \"hercules\" gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .valueMap () ; == > [ name: [ hercules ] ,type: [ demigod ] ,age: [ 30 ]] nebula> LOOKUP ON character WHERE character.name == 'hercules' YIELD character.name, character.age, character.type ; ========================================================================= | VertexID | character.name | character.age | character.type | ========================================================================= | 5976696804486077889 | hercules | 30 | demigod | -------------------------------------------------------------------------","title":"Has Filter Condition"},{"location":"manual-EN/5.appendix/gremlin-ngql/#limiting_returned_results","text":"Name Gremlin nGQL Constrain the number of rows to return limit() LIMIT Emit the last n-objects tail() ORDER BY \\ DESC LIMIT Skip n-objects skip() LIMIT \\ # Find the first two records gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .limit ( 2 ) ; == >v [ 9 ] == >v [ 10 ] nebula> GO FROM hash ( 'hercules' ) OVER battled | LIMIT 2 ; ======================= | battled._dst | ======================= | 530133512982221454 | ----------------------- | -695163537569412701 | ----------------------- # Find the last record gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .values ( 'name' ) .tail ( 1 ) ; == >cerberus nebula> GO FROM hash ( 'hercules' ) OVER battled YIELD $$ .character.name AS name | ORDER BY name | LIMIT 1 ; ============ | name | ============ | cerberus | ------------ # Skip the first record and return one record gremlin> g.V () .has ( 'character' , 'name' , 'hercules' ) .out ( 'battled' ) .values ( 'name' ) .skip ( 1 ) .limit ( 1 ) ; == >hydra nebula> GO FROM hash ( 'hercules' ) OVER battled YIELD $$ .character.name AS name | ORDER BY name | LIMIT 1 ,1 ; ========= | name | ========= | hydra | ---------","title":"Limiting Returned Results"},{"location":"manual-EN/5.appendix/gremlin-ngql/#finding_path","text":"Name Gremlin nGQL All path path() FIND ALL PATH Exclude cycles path simplePath() \\ Only cycles path cyclicPath() \\ Shortest path \\ FIND SHORTEST PATH NOTE: Nebula Graph requires the source vertex and the dest vertex to find path while Gremlin only needs the source vertex. # Find path from vertex pluto to the out adjacent vertices gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'pluto' ) .out () .path () ; == > [ v [ 8 ] ,v [ 12 ]] == > [ v [ 8 ] ,v [ 2 ]] == > [ v [ 8 ] ,v [ 5 ]] == > [ v [ 8 ] ,v [ 11 ]] # Find the shortest path from vertex pluto to vertex jupiter nebula> LOOKUP ON character WHERE character.name == \"pluto\" YIELD character.name AS name | \\ FIND SHORTEST PATH FROM $- .VertexID TO hash ( \"jupiter\" ) OVER * ; ============================================================ | _path_ | ============================================================ | 6761447489613431910 <brother,0> 4863977009196259577 ------------------------------------------------------------","title":"Finding Path"},{"location":"manual-EN/5.appendix/gremlin-ngql/#traversing_n_hops","text":"Name Gremlin nGQL Loop over a traversal repeat() N STEPS Times the traverser has gone through a loop times() N STEPS Specify when to end the loop until() \\ Specify when to collect data emit() \\ # Find vertex pluto's out adjacent neighbors gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'pluto' ) .repeat ( out ()) .times ( 1 ) ; == >v [ 12 ] == >v [ 2 ] == >v [ 5 ] == >v [ 11 ] nebula> LOOKUP ON character WHERE character.name == \"pluto\" YIELD character.name AS name | \\ GO FROM $- .VertexID OVER * ; ================================================================================================================ | father._dst | brother._dst | lives._dst | mother._dst | pet._dst | battled._dst | ================================================================================================================ | 0 | -5860788569139907963 | 0 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 4863977009196259577 | 0 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 0 | -4331657707562925133 | 0 | 0 | 0 | ---------------------------------------------------------------------------------------------------------------- | 0 | 0 | 0 | 0 | 4594048193862126013 | 0 | ---------------------------------------------------------------------------------------------------------------- # Find path between vertex hercules and vertex cerberus # Stop traversing when the dest vertex is cerberus gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .repeat ( out ()) .until ( has ( 'name' , 'cerberus' )) .path () ; == > [ v [ 6 ] ,v [ 11 ]] == > [ v [ 6 ] ,v [ 2 ] ,v [ 8 ] ,v [ 11 ]] == > [ v [ 6 ] ,v [ 2 ] ,v [ 5 ] ,v [ 8 ] ,v [ 11 ]] ... nebula> # Coming soon # Find path sourcing from vertex hercules # And the dest vertex type is character gremlin> g.V () .hasLabel ( 'character' ) .has ( 'name' , 'hercules' ) .repeat ( out ()) .emit ( hasLabel ( 'character' )) .path () ; == > [ v [ 6 ] ,v [ 7 ]] == > [ v [ 6 ] ,v [ 2 ]] == > [ v [ 6 ] ,v [ 9 ]] == > [ v [ 6 ] ,v [ 10 ]] ... nebula> # Coming soon # Find shortest path between pluto and saturn over any edge # And the deepest loop is 3 gremlin> g.V ( 'pluto' ) .repeat ( out () .simplePath ()) .until ( hasId ( 'saturn' ) .and () .loops () .is ( lte ( 3 ))) .hasId ( 'saturn' ) .path () ; nebula> FIND SHORTEST PATH FROM hash ( 'pluto' ) TO hash ( 'saturn' ) OVER * UPTO 3 STEPS ; ================================================================================================= | _path_ | ================================================================================================= | 6761447489613431910 <brother,0> 4863977009196259577 <father,0> -4316810810681305233 -------------------------------------------------------------------------------------------------","title":"Traversing N Hops"},{"location":"manual-EN/5.appendix/gremlin-ngql/#ordering_results","text":"Name Gremlin nGQL Order the items increasingly order().by() ORDER BY Order the items decreasingly order().by(decr) ORDER BY DESC Randomize the records order order().by(shuffle) \\ # Find pluto's brother and order by age decreasingly. gremlin> g.V ( pluto ) .out ( 'brother' ) .order () .by ( 'age' , decr ) .valueMap () ; == > [ name: [ jupiter ] ,type: [ god ] ,age: [ 5000 ]] == > [ name: [ neptune ] ,type: [ god ] ,age: [ 4500 ]] nebula> GO FROM hash ( 'pluto' ) OVER brother YIELD $$ .character.name AS Name, $$ .character.age as Age | ORDER BY Age DESC ; ================== | Name | Age | ================== | jupiter | 5000 | ------------------ | neptune | 4500 | ------------------","title":"Ordering Results"},{"location":"manual-EN/5.appendix/gremlin-ngql/#group_by","text":"Name Gremlin nGQL Group by items group().by() GROUP BY Remove repeated items dedup() DISTINCT Group by items and count groupCount() GROUP BY COUNT Note: The GROUP BY function can only be applied in the YIELD clause. # Group vertices by label then count gremlin> g.V () .group () .by ( label ) .by ( count ()) ; == > [ character:9,location:3 ] nebula> # Coming soon # Find vertex jupiter's out adjacency vertices, group by name, then count gremlin> g.V ( jupiter ) .out () .group () .by ( 'name' ) .by ( count ()) ; == > [ sky:1,saturn:1,neptune:1,pluto:1 ] nebula> GO FROM hash ( 'jupiter' ) OVER * YIELD $$ .character.name AS Name, $$ .character.age as Age, $$ .location.name | \\ GROUP BY $- .Name YIELD $- .Name, COUNT ( * ) ; ====================== | $- .Name | COUNT ( * ) | ====================== | | 1 | ---------------------- | pluto | 1 | ---------------------- | saturn | 1 | ---------------------- | neptune | 1 | ---------------------- # Find the distinct dest vertices sourcing from vertex jupiter gremlin> g.V ( jupiter ) .out () .hasLabel ( 'character' ) .dedup () ; == >v [ 1 ] == >v [ 8 ] == >v [ 5 ] nebula> GO FROM hash ( 'jupiter' ) OVER * YIELD DISTINCT $$ .character.name, $$ .character.age, $$ .location.name ; =========================================================== | $$ .character.name | $$ .character.age | $$ .location.name | =========================================================== | pluto | 4000 | | ----------------------------------------------------------- | neptune | 4500 | | ----------------------------------------------------------- | saturn | 10000 | | ----------------------------------------------------------- | | 0 | sky | -----------------------------------------------------------","title":"Group By"},{"location":"manual-EN/5.appendix/gremlin-ngql/#where_filter_condition","text":"Name Gremlin nGQL Where filter condition where() WHERE Predicates comparison: Name Gremlin nGQL Equal to eq(object) == Not equal to neq(object) != Less than lt(number) < Less than or equal to lte(number) <= Greater than gt(number) > Greater than or equal to gte(number) >= Whether a value is within the array within(objects\u2026\u200b) udf_is_in() gremlin> eq ( 2 ) .test ( 3 ) ; == >false nebula> YIELD 3 == 2 ; ========== | ( 3 == 2 ) | ========== | false | ---------- gremlin> within ( 'a' , 'b' , 'c' ) .test ( 'd' ) ; == >false nebula> YIELD udf_is_in ( 'd' , 'a' , 'b' , 'c' ) ; ====================== | udf_is_in ( d,a,b,c ) | ====================== | false | ---------------------- # Find pluto's co-habitants and exclude himself gremlin> g.V ( pluto ) .out ( 'lives' ) .in ( 'lives' ) .where ( is ( neq ( pluto ))) .values ( 'name' ) ; == >cerberus nebula> GO FROM hash ( \"pluto\" ) OVER lives YIELD lives._dst AS place | GO FROM $- .place OVER lives REVERSELY WHERE \\ $$ .character.name ! = \"pluto\" YIELD $$ .character.name AS cohabitants ; =============== | cohabitants | =============== | cerberus | ---------------","title":"Where Filter Condition"},{"location":"manual-EN/5.appendix/gremlin-ngql/#logical_operators","text":"Name Gremlin nGQL Is is() == Not not() != And and() AND Or or() OR # Find age greater than or equal to 30 gremlin> g.V () .values ( 'age' ) .is ( gte ( 30 )) ; == >10000 == >5000 == >4500 == >30 == >45 == >4000 nebula> LOOKUP ON character WHERE character.age > = 30 YIELD character.age ; ======================================== | VertexID | character.age | ======================================== | -4316810810681305233 | 10000 | ---------------------------------------\u2013 | 4863977009196259577 | 5000 | ---------------------------------------\u2013 | -5860788569139907963 | 4500 | ---------------------------------------\u2013 | 5976696804486077889 | 30 | ---------------------------------------\u2013 | -6780323075177699500 | 45 | ---------------------------------------\u2013 | 6761447489613431910 | 4000 | ---------------------------------------\u2013 # Find character with name pluto and age 4000 gremlin> g.V () .has ( 'name' , 'pluto' ) .and () .has ( 'age' ,4000 ) ; == >v [ 8 ] nebula> LOOKUP ON character WHERE character.name == 'pluto' AND character.age == 4000 ; ======================= | VertexID | ======================= | 6761447489613431910 | ----------------------- # Logical not gremlin> g.V () .has ( 'name' , 'pluto' ) .out ( 'brother' ) .not ( values ( 'name' ) .is ( 'neptune' )) .values ( 'name' ) ; == >jupiter nebula> LOOKUP ON character WHERE character.name == 'pluto' YIELD character.name AS name | \\ GO FROM $- .VertexID OVER brother WHERE $$ .character.name ! = 'neptune' YIELD $$ .character.name ; ===================== | $$ .character.name | ===================== | jupiter | ---------------------","title":"Logical Operators"},{"location":"manual-EN/5.appendix/gremlin-ngql/#statistical_operations","text":"Name Gremlin nGQL Sum sum() SUM() Max max() MAX() Min min() MIN() Mean mean() AVG() Nebula Graph statistical operations must be applied with GROUP BY . # Calculate the sum of ages of all characters gremlin> g.V () .hasLabel ( 'character' ) .values ( 'age' ) .sum () ; == >23595 nebula> # Coming soon # Calculate the sum of the out brother edges of all characters gremlin> g.V () .hasLabel ( 'character' ) .map ( outE ( 'brother' ) .count ()) .sum () ; == >6 nebula> # Coming soon # Return the max age of all characters gremlin> g.V () .hasLabel ( 'character' ) .values ( 'age' ) .max () ; == >10000 nebula> # Coming soon","title":"Statistical Operations"},{"location":"manual-EN/5.appendix/gremlin-ngql/#selecting_and_filtering_paths","text":"# Select the results of steps 1 and 3 from the path as the final result gremlin> g.V ( pluto ) .as ( 'a' ) .out () .as ( 'b' ) .out () .as ( 'c' ) .select ( 'a' , 'c' ) ; == > [ a:v [ 8 ] ,c:v [ 3 ]] == > [ a:v [ 8 ] ,c:v [ 1 ]] ... nebula> # Coming soon # Specify dimensions via by() gremlin> g.V ( pluto ) .as ( 'a' ) .out () .as ( 'b' ) .out () .as ( 'c' ) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:pluto,c:sky ] == > [ a:pluto,c:saturn ] ... nebula> # Coming soon # Selects the specified key value from the map gremlin> g.V () .valueMap () .select ( 'name' ) .dedup () ; == > [ saturn ] == > [ jupiter ] ... nebula> # Coming soon","title":"Selecting and Filtering Paths"},{"location":"manual-EN/5.appendix/gremlin-ngql/#branches","text":"# Traverse all vertices with label 'character' # If name is 'jupiter', return the age property # Else return the name property gremlin> g.V () .hasLabel ( 'character' ) .choose ( values ( 'name' )) .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 == >neptune ... # Lambda gremlin> g.V () .branch { it.get () .value ( 'name' )} .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 ... # Traversal gremlin> g.V () .branch ( values ( 'name' )) .option ( 'jupiter' , values ( 'age' )) .option ( none, values ( 'name' )) ; == >saturn == >5000 # Branch gremlin> g.V () .choose ( has ( 'name' , 'jupiter' ) ,values ( 'age' ) ,values ( 'name' )) ; == >saturn == >5000 # Group based on if then gremlin> g.V () .hasLabel ( \"character\" ) .groupCount () .by ( values ( \"age\" ) .choose ( is ( lt ( 40 )) ,constant ( \"young\" ) , choose ( is ( lt ( 4500 )) , constant ( \"old\" ) , constant ( \"very old\" )))) ; == > [ young:4,old:2,very old:3 ] Similar function is yet to be supported in Nebula Graph .","title":"Branches"},{"location":"manual-EN/5.appendix/gremlin-ngql/#coalesce","text":"The coalesce() step evaluates the provided traversals in order and returns the first traversal that emits at least one element. The optional() step returns the result of the specified traversal if it yields a result else it returns the calling element, i.e. the identity(). The union() step supports the merging of the results of an arbitrary number of traversals. # If type is monster, return type. Else return 'Not a monster'. gremlin> g.V ( pluto ) .coalesce ( has ( 'type' , 'monster' ) .values ( 'type' ) ,constant ( \"Not a monster\" )) ; == >Not a monster # Find the following edges and adjacent vertices of jupiter in order, and stop when finding one # 1. Edge brother out adjacent vertices # 2. Edge father out adjacent vertices # 3. Edge father in adjacent vertices gremlin> g.V ( jupiter ) .coalesce ( outE ( 'brother' ) , outE ( 'father' ) , inE ( 'father' )) .inV () .path () .by ( 'name' ) .by ( label ) ; == > [ jupiter,brother,pluto ] == > [ jupiter,brother,neptune ] # Find pluto's father, if there is not any then return pluto himself gremlin> g.V ( pluto ) .optional ( out ( 'father' )) .valueMap () ; == > [ name: [ pluto ] ,type: [ god ] ,age: [ 4000 ]] # Find pluto's father and brother, union the results then return the paths gremlin> g.V ( pluto ) .union ( out ( 'father' ) ,both ( 'brother' )) .path () ; == > [ v [ 8 ] ,v [ 2 ]] == > [ v [ 8 ] ,v [ 5 ]] Similar function is yet to be supported in Nebula Graph .","title":"Coalesce"},{"location":"manual-EN/5.appendix/gremlin-ngql/#aggregating_and_unfolding_results","text":"# Collect results of the first step into set x # Note: This operation doesn't affect subsequent results gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) ; == >v [ 12 ] == >v [ 2 ] ... # Specify the aggregation dimensions via by () gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) .by ( 'name' ) .cap ( 'x' ) ; == > [ tartarus,jupiter,neptune,cerberus ] # Find pluto's 2 hop out adjacent neighbors # Collect the results in set x # Show the neighbors' name gremlin> g.V ( pluto ) .out () .aggregate ( 'x' ) .out () .aggregate ( 'x' ) .cap ( 'x' ) .unfold () .values ( 'name' ) ; == >tartarus == >tartarus ... Similar function is yet to be supported in Nebula Graph .","title":"Aggregating and Unfolding Results"},{"location":"manual-EN/5.appendix/gremlin-ngql/#matching_patterns","text":"The match() step provides a more declarative form of graph querying based on the notion of pattern matching. With match(), the user provides a collection of \"traversal fragments,\" called patterns, that have variables defined that must hold true throughout the duration of the match(). # Matching each vertex with the following pattern. If pattern is met, return map<String, Object>, els filter it. # Pattern 1: a is jupiter's son # Pattern 2: b is jupiter # Pattern 3: c is jupiter's brother, whose age is 4000 gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) ; == > [ a:v [ 6 ] ,b:v [ 2 ] ,c:v [ 8 ]] # match() can be applied with select() to select partial results from Map <String, Object> gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:hercules,c:pluto ] # match () can be applied with where () to filter the results gremlin> g.V () .match ( __.as ( 'a' ) .out ( 'father' ) .has ( 'name' , 'jupiter' ) .as ( 'b' ) , __.as ( 'b' ) .in ( 'brother' ) .has ( 'age' , 4000 ) .as ( 'c' )) .where ( 'a' , neq ( 'c' )) .select ( 'a' , 'c' ) .by ( 'name' ) ; == > [ a:hercules,c:pluto ]","title":"Matching Patterns"},{"location":"manual-EN/5.appendix/gremlin-ngql/#random_filtering","text":"The sample() step accepts an integer value and samples the maximum number of the specified results randomly from the previous traverser. The coin() step can randomly filter out a traverser with the given probability. You give coin a value indicating how biased the toss should be. # Randomly select 2 out edges from all vertices gremlin> g.V () .outE () .sample ( 2 ) ; == >e [ 15 ][ 2 -brother->5 ] == >e [ 18 ][ 5 -brother->2 ] # Pick 3 names randomly from all vertices gremlin> g.V () .values ( 'name' ) .sample ( 3 ) ; == >hercules == >sea == >jupiter # Pick 3 randomly from all characters based on age gremlin> g.V () .hasLabel ( 'character' ) .sample ( 3 ) .by ( 'age' ) ; == >v [ 1 ] == >v [ 2 ] == >v [ 6 ] # Applied with local to do random walk # Starting from pluto, conduct random walk 3 times gremlin> g.V ( pluto ) .repeat ( local ( bothE () .sample ( 1 ) .otherV ())) .times ( 3 ) .path () ; == > [ v [ 8 ] ,e [ 26 ][ 8 -brother->5 ] ,v [ 5 ] ,e [ 18 ][ 5 -brother->2 ] ,v [ 2 ] ,e [ 13 ][ 2 -father->1 ] ,v [ 1 ]] # Filter each vertex with a probability of 0.5 gremlin> g.V () .coin ( 0 .5 ) ; == >v [ 1 ] == >v [ 2 ] ... # Return the name attribute of all vertices labeled location, otherwise return not a location gremlin> g.V () .choose ( hasLabel ( 'location' ) , values ( 'name' ) , constant ( 'not a location' )) ; == >not a location == >not a location == >sky ...","title":"Random filtering"},{"location":"manual-EN/5.appendix/gremlin-ngql/#sack","text":"A traverser that contains a local data structure is called a \"sack\". The sack() step is used to read and write sacks. Each sack of each traverser is created with withSack() . # Defines a Gremlin sack with a value of one and return values in the sack gremlin> g.withSack ( 1 ) .V () .sack () ; == >1 == >1 ...","title":"Sack"},{"location":"manual-EN/5.appendix/gremlin-ngql/#barrier","text":"The barrier() step turns the lazy traversal pipeline into a bulk-synchronous pipeline. It's useful when everything prior to barrier() needs to be executed before moving onto the steps after the barrier(). # Calculate the Eigenvector Centrality with barrier # Including groupCount and cap, sorted in descending order gremlin> g.V () .repeat ( both () .groupCount ( 'm' )) .times ( 5 ) .cap ( 'm' ) .order ( local ) .by ( values, decr ) ;","title":"Barrier"},{"location":"manual-EN/5.appendix/gremlin-ngql/#local","text":"A GraphTraversal operates on a continuous stream of objects. In many situations, it is important to operate on a single element within that stream. To do such object-local traversal computations, local() step exists. # Without local() gremlin> g.V () .hasLabel ( 'character' ) .as ( 'character' ) .properties ( 'age' ) .order () .by ( value,decr ) .limit ( 2 ) .value () .as ( 'age' ) .select ( 'character' , 'age' ) .by ( 'name' ) .by () ; == > [ character:saturn,age:10000 ] == > [ character:jupiter,age:5000 ] # With local() gremlin> g.V () .hasLabel ( 'character' ) .as ( 'character' ) .local ( properties ( 'age' ) .order () .by ( value ) .limit ( 2 )) .value () .as ( 'age' ) .select ( 'character' , 'age' ) .by ( 'name' ) .by () == > [ character:saturn,age:10000 ] == > [ character:jupiter,age:5000 ] == > [ character:neptune,age:4500 ] == > [ character:hercules,age:30 ] ... # Return the property map of monster gremlin> g.V () hasLabel ( 'character' ) .has ( 'type' , 'type' ) .propertyMap () ; == > [ name: [ vp [ name->nemean ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->20 ]]] == > [ name: [ vp [ name->hydra ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->0 ]]] == > [ name: [ vp [ name->cerberus ]] ,type: [ vp [ type->monster ]] ,age: [ vp [ age->0 ]]] # Find number of monster gremlin> g.V () hasLabel ( 'character' ) .has ( 'type' , 'monster' ) .propertyMap () .count ( local ) ; == >3 == >3 == >3 # Find the max vertices number labeled tha same tag gremlin> g.V () .groupCount () .by ( label ) .select ( values ) .max ( local ) ; == >9 # List the first attribute of all vertices gremlin> g.V () .valueMap () .limit ( local, 1 ) ; == > [ name: [ saturn ]] == > [ name: [ jupiter ]] == > [ name: [ sky ]] ... # Without local gremlin> g.V () .valueMap () .limit ( 1 ) ; == > [ name: [ saturn ] ,type: [ titan ] ,age: [ 10000 ]] # All vertices as a set, sample 2 from it gremlin> g.V () .fold () .sample ( local,2 ) ; == > [ v [ 8 ] ,v [ 1 ]]","title":"Local"},{"location":"manual-EN/5.appendix/gremlin-ngql/#statistics_and_analysis","text":"Gremlin provides two steps for statistics and analysis of the executed query statements: The explain() step will return a TraversalExplanation. A traversal explanation details how the traversal (prior to explain()) will be compiled given the registered traversal strategies. The profile() step allows developers to profile their traversals to determine statistical information like step runtime, counts, etc.","title":"Statistics and Analysis"},{"location":"manual-EN/5.appendix/upgrade-guide/","text":"Nebula Graph Upgrading Guide \u00b6 This document describes how to upgrade Nebula Graph. Find the corresponding upgrade guide to your version below. Upgrading From Nebula Graph RC3 to RC4 \u00b6 Stop all the Nebula Graph services Run scripts/nebula.service stop all command on each machine Then run scripts/nebula.service status all command to confirm all services have exited successfully Install the new rpm package on each machine Run https://github.com/vesoft-inc/nebula/releases/tag/v1.0.0-rc4 command to download the package Then run rpm -Uvh nebula-1.0.0-rc4.el7-5.x86_64.rpm command to install Nebula Graph Start Nebula Graph services Run scripts/nebula.service start all command on each machine Then run scripts/nebula.service status all command to confirm all services have started successfully Or run nebula> SHOW HOSTS to check services status in Nebula console Reimport your data","title":"Upgrading Nebula Graph"},{"location":"manual-EN/5.appendix/upgrade-guide/#nebula_graph_upgrading_guide","text":"This document describes how to upgrade Nebula Graph. Find the corresponding upgrade guide to your version below.","title":"Nebula Graph Upgrading Guide"},{"location":"manual-EN/5.appendix/upgrade-guide/#upgrading_from_nebula_graph_rc3_to_rc4","text":"Stop all the Nebula Graph services Run scripts/nebula.service stop all command on each machine Then run scripts/nebula.service status all command to confirm all services have exited successfully Install the new rpm package on each machine Run https://github.com/vesoft-inc/nebula/releases/tag/v1.0.0-rc4 command to download the package Then run rpm -Uvh nebula-1.0.0-rc4.el7-5.x86_64.rpm command to install Nebula Graph Start Nebula Graph services Run scripts/nebula.service start all command on each machine Then run scripts/nebula.service status all command to confirm all services have started successfully Or run nebula> SHOW HOSTS to check services status in Nebula console Reimport your data","title":"Upgrading From Nebula Graph RC3 to RC4"}]}